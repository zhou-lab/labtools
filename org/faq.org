#+STARTUP: overview
#+STARTUP: indent
#+TAGS: memorize(a) unknown(u)
* bioinformatics
** bedtools
*** count GC content in a bed file/gff file
bedtools nuc -fi hg19.fa -bed exons.bed
*** how many feature B are overlapping feature A?
bedtools intersect -a A -b B -c
**** count C methylation in each window
bedtools intersect -c -a window.bed -b cytosine.bed
*** count base overlap between feature A and feature B?
bedtools intersect -a A -b B -wo
*** how to output feature A that are NOT overlapping with any feature B?
bedtools intersect -a A -b B -v
*** how to require minimal fraction of overlap A by a feature B?
bedtools intersect -a A -b B -wo -f 0.50
*** how to sort and use sorted bed files?
Each dataset must be sorted by the chromosome and then by start position
sort -k1,1 -k2,2n
*** how to overlap feature A with multiple feature B, C, D
require bedtools 2.21.0
bedtools intersect -a A -b B C D -sorted
# how many cpg, gwas and chromHmm are overlapping with exons?
bedtools intersect -a exons.bed -b cpg.bed gwas.bed hesc.chromHmm.bed -sorted
**** using -names for labeling compared features.
bedtools intersect -a exons.bed -b cpg.bed gwas.bed hesc.chromHmm.bed -sorted -wa -wb -names cpg gwas chromhmm | head -10000 | tail -10
*** given bam and a bed file, find region in the bed not covered by the bam
bedtools genomecov -ibam aln.bam -bga | awk '$4==0' | bedtools intersect -a regions -b - > foo

*** find genome coverage of a bam file
by base
bedtools genomecov -ibam aln.bam -d
by bedGraph
bedtools genomecov -ibam aln.bam -bg
by bedGraph with zero coverage
bedtools genomecov -ibam aln.bam -bga

*** report coverage accounting for split alignment (as in RNA seq, across exons)
bedtools genomecov -ibam aln.bam -split

*** report coverage by reads on + strand 
bedtools genomecov -ibam aln.bam -bg -strand +

*** identify regions of the genome with sufficient coverage
bedtools genomecov -ibam aln.bam -bg | awk '$4>50'

*** annotate a bed with difference counts
bedtools annotate -counts -i target.bed -files genes.bed conserve.bed knownvar.bed

*** extract tag from bam
bedtools bamtobed -i aln.bam -tag NM

*** bamtofastq in bedtools
# sort by qname first
samtools sort -n aln.bam aln.qsort
bedtools bamtofastq -i aln.bam -fq s1.fq -fq2 s2.fq

*** group features by genomic proximity
bedtools cluster -i in.bed -d 1000 # cluster all intervals within 1000 bp 

*** how to compare coverage across samples
bedtools unionbedg -i 1.bg 2.bg 3.bg -header -names sample1 sample2 sample3

*** compare calling results from multiple samples
bedtools unionbedg -i snp1.bg snp2.bg -filler -/-
chr1 0 1 A/G C/C A/G
chr1 5 6 C/T -/- C/T
chr1 7 8 -/- T/T -/-

*** search near features
bedtools window -w 5000 -a a.bed -b b.bed
#searches 5kb up and downstream of each feature in a

** bedops

*** sort-bed
For example, the following command allocates two gigabytes of memory to sorting a large BED file:

#+BEGIN_SRC sh
$ sort-bed --max-mem 2G unsortedBigData.bed > sortedBigData.bed
#+END_SRC

This revision of sort-bed includes the functionality of the former BEDOPS bbms (Big Bed Merge Sort) script, which implemented a merge sort of BED input on memory-constrained workstations.

With bbms, once each of the subsets was sorted in memory with sort-bed, sorted output was constructed from the entire set, applying a multiset union operation with BEDOPS bedops.

** how to unmark sam's duplicate flag
samtools view -h input.bam | awk '{if(and($2,1024)){$2-=1024}; print}' | samtools view -Sb > input.unmarked.bam
** to use BioPerl to change 1-letter amino acid code to 3-letter amino acid code
perl -MBio::SeqUtils -alne 'print "p.".$Bio::SeqUtils::THREECODE{$F[6]}.$F[2].$Bio::SeqUtils::THREECODE{$F[7]};' <input file>
** how to sort chr1, chr20 etc?
use --version-sort see:
sort -V
it sorts natural numbers within text
see also vcf-sort which uses sort -V
* unix
** curl download http
curl -s http://jaspar.genereg.net/html/DOWNLOAD/bed_files/ | grep href | grep '.bed' | sed 's/.*href="//' | sed 's/".*//' | while read file; do echo $file; curl -OL http://jaspar.genereg.net/html/DOWNLOAD/bed_files/$file; done

-s is --silent, no output of error messge or progress
-O use remote name
-L follow indirect link
** encryption and decryption using openssl

encrypt:
tar cz target_folder | openssl enc -aes-256-cbc -e > target.tar.gz.enc

decrypt
openssl aes-256-cbc -d -in out.tar.gz.enc -out decrypted.tar.gz

decrypt:
openssl aes-256-cbc -d -in target.tar.gz.enc | tar xz

sidenote: zip encryption is not safe
** using gpg for encryption
gpg --encrypt out.tar.gz

** other
*** change default bash
edit /etc/shells and make sure the shell you want to use is in that file
chsh -s /usr/local/bin/bash
*** convert rows to columns
**** paste -s -d, input_file (best)
**** use tr
tr "\n" "," < input_file
and remove last comma
tr "\n" ","  < input_file | sed 's/,$//'
**** use xargs and sed
xargs < infile | sed 's, ,\,,g'
**** use awk
awk 'BEGIN{FS="";ORS=","} {print}' test | sed 's/,$//'

*** how to remove blank lines
grep -v "^$" file.txt
sed '/^$/d' file.txt
awk '/./' file.txt
tr -s '\n' < file.txt

*** how to show non-printable characters like line end and tab
cat -vet | less
*** how to merge extra blank lines
cat -s
*** how to remove all blank space
#+BEGIN_SRC sh
echo -e ' test test test ' | tr -d '[[:space:] ]'
# testtesttest
#+END_SRC

*** how to remove leading and trailing white space
sed
#+BEGIN_SRC sh
$(echo -e "${FOO}" | sed -e 's/^[[:space:] ]*//' -e 's/[[:space:] ]*$//')"
#+END_SRC

xargs
#+BEGIN_SRC sh
echo "   lol  " | xargs
#+END_SRC

pure bash
#+BEGIN_SRC sh
var="    abc    "
var="${var#"${var%%[![:space:]]*}"}"   # remove leading whitespace characters
var="${var%"${var##*[![:space:]]}"}"   # remove trailing whitespace characters
echo "===$var==="

trim() {
    local var="$*"
    var="${var#"${var%%[![:space:]]*}"}"   # remove leading whitespace characters
    var="${var%"${var##*[![:space:]]}"}"   # remove trailing whitespace characters
    echo -n "$var"
}
#+END_SRC

** lftp
lftp
> open -u name,password -p port site
> open -u anonymous,ftpuser@adobe.com -p 21 ftp.adobe.com

add site
> site addip admin mewbies@127.0.0.1

login a site
> open ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sra-instant/reads/ByStudy/sra/SRP%2FSRP032%2FSRP032967/SRR1029055/

list files
> ls

open a http site
> open http://gdac.broadinstitute.org/runs/analyses__2014_10_17/data/ESCA/20141017/

disconnect from site
> close

disconnect and exit
> exit
or
> quit
or
> bye

issue shell commands
> !mkdir downloads

change to local directory
> lcd downloads

bookmark site
> bookmark add adobe

list all bookmarks
> bookmark list

download directory (quotes are necessary only containing space)
> queue mirror "directory name"

download file (quotes are necessary only containing space)
> queue pget "my file"

queue automatically starts

view queued commands
> queue
Now executing: [2] mirror SRR1029056 -- 625M/18G (3%) 5.42 MiB/s
Commands queued:
 1. mirror SRR1029055

delete job number 1
> queue -d 1

delete entire queue
> queue -d

view jobs in waiting
> jobs

cancel a transfer
> Ctrl+c

resume download
> mirror -c "directory name"
> pget -c "file name"

configuration
~/.lftprc

view default configuration
> set -a

skip file_id.diz, *.nfo and any directory and [
> set mirror:exclude-regex ^.*file_id\.diz.*$\|^.*\.nfo.*$\[.*$
one can put the above to .lftprc

*** one liner
upload a file (one liner)
lftp -e "put /local/path/yourfile.mp4; bye' -u user,password ftp.foo.com

download binary mode
lftp -e 'set net:timeout 10; get yourfile.mp4; bye' -u user,password ftp.foo.com

upload a directory (reverse mirror)
lftp -e 'mirror -R /local/path/ /' -u user,password ftp.foo.com

recursive down (mirror)
lftp -e 'mirror / /local/path/' -u user,password ftp.foo.com

recursive upload with regular expression
lftp -e 'mirror -R -i "\.(html?|css|js)$" /local/path/ /' -u user,password ftp.foo.com

non recursive upload
lftp -e 'mirror -R -r -i "\.(html?|css|js)$" /local/path/ /' -u user,password ftp.foo.com

*** special characters in username and password
lftp -e 'put /local/path/yourfile.mp4; bye' -u user,password\!\! ftp.foo.com

** find
*** let find -regex support [ [:digit:]]
does not support POSIX character classes like [ [:digit:] ]
or you can do 
find -regextype posix-extended -regex '.*[ [:digit:] ]'
*** find -exec
**** change mode of all directories
find . -type d -exec chmod 777 {} \;
*** find subfolders in the current directory (including the current directory)
find . -type d -maxdepth 1 | wc -l 
*** find with regular expression
http://www.gnu.org/software/findutils/manual/html_mono/find.html#Regular-Expressions
Regular expressions with character classes (e.g. [[:digit:]]) are not supported in the default regular expression syntax used by find. You need to specify a different regex type such as posix-extended in order to use them.
The regular expression counts from the "./" or the relative mother path. Therefore, one'd better use the following (start from ".*"):
find . -regex ".*COG.*_phyml_tree.*"
find . ! -regex '.*.phylip$' -type f | xargs rm -f
GNU Emacs style regular expression
1. No need to escape '.'. File name MATCH from the beginning. It is not a search as is the case in Perl.
2. no character classes such as  [[:digit:]] Need to use [0-9] instead.
3. escape + and ? only (\+ and \?)
4. ‘\<’ matches the beginning of a word
5. ‘\>’ matches the end of a word
6. ‘\b’ matches a word boundary
7. ‘\B’ matches characters which are not a word boundary
8. ‘\`’ matches the beginning of the whole input
9. ‘\'’ matches the end of the whole input
-regextype change regular expression type
-regextype emacs
-regextype posix-awk
-regextype posix-basic
-regextype posix-egrep
-regextype posix-extended
find . -regextype posix-extended -regex '.*COG[[:digit:]]+.phylip$'

*** find -name and -iname (case insensitive)
find /usr/lib -name '*netcdf*'
find /usr/lib* -name '*netcdf*'
find /lib* -name '*netcdf*'
note we cannot put more than 1 wildcards in the pattern
e.g., find . -name *.log would work
but find . -name *alt* wouldn't work, need to put a quotation

*** find file names that doesn't match pattern
find . -maxdepth 1 ! -name *.py -print
-maxdepth 1 不搜索子目录
! -name 名字不是*.py

*** -and -or and other logic
find ! -regex '.*.sql' -and ! -regex '.*.txt'

*** find according to type
find -type f -executable | xargs chmod 644
find . -type d -exec chmod 777 {} \;

*** find with execution
It replaces the string ‘{}’ by the current file name being processed everywhere it occurs in the command. Both of these constructions need to be escaped (with a ‘\’) or quoted to protect them from expansion by the shell. The command is executed in the directory in which find was run.
find . -name "*.java" -exec wc -l '{}' \; | awk '{sum += $1} END {print sum}'
find . -type f -size +10000 -exec ls -al {} \;
find . -type d -exec chmod 777 {} \;
find . ! -name '*.net' -type f -exec rm {} \; 删除不含.net的普通文件
sudo find / -type f -name *.jpg -exec cp {} . \;
find . -atime +1 -type f -exec mv {} TMP \; # mv files older then 1 day to dir T
find . -name "-F" -exec rm {} \; # a script error created a file called -F
find . -exec grep -i "vds admin" {} \;
find . \! -name "*.Z" -exec compress -f {} \;
find . -type f \! -name "*.Z" \! -name ".comment" -print | tee -a /tmp/list
find . -user xuser1 -exec chown -R user2 {} \;
find . -exec grep PW0 {} \;
find . -exec grep -i "pw0" {} \;
find . -atime +6
find . -atime +6 -exec ll | more
find . -atime +6 -exec ll | more \;
find . -atime +6 -exec ll \;
find . -atime +6 -exec ls \;
find . -atime +30 -exec ls \;
find . -atime +30 -exec ls \; | wc -l
find . -name auth*
find . -exec grep -i plotme10 {};
find . -exec grep -i plotme10 {} \;
find . -ls -exec grep 'PLOT_FORMAT 22' {} \;
find . -print -exec grep 'PLOT_FORMAT 22' {} \;
find . -print -exec grep 'PLOT_FORMAT' {} \;
find . -print -exec grep 'PLOT_FORMAT' {} \;
find ./machbook -exec chown 184 {} \;
find . \! -name '*.Z' -exec compress {} \;
find . \! -name "*.Z" -exec compress -f {} \;
find /raid/03c/ecn -xdev -type f -print
find /raid/03c/ecn -xdev -path -type f -print
find / -name .ssh* -print | tee -a ssh-stuff
find . -name "*font*"
find . -name hpmcad*
find . -name *fnt*
find . -name hp_mcad* -print
find . -grep Pld {} \;
find . -exec grep Pld {} \;
find . -exec grep Pld {} \;
find . -exec grep PENWIDTH {} \; | more
find . -name config.pro
find . -name config.pro
find /raid -type d ".local_sd_customize" -print
find /raid -type d -name ".local_sd_customize" -print
find /raid -type d -name ".local_sd_customize" -ok cp /raid/04d/MCAD-apps/I_Custom/SD_custom/site_sd_customize/user_filer_project_dirs {} \;
find /raid -type d -name ".local_sd_customize" -exec cp /raid/04d/MCAD-apps/I_Custom/SD_custom/site_sd_customize/user_filer_project_dirs {} \;
find . -name xeroxrelease
find . -exec grep xeroxrelease {} \;
find . -name xeroxrelease
find . -name xeroxrelease* -print 2>/dev/null
find . -name "*release*" 2>/dev/null
find / -name "*xerox*" 2>/dev/null
find . -exec grep -i xeroxrelease {} \;
find /raid/03c/inwork -xdev -type f -print >> /raid/04d/user_scripts/prt_list.tmp
find . -exec grep '31.53' {} \;
find . -ls -exec grep "31/.53" {} \; > this.lis
find . -print -exec grep "31/.53" {} \; > this.lis
find . -print -exec grep 31.53 {} \; > this.lis
find . -exec grep -i pen {} /;
find . -exec grep -i pen {} \;
find . -print -exec grep -i pen {} \; | more
find . -exec grep -i pen {} \;
find . -atime +6 -exec ll | more \;
find . -atime +6 -exec ll \;
find . -atime +6 -exec ls \;
find . -atime +30 -exec ls \;
find . -atime +30 -exec ls \; | wc -l
find . \! -name '*.Z' -exec compress -f {} \;
find . -name 'cache*' -depth -exec rm {} \;
find . -name 'cache*' -depth -print | tee -a /tmp/cachefiles
find . -name 'cache[0-9][0-9]*' -depth -print | tee -a /tmp/cachefiles
find . -name 'hp_catfile' 'hp_catlock' -depth -print | tee -a /tmp/hp.cats
find . -name 'hp_catfile' -name 'hp_catlock' -depth -print | tee -a /tmp/hp.cats
find . -name 'hp_cat*' -depth -print | tee -a /tmp/hp.cats
find . -name 'hp_cat[fl]*' -depth -print | tee -a /tmp/hp.cats
find /raid -name 'hp_cat[fl]*' -depth -print
find . \! -name '*.Z' -exec compress -f {} \;
find . -name '*' -exec compress -f {} \;
find . -xdev -name "wshp1*" -print
find . -xdev -name "wagoneer*" -print
find . -name "xcmd" -depth -print
find /usr/contrib/src -name "xcmd" -depth -print
*** find file names that doesn't match pattern
find . -maxdepth 1 ! -name *.py -print
-maxdepth 1 不搜索子目录
! -name 名字不是*.py
*** -and -or and other logic
     find ! -regex '.*.sql' -and ! -regex '.*.txt'

*** note we cannot put more than 1 wildcards in the pattern
     e.g., find . -name *.log would work
     but find . -name *alt* wouldn't work
*** find all the executable regular files and change their privilege
     find -type f -executable | xargs chmod 644

*** some examples
***** find the total number of lines in multiple files
       find . -name "*.java" -exec wc -l '{}' \; | awk '{sum += $1} END {print sum}'
*** to find regular files in the directory
find . -type f | wc -l
*** unclassified
     find . ! -name '*.net' -type f -exec rm {} \; 删除不含.net的普通文件
     sudo find / -type f -name *.jpg  -exec cp {} . \;

     find . -type f -size +10000 -exec ls -al {} \;
     find . -atime +1 -type f -exec mv {} TMP \; # mv files older then 1 day to dir T
     find . -name "-F" -exec rm {} \;   # a script error created a file called -F
     find . -exec grep -i "vds admin" {} \;
     find . \! -name "*.Z" -exec compress -f {} \;
     find . -type f \! -name "*.Z" \! -name ".comment" -print | tee -a /tmp/list
     find . -name *.ini
     find . -exec chmod 775 {} \;
     find . -user xuser1 -exec chown -R user2 {} \;
     find . -name ebtcom*
     find . -name mkbook
     find . -exec grep PW0 {} \;
     find . -exec grep -i "pw0" {} \;
     find . -atime +6
     find . -atime +6 -exec ll | more
     find . -atime +6 -exec ll | more \;
     find . -atime +6 -exec ll \;
     find . -atime +6 -exec ls \;
     find . -atime +30 -exec ls \;
     find . -atime +30 -exec ls \; | wc -l
     find . -name auth*
     find . -exec grep -i plotme10 {};
     find . -exec grep -i plotme10 {} \;
     find . -ls -exec grep 'PLOT_FORMAT 22' {} \;
     find . -print -exec grep 'PLOT_FORMAT 22' {} \;
     find . -print -exec grep 'PLOT_FORMAT' {} \;
     find . -print -exec grep 'PLOT_FORMAT' {} \;
     find ./machbook -exec chown 184 {} \;
     find . \! -name '*.Z' -exec compress {} \;
     find . \! -name "*.Z" -exec compress -f {} \;
     find /raid/03c/ecn -xdev -type f -print
     find /raid/03c/ecn -xdev -path -type f -print
     find / -name .ssh* -print | tee -a ssh-stuff
     find . -name "*font*"
     find . -name hpmcad*
     find . -name *fnt*
     find . -name hp_mcad* -print
     find . -grep Pld {} \;
     find . -exec grep Pld {} \;
     find . -exec grep Pld {} \;
     find . -exec grep PENWIDTH {} \; | more
     find . -name config.pro
     find . -name config.pro
     find /raid -type d ".local_sd_customize" -print
     find /raid -type d -name ".local_sd_customize" -print
     find /raid -type d -name ".local_sd_customize" -ok cp /raid/04d/MCAD-apps/I_Custom/SD_custom/site_sd_customize/user_filer_project_dirs {} \;
     find /raid -type d -name ".local_sd_customize" -exec cp /raid/04d/MCAD-apps/I_Custom/SD_custom/site_sd_customize/user_filer_project_dirs {} \;
     find . -name xeroxrelease
     find . -exec grep xeroxrelease {} \;
     find . -name xeroxrelease
     find . -name xeroxrelease* -print 2>/dev/null
     find . -name "*release*" 2>/dev/null
     find / -name "*xerox*" 2>/dev/null
     find . -exec grep -i xeroxrelease {} \;
     find . -print -exec grep -i xeroxrelease {} \;
     find . -print -exec grep -i xeroxrelease {} \; > xeroxrel.lis
     find . -exec grep -i xeroxrel {} \;
     find . -print -exec grep -i xeroxrel {} \;
     find . -print -exec grep -i xeroxrel {} \; | more
     find /raid/03c/inwork -xdev -type f -print >> /raid/04d/user_scripts/prt_list.tmp
     find . -exec grep '31.53' {} \;
     find . -ls -exec grep "31/.53" {} \; > this.lis
     find . -print -exec grep "31/.53" {} \; > this.lis
     find . -print -exec grep 31.53 {} \; > this.lis
     find . -exec grep -i pen {} /;
     find . -exec grep -i pen {} \;
     find . -print -exec grep -i pen {} \; | more
     find . -exec grep -i pen {} \;
     find . -atime +6 -exec ll | more \;
     find . -atime +6 -exec ll \;
     find . -atime +6 -exec ls \;
     find . -atime +30 -exec ls \;
     find . -atime +30 -exec ls \; | wc -l
     find . \! -name '*.Z' -exec compress -f {} \;
     find . -name 'cache*' -depth -exec rm {} \;
     find . -name 'cache*' -depth -print | tee -a /tmp/cachefiles
     find . -name 'cache[0-9][0-9]*' -depth -print | tee -a /tmp/cachefiles
     find . -name 'hp_catfile' 'hp_catlock' -depth -print | tee -a /tmp/hp.cats
     find . -name 'hp_catfile' -name 'hp_catlock' -depth -print | tee -a /tmp/hp.cats
     find . -name 'hp_cat*' -depth -print | tee -a /tmp/hp.cats
     find . -name 'hp_cat[fl]*' -depth -print | tee -a /tmp/hp.cats
     find /raid -name 'hp_cat[fl]*' -depth -print
     find . \! -name '*.Z' -exec compress -f {} \;
     find . -name '*' -exec compress -f {} \;
     find . -xdev -name "wshp1*" -print
     find . -xdev -name "wagoneer*" -print
     find . -name "xcmd" -depth -print
     find /usr/contrib/src -name "xcmd" -depth -print
     find /raid -type d -name ".local_sd_customize" -exec ls {} \; 
     find /raid -type d -name ".local_sd_customize" \
     -exec cp /raid/04d/MCAD-apps/I_Custom/SD_custom/site_sd_customize/user_filer_project_dirs {} \;
** how to print pattern to pattern using sed and awk
sed -n '/StartPattern/,/EndPattern/p' FileName
awk '/StartPattern/,/EndPattern/' FileName

** join lines in a file                                            :unknown:
perl -ne 'chomp;print;' file
tr -d '\n' < file
awk '{printf $0;}' file
paste -s --delimiters="" file
sed -e :a  -e 'N;s/\n//;ta'  file
** delete character in line using tr
tr -d "\n"
remove all newline characters.

** how to use basic calculator (bc) in unix
-l load standard math library
bc -l <<< "5*7/3"
11.6666
$ result=$(echo "scale=2; 5 * 7 /3;" | bc)
$ echo $result
11.66

** to interpret MS dos (Windows) file with ^M                      :unknown:
The ^M that you are seeing is actually a CR or \r. If you want to test for carraige returns in a file, you want to look for \r. Try this on the file
dos2unix
or
tr '\r' '\n' < [file] > [output]

*** this is a better solution
tr -d "\015" <file1 
** ssh X11 tunneling
ssh -X
to make default, put in ~/.ssh/config
ForwardX11 yes
on the server side, put in /etc/ssh/sshd_config
X11Forwarding yes

** mkdir and cd into it
mkdir /foo/bar && cd $_;
$_ is the most recent parameter
** curl
*** download following redirection
curl -O -L http://sourceforge.net/...
** wget
*** recursive download folder via ftp using wget
wget -r ftp://yourfolder
*** no host directory structure
wget -nH 

No options        -> ftp.xemacs.org/pub/xemacs/
-nH               -> pub/xemacs/
-nH --cut-dirs=1  -> xemacs/
-nH --cut-dirs=2  -> .
*** wget with regular expression
#+BEGIN_SRC 
-A acclist --accept acclist
-R rejlist --reject rejlist
     Specify comma-separated lists of file name suffixes or patterns to 
     accept or reject. Note that if any of the wildcard characters, *, ?,
     [ or ], appear in an element of acclist or rejlist, it will be 
       treated as a pattern, rather than a suffix.
       
--accept-regex urlregex
--reject-regex urlregex
     Specify a regular expression to accept or reject the complete URL

-np --no-parent
#+END_SRC

wget -r -np -A 'bar.*.tar.gz' http://url/dir/
*** no creating directory
-nd

** gnu parallel
*** how to redirect inside parallel
use quote "'>'"
e.g.,
find /data/largeS2/pl-bs/data/2015-01-27-bam-all/ -name *.bam | parallel --gnu -j 10 samtools view -f 4 {} '>' unaligned_reads/{/.}.unaligned.bam
*** parallelize large data copy
find . -type f | parallel 'mkdir -p ../Copy/{//}; rsync -a {} ../Copy/{}'
or simply
find . -type f | parallel cp -a {} .
*** use perl expression in replacement string

{= perl expression =} can be used as replacement string. The expression should modify $_. E.g. {= s/\.gz$// =} to remove .gz from the string. This makes replacement strings extremely flexible.

Positional perl expressions (similar to {2}) are given as {=2 perl expression=} where 2 is the position.

One small backwards incompatability: {1}_{2} will replace {2} with the empty string if there is only one argument. Previously {2} would have been left untouched.

Replacement strings can be defined using --rpl. E.g. parallel --rpl '{.gz} s/\.gz$//' echo {.gz} ::: *.gz

The parenthesis around {= perl expression =} can be changed with --parens.
** pretty print columns using "pr"
pr -t -e15 your_file | less
-e15 replace tab stops to the next 15 spaces column

** gnu screen
*** gnu screen set window title to be working directory
http://unix.stackexchange.com/questions/28430/screen-status-bar-to-display-current-directory-for-zsh-bash-shell
http://vim.wikia.com/wiki/Automatically_set_screen_title
fix the bug
http://superuser.com/questions/244299/gnu-screen-how-to-update-dynamically-the-title-of-a-window
about using PROMPT_COMMAND (specific to bash, allows putting command result before the PS
http://www.tldp.org/HOWTO/Bash-Prompt-HOWTO/x264.html
** shell utility
*** sort
version 8.23 support --parallel=N
this is the mergesort which might not be as fast as quicksort

--buffer-size=5G
cap buffer size in RAM

sort -S 50% file
to use (up toe) 50% of your memory as a sorting buffer do

The -T option allows you to specify somewhere else besides /tmp for `sort` to put its stuff.

The --compress-program option, tells `sort` to compress its temp files using whatever compressor you tell it to use.

use
LC_ALL=C sort
to enforce a consistent sort order on alphabets
how sort sorts is dependent on the locale (language) settings of the environment that the script is running under.

#+BEGIN_SRC sh
$ echo 'CO2_
CO_' | env LC_ALL=C sort
CO2_
CO_

$ echo 'CO2_
CO_' | env LC_ALL=en_US sort
CO_
CO2_
#+END_SRC

-k --key=POS1
-n --numeric-sort sort according to string numerical value
-r --reverse
-t --field-separator
examples:
#+BEGIN_SRC sh
ps | sort > pssort.out
ps | sort | more
ps xo comm | sort | uniq | grep -v sh | more
#+END_SRC
**** split and merge sort

#+BEGIN_SRC sh
split -l5000000 data.tsv '_tmp';
ls -1 _tmp* | while read FILE; do sort $FILE -o $FILE & done;
Followed by:
sort -m _tmp* -o data.tsv.sorted
#+END_SRC

`--merge'
Merge the given files by sorting them as a group. Each input file
must always be individually sorted. It always works to sort
instead of merge; merging is provided because it is faster, in the
case where it works

**** sort file by size
du -sh * | sort -k1,1 -h | les

**** sort ignore header
***** using awk (best solution)
awk 'NR==1;NR>1{print|"sort"}' [in] > [out]

***** using bash (2nd best solution)
cat [in] | (read -r; print "%s\n" "$REPLY"; sort)
explanation: read -r is for reading one line from the standard input without escaping backslash "\". the result is put into variable $REPLY. -r is needed if the header line contains backslashes.
echo without $REPLY will condense whitespace

***** make a function for bash solution (only skip the first line)
#+BEGIN_SRC sh
body() {
  IFS=read -r header
  printf '%s\n' "$header"
  "$@"
}
#+END_SRC
example usage:
ps -o pid,comm | body sort -k2
ps -o pid,comm | body grep "somethingtofind"

***** or using head and tail
(head -n2 <file> && tail -n+3 <file> | sort) > newfile

***** or using head and sed
head -1 [file] > [out]
sed 1d [file] | sort >> [out]

***** using perl
cat [in] | perl -e 'print scalar (<>); print sort {...} <>'
The perl solution uses the sort function in perl instead the unix sort.
The ... in sort {...} defines how sorting takes place. for example, sort {$a <=> $b} means numerical ascending sort. See perl sort manual for detail.
*** recall commands from shell prompt
C-r search backward the command
!! last command
*** xargs
file -Lz * | grep ASCII | cut -d":" -f1 | xargs ls -ltr
file * | grep ASCII | cut -d":" -f1  | xargs wc -l
wc -l ‘file * | grep ASCII | cut -d":" -f1 | grep ASCII | cut -d":" -f1‘
ls | xargs -t -i mv {} {}.bak
file * | grep ASCII | cut -d":" -f1 | xargs vi
file * | grep ASCII | cut -d":" -f1 | xargs -p vi
file * | grep SSSSSS | cut -d":" -f1 | xargs -t -r wc -l
file * | grep ASCII | cut -d":" -f1 | xargs -t -n2 ls -ltr
ls | xargs -n 20 rm -fr
*** DIR stack and cd path
pushd /home/wan/program
popd: to put the current directory into a stack
$DIRSTACK the top value of pushd and popd stack.
cd - return to the last directory
*** how to remove line wrap in less, very useful for displaying wide tabular data
/-S
*** read - read from standard input
read -p "Enter range of number to display using 0..10 format: " range
*** tr
reads from standard in to standard out
it does not take any input files, usually used to convert cases.

convert lower case to upper case
tr a-z A-Z
tr '[:lower:]' '[:upper:]'

`tr' performs translation when SET1 and SET2 are both given and the `--delete' (`-d') option is not given. `tr' translates each character
of its input that is in SET1 to the corresponding character in SET2.
Characters not in SET1 are passed through unchanged. When a character appears more than once in SET1 and the corresponding characters in SET2
are not all the same, only the final one is used.
tr aaa xyz
is equivalent as 
tr a z

*** file - determine the file type of a file
file index.html 
output:
index.html: HTML document, UTF-8 Unicode text, with very long lines

*** diff/rsync/md5sum/md5deep - detect difference in directories
**** use diff
diff -rq dir1 dir2
diff -rq /backups/external{3,4}/var/subsonic/thumbs/110
outputs:
Files /tmp/a/c and /tmp/q/c differ
Only in /tmp/a/d/e: f
Only in /tmp/q/d/e: g
this method is best for two-way comparison
-r: recursive
-q: brief report

**** use rsync
use rsync dry-run (without making changes) to detect directory differences
rsync -nric  dev2/py/lib/sysami/ dev/py/lib/sysami/
-n: dry run (no change)
-i: output a change-summary for all updates
-c: skip based on checksum, not mod-time & size

REMEMBER to put / at the end of the source directory
*An easy nemo is to append / to the end of both directories*

**** use md5sum
to create a fast check sum
ls -AlR pbs2/pbs | awk 'NR>1' | md5sum
ls -A list all hidden files but not the current directory and the parent directory.
ls -h list in human readable form
ls -R means recursive
note that ls -r is reverse order
If you do not want the time stamp (as would be preserved by rsync -a), you can
ls -AlR pbs2/pbs | awk '{$6="";$7="";}NR>1' | md5sum
directories can give wrong size estimate due to defragmentation
ls -AlR pbs2/pbs | awk '{$6="";$7="";}(NR>1 && !/^total/ && !/^drwx/)' | md5sum

**** use md5deep
hash a directory, store the result locally for comparison
md5deep -rel “test_directory” > results_file.md5
r = recursive operation
e = compute estimated time remaining for file name
l = print relative paths for file name
this method is best for multi-way comparison
*** cut
output first to fifth field
#+BEGIN_SRC sh
cut -f 1-5 file
#+END_SRC

output the fourth through tenth characters of each line
#+BEGIN_SRC sh
cut -c 4-10 file
#+END_SRC

output the fifth field through the end of the line of each line using the colon character as the field delimiter
#+BEGIN_SRC sh
cut -d ":" -f 5- file
#+END_SRC

output only the third field
#+BEGIN_SRC sh
cut -d " " -f 3 file
#+END_SRC

other examples
#+BEGIN_SRC sh
hg st | grep '.*.rst' | cut -d " " -f 2 | xargs hg add
for f in *; do a=$(wc -l $f | cut -d" " -f1); cntall=$(expr $cntall + $a); done
#+END_SRC

*** top
s  change refresh rate, default is 3s
t  切换summary information
m  切换memory information
A  按各种系统资源使用排序
f   Enters an interactive configuration screen for top. Helpful for setting up top for a specific task
o  自定义显示顺序
r  发送renice命令
k  发送kill命令
z  切换彩色模式
*** ulimit - setting resource limit
ulimit -a
show all the current limits
ulimit -Sa
show all the soft limits, soft limits are recommended limit, have no actual effect
ulimit -Ha
show all the hard limits

throttle memory usage
ulimit -Sv 1000000 (six “zero”s ~1 G, unit is kilobyte)

*** uniq

count occurrence

#+BEGIN_SRC sh
cat somefile | sort | uniq -c 
#+END_SRC

*** wc
number of lines
wc -l

查看当前目录文件数
ls -1 | wc -l
ls -l | grep -v ^l | wc -l (excluding symbolic links)

wc -c myfile 数myfile文件的byte数。
*** nl - index lines in a file
nl file
---
aaaa
bbbb
---
becomes
---
1  aaaa
2  bbbb
---

*** locate and updatedb
locate netcdf
locate --regex '\/algorithm$'
*** head and tail
**** print the last line
#+begin_src sh
tail -n1 myfile
# or
tail -1 myfile
#+end_src
**** show second to last line in a file
#+BEGIN_SRC sh
tail -2 myfile | head -1
#+END_SRC
**** delete last line of a file
#+BEGIN_SRC sh
head -n -1 foo.txt > temp.txt; mv temp.txt foo.txt
#+END_SRC
**** delete the first line of a file
#+BEGIN_SRC sh
tail -n +2 foo.txt > temp.txt; mv temp.txt foo.txt
#+END_SRC
*** echo
**** suppress new line using -n
echo -n "Please answer yes or no: "

**** interpret "\t" and "\n" using -e
echo -e "a\tb"

*** printf
#+BEGIN_SRC sh
printf "%s\n" "$now"
#+END_SRC

*** basename
basename /usr/bin/sort
-> "sort"

basename include/stdio.h .h
-> "stdio"

basename -s .h include/stdio.h
-> "stdio"

basename -a any/str1 any/str2
-> "str1" followed by "str2"

*** dirname
#+BEGIN_SRC sh
dirname /a/b/c/d
#+END_SRC
output: /a/b/c
*** ls
-A list all hidden but not current (.) and parent (..)
-a list all hidden and current (.) and parent (..)
-R recursive
-r list in reverse order

-g long list no owner
-o long list no group
-go long list no owner or group

-x sorted from left to right instead of from top to bottom

*** redirection
**** redirect error message to file
prog 2> error_file

**** redirect both output and error to the same file
prog > out_and_err_file 2>&1
*** dd
**** dd if=/dev/fd0H1440 of=floppy_image
**** dd if=floppy_image of=/dev/fd0H1440
      (two commands, dd="data duplicator") Create an image of a floppy to the file called "floppy_image" in the current directory. Then copy floppy_image (file) to another floppy disk. Works like DOS "DISKCOPY".
*** time

#+BEGIN_EXAMPLE
$ time ./foo.pl
  real 0m0.014s
  user 0m0.000s
  sys  0m0.010s
#+END_EXAMPLE

*** ls color
change ls color, because sometimes the folder color is blue and the background is black. they are not well constrasted.
put into .bashrc
#+BEGIN_SRC bash
LS_COLOR='di=0;35'; export LS_COLORS
#+END_SRC

The color codes are:
#+BEGIN_EXAMPLE
Black       0;30     Dark Gray     1;30
Blue        0;34     Light Blue    1;34
Green       0;32     Light Green   1;32
Cyan        0;36     Light Cyan    1;36
Red         0;31     Light Red     1;31
Purple      0;35     Light Purple  1;35
Brown       0;33     Yellow        1;33
Light Gray  0;37     White         1;37
#+END_EXAMPLE

see [[http://www.tldp.org/HOWTO/Bash-Prompt-HOWTO/x329.html][Bash Prompt HowTo]]

**** alias ls="ls --color=tty"
      Create an alias for the command "ls" to enhance its format with color. In this example, the alias is also called "ls" and the "color" option is only envoke when the output is done to a terminal (not to files). Put the alias into the file /etc/bashrc if you would like the alias to be always accessible to all users on the system. Type "alias" alone to see the list of aliases on your system.
*** cal
     -m	monday first	-y	the year
     -s	sunday first	-j	Julean date
     -1	one month
     -3	three month
*** bc - a simple calculator
echo "scale=20; 35.5 / 2.3 " | bc
scale is the number of digits after the decimal points.

use bash here string to do the pipe
bc <<< 'scale=2; 2 / 5'

use bc -l to evoke the standard (but not default!) mathlib and see the result in floating point at max scale
bc -l <<< "10.5 / 3.3"

*** od - display binary file
it can read a binary file and interpret using certain encoding, i.e., ASCII
舉例來說，例如 /usr/bin/passwd 這個執行檔的內容時， 又該如何去讀出資訊呢？事實上，由於執行檔通常是 binary file ，使用上頭提到的指令來讀取他的內容時， 確實會產生類似亂碼的資料啊！那怎麼辦？沒關係，我們可以利用 od 這個指令來讀取喔！

[root@www ~]# od [-t TYPE] 檔案
選項或參數：
-t  ：後面可以接各種『類型 (TYPE)』的輸出，例如：
      a       ：利用預設的字元來輸出；
      c       ：使用 ASCII 字元來輸出
      d[size] ：利用十進位(decimal)來輸出資料，每個整數佔用 size bytes ；
      f[size] ：利用浮點數值(floating)來輸出資料，每個數佔用 size bytes ；
      o[size] ：利用八進位(octal)來輸出資料，每個整數佔用 size bytes ；
      x[size] ：利用十六進位(hexadecimal)來輸出資料，每個整數佔用 size bytes ；

**** 範例一：請將/usr/bin/passwd的內容使用ASCII方式來展現！
[root@www ~]# od -t c /usr/bin/passwd
0000000 177   E   L   F 001 001 001  \0  \0  \0  \0  \0  \0  \0  \0  \0
0000020 002  \0 003  \0 001  \0  \0  \0 260 225 004  \b   4  \0  \0  \0
0000040 020   E  \0  \0  \0  \0  \0  \0   4  \0      \0  \a  \0   (  \0
0000060 035  \0 034  \0 006  \0  \0  \0   4  \0  \0  \0   4 200 004  \b
0000100   4 200 004  \b 340  \0  \0  \0 340  \0  \0  \0 005  \0  \0  \0
.....(後面省略)....
# 最左邊第一欄是以 8 進位來表示bytes數。以上面範例來說，第二欄0000020代表開頭是
# 第 16 個 byes (2x8) 的內容之意。

**** 範例二：請將/etc/issue這個檔案的內容以8進位列出儲存值與ASCII的對照表
[root@www ~]# od -t oCc /etc/issue
0000000 103 145 156 164 117 123 040 162 145 154 145 141 163 145 040 065
          C   e   n   t   O   S       r   e   l   e   a   s   e       5
0000020 056 062 040 050 106 151 156 141 154 051 012 113 145 162 156 145
          .   2       (   F   i   n   a   l   )  \n   K   e   r   n   e
0000040 154 040 134 162 040 157 156 040 141 156 040 134 155 012 012
          l       \   r       o   n       a   n       \   m  \n  \n
0000057
# 如上所示，可以發現每個字元可以對應到的數值為何！
# 例如e對應的記錄數值為145，轉成十進位：1x8^2+4x8+5=101。
** 进程管理
    fg PID	## bring a background or stopped process to the foreground
    bg PID(=<Ctrl z>)	## send the process to the background, opposite to fg. If you have stopped jobs, you have to type exit twice in row to log out.
    any_command&	# run command in the background.
    batch any_command	# run any command when the system load is low.
    at 17:00	# Specify the time for the command which will be prompted to run
    kill PID
    Force a process shutdown
*** bring a background or stopped process to the foreground
fg PID

*** send the process to the background, opposite to fg. If you have stopped jobs, you have to type exit twice in row to log out.
bg PID(=<Ctrl z>)

*** run command in the background.
any_command&

*** run any command when the system load is low.
batch any_command

*** Specify the time for the command which will be prompted to run
at 17:00

*** Force a process shutdown
kill PID

*** kill with signal
kill -HUP 1234

*** kill based on name
pkill firefox

*** ps
-e: every process
-o: means user defined format

# comm means command
# no space is allowed between the format
ps -eo euser,ruser,suser,fuser,f,comm,label

ps -eo pid,tid,class,rtprio,ni,pri,psr,pcpu,stat,wchan:14,comm
ps axo stat,euid,ruid,tty,tpgid,sess,pgrp,ppid,pid,pcpu,comm
ps -eopid,tt,user,fname,tmout,f,wchan

ps -Afe | grep time-tracker

**** look up a process with a process id
ps -p 4707

*** put to background
**** use nohup
nohup test.sh &

**** use disown
test.sh &
disown

**** use “()”
put () around your command, e.g.,
(python 2011_04_23_rewire_network.py > ~/output.log 2>&1 &)

*** signal
     <Ctrl>-s stop data transfer
     <Ctrl>-q resume data transfer
*** w
     find out who is logged on and what they are doing
     w wanding
*** put foreground process to background
Ctrl-Z
bg
jobs
*** nohup / disown
     比如需在后台运行程序，
     nohup test.sh &
     或者使用
     test.sh &
     disown
     A BETTER SOLUTION:
     put () around your command, e.g.,
     (python 2011_04_23_rewire_network.py > ~/output.log 2>&1 &)
*** pkill and pgrep
     pkill firefox
*** top
     t 切换summary information
     m 切换memroy information
     A 按各种系统资源使用排序
     f Enters an interactive configuration screen for top. Helpful for setting up
     top for a specific task
     o 自定义显示顺序
     r 发送renice命令
     k 发送kill命令
     z 切换彩色模式
*** uptime
     tell how long the system has been running and the system load averages for the past 1, 5, and 15 minutes.
*** ps (= print status)
     report a snapshot of the current processes. To select all processes use the -A or -e option
     ps -A
**** ps summary
      # here -e means every process, -o means user defined format
      # no space is allowed between the format
      # comm means command
      ps -eo euser,ruser,suser,fuser,f,comm,label
      ps -eo pid,tid,class,rtprio,ni,pri,psr,pcpu,stat,wchan:14,comm
      ps axo stat,euid,ruid,tty,tpgid,sess,pgrp,ppid,pid,pcpu,comm
      ps -eopid,tt,user,fname,tmout,f,wchan
**** look up a process with a process id
      ps -p 4707
**** show memory information
ps -v
**** show long format output
      ps -Al
      ps -AlF
**** to see Threads (LWP and NLWP) 
      ps -AlFH
**** To see threads after processes
      ps -AlLm
**** Print all processes on the server
      ps ax
      ps axu	# display all processes on my system.
**** Print a process Tree
      ps -ejH
      ps axjf
      pstree
**** Print Security Information
      ps -eo euser, ruser, suser, fuser, f, comm, label
      ps axZ
      ps -eM
**** See every process running as user wanding
      ps -U wanding -u wanding u
**** Set output in a user-defined format
      ps  -eo pid,tid,class,rtprio,ni,pri,psr,pcpu,stat,wchan:14,comm
      ps axo stat,euid,ruid,tty,tpgid,sess,pgrp,ppid,pid,pcpu,comm
      ps -eopid,tt,user,fname,tmout,f,wchan
**** Display only the process IDs of Lighttpd
      ps -C lighttpd -o pid=
      pgrep lighttpd
      pgrep -u wanding php-cgi
**** Display the name of PID 55977
      ps -p 55977 -o comm=
      这是pgrep -l的反响操作
**** Find out the top 10 Memory Consuming Process
ps -auxf | sort -nr -k 4 | head -10
还可用top
**** Find out top 10 CPU consuming process
ps -auxf | sort -nr -k 3 | head -10
*** 进程控制
**** 查询进程
      pgrep -l gaim
      pkill gaim
**** 常用POSIX 信号
***** SIGHUP #1
       挂起信号
       e.g., kill -HUP [pid]
       killall -HUP [process-name]
       kill -1 [pid]
       killall -1 [process-name]
***** SIGINT #2
       中断信号，SIGINT是一个symbolic constant defined in signal.h, Control-C发送该信号。
***** SIGKILL #9
       立即终止，不允许clean-up
***** SIGTERM #15
       可能被进程捕捉或忽略或以不同方式被解释。SIGTERM 相当于要求进程用较温和的方式关闭。允许关闭文件，清理内存。
***** SIGSTOP #17,19,23
       kill -STOP [pid]
       停止进程，但不消灭这一进程。该信号不能被捕捉。
***** SIGCONT #19,18,25
       continue
       重新开始一个停止的进程
       kill -CONT [pid]
***** kill -9 -1 终止全部进程
**** xkill
      Kill a GUI-based program with mouse.(point with your mouse cursor at the window and click)
**** pkill
      同killall，但可以附加其他属性
      e.g., 
      ==============
      pgrep -l gaim
      pkill gaim
      ==============
**** kill ［信号代码］ 进程ID
      kill的应用是和ps auxf或pgrep -l命令结合在一起使用的；
      注：信号代码可以省略；我们常用的信号代码是 -9 ，表示强制终止；
      举例：
      [root@localhost ~]# ps auxf |grep httpd
      root 4939 0.0 0.0 5160 708 pts/3 S+ 13:10 0:00 \_ grep httpd
      root 4830 0.1 1.3 24232 10272 ? Ss 13:02 0:00 /usr/sbin/httpd
      apache 4833 0.0 0.6 24364 4932 ? S 13:02 0:00 \_ /usr/sbin/httpd
      apache 4834 0.0 0.6 24364 4928 ? S 13:02 0:00 \_ /usr/sbin/httpd
      apache 4835 0.0 0.6 24364 4928 ? S 13:02 0:00 \_ /usr/sbin/httpd
      我们查看httpd 服务器的进程；您也可以用pgrep -l httpd 来查看；

      我们看上面例子中的第二列，就是进程PID的列，其中4830是httpd服务器的父进程，从
      4833－4840的进程都是它4830的子进程；如果我们杀掉父进程4830的话，其下的子进程也
      会跟着死掉；
      [root@localhost ~]# kill 4840 注：杀掉4840这个进程；
      [root@localhost ~]# ps -auxf |grep httpd 注：查看一下会有什么结果？是不是httpd
      服务器仍在运行？
      [root@localhost ~]# kill 4830 注：杀掉httpd的父进程；
      [root@localhost ~]# ps -aux |grep httpd 注：查看httpd的其它子进程是否存在，
      httpd服务器是否仍在运行？
      对于僵尸进程，可以用kill -9 来强制终止退出；
      比如一个程序已经彻底死掉，如果kill 不加信号强度是没有办法退出，最好的办法就是加
      信号强度 -9 ，后面要接杀父进程；比如；
      [root@localhost ~]# ps aux |grep gaim
      beinan 5031 9.0 2.3 104996 17484 ? S 13:23 0:01 gaim
      root 5036 0.0 0.0 5160 724 pts/3 S+ 13:24 0:00 grep gaim
      或 [root@localhost ~]# pgrep -l gaim
      5031 gaim
      [root@localhost ~]# kill -9 5031
**** killall [信号代码］ 进程名字
      同kill，但是使用进程名字，更常用。
      e.g. killall -9 opera 杀掉opera进程
      killall 发出的信号是 SIGTERM
      killall -9 ProgramName 发出信号是 SIGKILL
**** 数据库服务器不能强制杀死
      为什么数据库服务器的父进程不能用这些工具杀死呢？原因很简单，这些工具在强行终止数据库服务器时，会让数据库产生更多的文件碎片，当碎片达到一定程度的时候，数据库就有崩溃的危险。比如mysql服务器最好是按其正常的程序关闭，而不是用pkill mysqld 或killall mysqld这样危险的动作；当然对于占用资源过多的数据库子进程，我们应该用kill来杀掉。
*** /proc file system
     cat /proc/cpuinfo
     cat /proc/meminfo
     cat /proc/zoneinfo
     cat /proc/mounts
*** nagios
     server and network monitoring
     monitor all your hosts, network equipment and services. It can send alert when things go wrong and again when they get better.
*** cacti
     web-based monitoring tool
*** KDE system guard
*** Gnome system monitor
** grep
grep [pattern] [place]
grep -n 输出行号
-v invert match
-q 什么也不写quiet
-L --files-without-match
*** grep "tab"                                                    :unknown:
Control-v i
or use perl style regular expression by 
grep -P '\t' *

just use grep "<Ctrl+V><TAB>", it works (if first time: type grep " then press Ctrl+V key combo, then press TAB key, then type " and hit enter, voilà!)
ctrl+v is a REALLY BAD IDEA ! ... yes it may work from console command, but it may NOT WORK TO TYPE IT IN A SCRIPT (you are at the mercy of the editor, for example i use mcedit and ctrl+v DON'T work there)
*** match as a word with "grep -w"
-w match as a word (Select only those  lines  containing  matches  that  form  whole words
*** display only file names which matches with "grep -l"
-l --files-with-match
*** display only the matched string with "grep -o"
#+BEGIN_SRC sh
grep -o "is.*line" demo_file
#+END_SRC
*** invert match with "grep -v"
#+BEGIN_SRC sh
grep -v '00001\|00002' file1  # 写不含00001和00002的行。
#+END_SRC
*** count number of matches with "grep -c"
*** display lines which does not match all the given pattern
#+BEGIN_SRC sh
grep -v -e "pattern1" -e "pattern2" testfile.txt
#+END_SRC
*** grep recursively with "grep -r"
*** display the position of match in file with "grep -b"
#+BEGIN_SRC sh
grep -o -b "3" temp_file.txtx
#+END_SRC
*** find+grep
#+BEGIN_SRC sh
  find . -name '*model*' -exec grep -n 'gene_table' {} \;
#+END_SRC
the previous is equivalent to
#+BEGIN_SRC sh
  grep 'gene_table' `find . -name '*model*'` ## note the antiquote
#+END_SRC
And for some reason, the second command has colorfull output.

*** other examples
#+BEGIN_SRC sh
  grep '^EQUATION\|^ENZYME' file1 >> file2
  file -Lz * | grep ASCII | cut -d":" -f1 | xargs ls -ltr
  file * | grep ASCII | cut -d":" -f1  | xargs wc -l
  wc -l ‘file * | grep ASCII | cut -d":" -f1 | grep ASCII | cut -d":" -f1‘
  file * | grep ASCII | cut -d":" -f1 | xargs vi
  file * | grep ASCII | cut -d":" -f1 | xargs -p vi
  file * | grep SSSSSS | cut -d":" -f1 | xargs -t -r wc -l
  file * | grep ASCII | cut -d":" -f1 | xargs -t -n2 ls -ltr
#+END_SRC

*** display N lines after match with "grep -A"
#+BEGIN_SRC sh
# grep -A <N> "string" FILENAME
grep -A 3 -i "example" demo_text
#+END_SRC

*** display N lines before match with "grep -B"
#+BEGIN_SRC sh
# grep -B <N> "string" FILENAME
grep -B 2 "single WORD" DEMO_TEXT
#+END_SRC

*** display N lines around (before and after) match with "grep -C"
#+BEGIN_SRC sh
grep -C 2 "Example" demot_text
#+END_SRC

*** 查看当前目录文件数
     ls -1 | wc -l
     ls -l | grep -v ^l | wc -l (excluding symbolic links)
*** how to judge based on grep's exit code?
**** grep -q PATTERN file.txt
grep returns a different code if it found something (zero) vs. if it hasn't found anything (non-zero). In an if statement, a zero exit code is mapped to "True" and a non-zero exit code is mapped to "False". In addition, grep has a -q argument to not output the matched text.
#+begin_src bash
if grep -q PATTERN file.txt; then
echo found
else
echo not found
fi
#+end_src

**** grep -c PATTERN file.txt
grep -c returns the count of the matched string.
grep -cm1 return 1 regardless the amount of found matches
** regular expression:
    regular expression special characters: .*?+[]{}()^$|\
    \Q turns off special meanings
    \E turns on special meanings
    e.g. if (\Q$pattern\E/) {...}
    ^ is the beginning of the string, while $ is the end of the string.
    character classes: w[aoi]nder
    everything except: th[^eo]
    character range: [0-9], [a-z]
    To match a single hexadecimal digit: [0-9A-F] or [0-9A-Fa-f] if you want to include lowercase.
    \d: [0-9]
    \w: [0-9A-Za-z_]
    \s: [\t\n\r]
    .:any single character
    \D: [^0-9] non-digit
    \W: [^0-9A-Za-z] non-word character
    \S: [^\t\n\r] non-blank character

    \b: b for boundary
    e.g. To find a five-letter word in the middle of a sentence: \s\w\w\w\w\w\s
    e.g. To find a five-letter word anywhere:\b\w\w\w\w\w\b or \b\w{5}\b
    e.g. r..h

    POSIX:
    [[:alpha:]]: [a-zA-Z]
    [[:alnum:]]: [0-9A-Za-z]
    [[:digit:]]: \d
    [[:lower:]]: [a-z]
    [[:upper:]]: [A-Z]
    [[:punct:]]: [!"#$%&'()*+,-./:;<=>?@\[\\\]^_`{|}~]

    For Unicode standard: \p{IsUpper}
    Alternative: | e.g. yes|maybe
    e.g. ye(s|t),(this)|(that)|(the other),th(is|at|e other)
    e.g. (the(\s[a-z]))|or matches the '(\s|[a-z])' or 'or'. In fact, (\s|[a-z]) can be replaced by [\sa-z]
    ? means the immediately preceding character(s) or metacharacter(s)-may appear zero times or once. e.g. \bs?he\b means 'he' or 'she'
    e.g. what the Entish (word )?is means either 'what the Entish is' or 'what the Entish word is' Note that the two spaces in front of and behind 'word' are important.
    + :e.g. To match an entire word:\b\w+\b
      *: means the preceding character exist any number of times including zero one or many. e.g. ^\s*[A-Z] the start of the string then any number of whitespace, then a capital.
      Summary: /bea?t/ Matches either 'beat' or 'bet'
      Summary: /bea+t/ Matches 'beat','beaat','beaaat'...
      Summary: /bea*t/ Matches 'bet','beat','beaat'...
      Note: .* and .+ in the middle of a regular expression will match as much of the string a they possibly can.

      well-defined repetition: \s{2,3} means whitespace repeated for 2 to 3 times. {3,} means more than 3 times, {,2} means fewer than 2 times.

      if you change the delimiters on //, you must put an m in front of it.(m for 'match') e.g. one may write:
      s#/usr/local/share/#/usr/share/#g;
      for substituting: s/\/usr\/local\/share\//\/usr\/share\//g;
      Modifiers:
      /m: treat the string as multiple lines.
      \s: treat the string as a single line. when /s is given, . will match a new line.
      \G: placing at the beginning of the regexp will anchor it to the end point of the last match.
      /x: allow the use of whitespace and comments inside a match.

      Good way of writing regular expression:
      Time in $1,machine name in $2,text in $3, note time is like: 14:52:34
      /^([0-2]\d:[0-5]\d:[0-5]\d)\s+\[([^\]]+)\]\s+(.*)$/
      Another clear way to write is:
      /^
      (         # First group: time
      [0-2]\d
      :
      [0-5]\d
      :
      [0-5]\d
      )
      \s+
      \[       # Square bracket
      (    # Second group:machine name
      [^\]]+			# Anything that isn't a square bracket
      )
      \]
      \s+
      (				# Third group:everything else
      .*
      )
      $/x
      The Third way to write this is:
      my $time_re='([0-2]\d:[0-5]\d:[0-5]\d';
      my $host_re='\[([^\]]+)\]';
      my $mess_re='(.*)';

      Rule One: Once the engine starts matching, it will keep matching a character at a time for as long as it can. Once it sees something that doesn't match, it has to stop.

      Rule Two: The engine is eager. It's eager to start work and eager to finish, and it starts matching as soon as possible in the stirng.

      Rule Three: The engine is greedy. If you use the + or * operators, they will try and steal as much of the string as possible.If the rest of the expression does not match it grudgingly gies up a character at a time and tries to match again, in order to find the fullest possible match. We can turn a RegExp into a non-greedy one by adding ?. e.g. ([a-z]+)(.*?)([a-z]+), ([a-z]+?)(.*?)([a-z]+?)


      Rule Four: The regular expression engine hates decisions. If there are two branches, it will always choose the first one, even though the second one might allow it to gain a longer match.

      To summarize: The regular expression engine starts as soon as it can, grabs as much as it can, then tries to finish as soon as it can, while taking the first decision available to it.


      inline comment (?#) pattern: e.g. /^Today's (?# This is ignored, by the way)date:/
      inline modifier: (?i): for case insensitive.e.g. (?!)one way to do it!
      To turn off the inline modifier temporarily: (?-i) for case sensitive.e.g. 
      /There's More Than ((?-i))One Way) To Do It!/i

      (?: ) Grouping without backreferences: /(?:X-)?Topic: (\w+)/; will have $1 always for \w+
      Note that for indeterminate quantifier like .,?,and * are NOT allowed in the lookahead and lookbehind assertion.
      To match inside the assertion, one needs to replace $1,$2,...with \1,\2...since $1,$2.. are only set after the match is complete.
      grep reverse ./*	find the string "reverse" in all the file under .
      apropos topic(list of command that has something to do with my topic)
      cat /proc/cpuinfo
      cat /proc/interrupts
      cat /proc/version
      cat /proc/filesystems
      cat /etc/printcap
      <Ctrl><Alt><Pg-Up> shift between tabs of gedit.

      system info: pwd whoami hostname id date time(the amount of time to do some command, e.g. "time ls" shows the time for ls), rwho -a(all the users logged on the network),finger <username>(system info about a user) last(list of users last logged in your system),history|more(show the last 1000 command you typed),history -c(clear the command history),uptime(last time since the last reboot), uname -a(unix name info) free(memory info) /sbin/lsmod(show the kernel modules that are currently loaded) set|more(show the current user environment) dmesg|less(Use less /var/log/dmesg  to see what "dmesg" dumped into this file right after the last system bootup.)

      du / -bh | more
      (=disk usage) Print detailed disk usage for each subdirectory starting at the "/" (root) directory (in human legible form).
      du -sh: print out the disk usage of all."s" for "summary"
      
      ps
      (=print status) List the processes currently run by the current user.

      ps axu | more
      List all the processes currently running, even those without the controlling terminal, together with the name of the user that owns each process.
      
      top
      Keep listing the currently running processes, sorted by cpu usage (top users first). In KDE, you can get GUI-based Ktop from "K"menu under "System"-"Task Manager" (or by executing "ktop" in an X-terminal).

      <MiddleMouseButton>
      Paste the text which is currently highlighted somewhere else. This is the normal "copy-paste" operation in Linux.  (It doesn't work with Netscape and WordPerfect which use the MS Windows-style "copy-paste". It does work in the text terminal if you enabled "gpm" service using "setup".) Best used with a Linux-ready 3-button mouse (Logitech or similar) or else set "3-mouse button emulation").

      <Ctrl>s
      Stop the transfer to the terminal.
      
      <Ctrl>q
      Resume the transfer to the terminal. Try if your terminal mysteriously stops responding.

      reset
      Restore a screwed-up terminal (a terminal showing funny characters) to default setting. Use if you tried to "cat" a binary file. You may not be able to see the command as you type it.

      <Ctrl>d
      Log out from the current terminal.  See also the next command.

      <Ctrl><Alt><Del>
      Shut down the system and reboot. This is the normal shutdown command for a user at the text-mode console. Don't just press the "reset" button for shutdown!

      <Ctrl><Alt><BkSpc>
      (in X-windows) Kill the current X-windows server. Use if the X-windows server crushes and cannot be exited normally.
      
      <Shift><PgUp>
      Scroll terminal output up. Work also at the login prompt, so you can scroll through your bootup messages.

      tty: print the name of the terminal in which you run this command.
      
      MANPATH: environment variable for the location of the manual page.
      
      filespace usage:
      du -h: -h is for human readable unit(M,G,K etc.).
      system disk space usage:
      df -h
      (=disk free) Print disk info about all the filesystems (in human-readable form)

      ln -s target_file symbolic_link要求绝对路径

      umask看当前umask值，umask 022设定新的umask值
      计算umask值代表的意义：先记下777在除掉umask值，再将文件权限各位去掉1（去掉执行权限）
      
      login shell用.bash_profile或.profile
      nonlogin shell用.bashrc

      chown改变文件属主
      chown -h wanding helloworld.f90 其中-h表示不影响符号连接指向的目标文件
      
      id wanding和groups wanding用来查询所属目标用户的用户组

      chgrp改变文件所属的用户组

      ls -l | grep '^...s' 查找suid	4
      ls -l | grep '^...s..s' 查找guid	2
      suid+guid	6

      s代替x出现表示设置了suid或guid

      目录的权限位：r:可列文件 w:可创建或删除 x:可进入或搜索 去掉x可使别的用户访问受限

      chmod -R 664 /usr/local/home/dave/* 连同子目录一起设置
** systems
*** free
free displays the total amount of free and used physical and swap
memory in the system, as well as the buffers used by the kernel.
display free memory size in MB
#+BEGIN_SRC sh
free -m
#+END_SRC

display free memory size in GB
#+BEGIN_SRC sh
free -g
#+END_SRC

display total memory (after summing swap and ram)
#+BEGIN_SRC sh
free -t
#+END_SRC

Note that the second line is what I want
#+BEGIN_EXAMPLE
             total       used       free     shared    buffers     cached
Mem:             5          3          2          0          0          1
-/+ buffers/cache:          1          4
Swap:            9          0          9
#+END_EXAMPLE
In this case, 4Gb is free and 1Gb is used.

update memory info every 2 seconds
#+BEGIN_SRC sh
free -s 2
#+END_SRC

update memory info 5 times each lasting 2 seconds
#+BEGIN_SRC sh
free -c 5 -s 2
#+END_SRC

*** vmstat
vitual memory statisitics
also displays 1) process 2) memory 3) paging 4) block I/O 5) traps 6) disks 7) cpu activity

Display memory utilization slabinfo
#+BEGIN_SRC sh
vmstat -m
#+END_SRC

Get information about active / inactive memory pages
#+BEGIN_SRC sh
vmstat -a
#+END_SRC

Update every 1 sec for 10 times (delay, count)
#+BEGIN_SRC sh
vmstat 1 10
#+END_SRC

*** iostat
average CPU load and disk activity
system input/output device loading by observing the time the devices are active in relation to their average transfer rates. For example, it is useful to monitor disk throughput.
*** atop and htop
both are non-default but great tools
*** sar
find out what linux is doing all the time. it can generate report and email them to sys admin

it displays:
cpu and queue
disk I/O
swap and memory
cpu interrupts

collect, report, and save system activity information
#+BEGIN_SRC sh
sar -n DEV | more    # see the network counter
sar -n DEV -f /var/log/sa/sa24 | more # to display the network counters from the 24th
#+END_SRC

display real time usage using sar:
#+BEGIN_SRC sh
sar 4 5
#+END_SRC
*** mpstat
     displays activities for each available processor, processor 0 being the first one. 
     mpstat -P ALL to display average CPU utilization per processor.
*** pmap
     report memory map of a process. Use this command to find out causes of memory bottlenecks
     pmap -d 47394 # display process memory information for pid 47394
     ===============================================================
     ......
     mapped: 933712K    writeable/private: 4304K    shared: 768000K
     ===============================================================
     The last line is very important:

     • mapped: 933712K total amount of memory mapped to files
     • writeable/private: 4304K the amount of private address space
     • shared: 768000K the amount of address space this process is sharing with
     others
*** netstat and ss
     network satistics, network connections, routing tables, interface statistics, masquerade connections, and multicast memberships. ss command is used to dump socket statistics. It allows showing information similar to netstat. 
*** iptraf
     需要安装 interactive colorful IP LAN monitor. It is an
     ncurses-based IP LAN monitor that generates various network statistics
     including TCP info, UDP counts, ICMP and OSPF information, Ethernet load info,
     node stats, IP checksum errors, and others. It can provide the following info
     in easy to read format. 
*** tcpdump
     It provides detailed network traffic analysis. The tcpdump is simple command that dump traffic on a network. However, you need
     good understanding of TCP/IP protocol to utilize this tool. For.e.g to display
     traffic info about DNS, enter:
     # tcpdump -i eth1 'udp port 53'
     To display all IPv4 HTTP packets to and from port 80, i.e. print only packets
     that contain data, not, for example, SYN and FIN packets and ACK-only packets,
     enter:
     # tcpdump 'tcp port 80 and (((ip[2:2] - ((ip[0]&0xf)<<2)) - ((tcp[12]&0xf0)>>
     2)) != 0)'
     To display all FTP session to 202.54.1.5, enter:
     # tcpdump -i eth1 'dst 202.54.1.5 and (port 21 or 20'
     To display all HTTP session to 192.168.1.5:
     # tcpdump -ni eth0 'dst 192.168.1.5 and tcp and port http'
     Use wireshark to view detailed information about files, enter:
     # tcpdump -n -i eth1 -s 0 -w output.txt src or dst port 80
*** strace
     Trace system calls and signals. This is useful for debugging webserver and
     other server problems.
*** disk usage analysis with df/du/ncdu
du -sh *
du -h --max-depth=0 * | sort -n | less
**** df
df -h
-h for “human readable form”
Filesystem            Size  Used Avail Use% Mounted on 
/dev/sda1             224G  135G   78G  64% / 
udev                  995M  4.0K  995M   1% /dev 
tmpfs                 401M  1.1M  400M   1% /run 
none                  5.0M     0  5.0M   0% /run/lock 
none                 1002M  1.7M 1001M   1% /run/shm 

find your current format and partition:
df -T

**** du
du -sh ~
-s for summary
-h for human readable form
26G /Users/MacUser

du -s -m *
-m for listing by MB

alias largest='du –max-depth=1 2> /dev/null | sort -n -r | head -n20'
--max-depth=1 the first level sub{items, directories}.
To see the 20 largest directories.

Or you can use
du -h –max-depth=1
to see the results directly

du -bs
-b: display in bytes
-s: summarize

du -b --total
--total : output counting just the files within each directory

**** ncdu
NCDU is for analyzing disk usage. The size of a directory can be misleading (a little larger than the actual size) due to defragmentation. If you want to look for difference between files/directories, use diff or rsync.

ncdu [dirname]
s: Order by filesize (press again for descending order)
i: Show information about the current selected item. (press again to hide)
d: delete file

ncdu -x /
scan a full filesystem, your root filesystem

ncdu -lxo / | gzip >export.gz
# ...some time later:
zcat export.gz | ncdu -f-

ncdu -o- | tee export.file | ./ncdu -f-
export a directory and browse it once scanning is done
*** see the terminal key setting
比如那个是EOF
#+BEGIN_SRC bash
stty -a
#+END_SRC
*** cpu info and memory info
less /proc/cpuinfo
less /proc/meminfo
*** check distro information
less /proc/version
*** memory information
Display RAM information
free -m
the “-m” displays the output in megabytes which makes it easier for me.  The default is to display in kilobytes.

Dmidecode will grab all of your system details, not just memory and it will give you a very detailed report about your hardware without you having to crack the case open.  To check just the memory, you can do the following:
sudo dmidecode –type 17 | less

sudo lshw -short -C memory
*** find your current format and partition:
df -T

*** list all devices on your system
dmesg

*** list USB devices
sudo fdisk -l
or in short
fdisk -l | grep '^Disk'
*** 字体配置
     fontconfig 
     两个重要目录
     /etc/fonts/conf.d/
     和
     /etc/fonts/conf.avail/
     The fontconfig-config package was responsible for creating the sym links in your /etc/fonts/conf.d/ directory from the available options in /etc/fonts/conf.available/. Performing these steps manually is easy enough.

     For example, unlike firefox-3.0, firefox-3.5 takes its font hinting orders from fontconfig, instead of gnome's font settings. My desktop uses medium, subpixel hinting in gnome, but my firefox fonts were using slight, greyscale hinting (the fontconfig default). To fix this, I did this:
     sudo rm /etc/fonts/conf.d/10-hinting-slight.conf
     sudo rm /etc/fonts/conf.d/10-no-sub-pixel.conf
     sudo ln -s /etc/fonts/conf.available/10-hinting-medium.conf /etc/fonts/conf.d/.
     sudo ln -s /etc/fonts/conf.available/10-sub-pixel-rgb.conf /etc/fonts/conf.d/.
     sudo dpkg-reconfigure fontconfig
*** manpage
col
convert man page to text
man bash | col -b | lpr
**** create a manpage
***** groff
       GNU troff is the program that reads the man page.
***** man example
       .\" Manpage for nuseradd.
       .\" Contact vivek@nixcraft.net.in to correct errors or typos.
       .TH man 8 "06 May 2010" "1.0" "nuseradd man page"
       .SH NAME
       nuseradd \- create a new LDAP user
       .SH SYNOPSIS
       nuseradd [USERNAME]
       .SH DESCRIPTION
       nuseradd is high level shell program for adding users to LDAP server.  On Debian
       .SH OPTIONS
       The nuseradd does not take any options. However, you can supply username.
       .SH SEE ALSO
       useradd(8), passwd(5), nuseradd.debian(8)
       .SH BUGS
       No known bugs.
       .SH AUTHOR
       Vivek Gite (vivek@nixcraft.net.in)

***** set up MANPATH
       in /etc/man.config file
       MANPATH /usr/man
       MANPATH /usr/share/man
       MANPATH /usr/local/man
       MANPATH /usr/local/share/man
       MANPATH /usr/X11R6/man
       
       see man manpath for detail

***** manpage macros
       see "man 7 mdoc" command

***** install a manpage
       cp nuseradd /usr/local/man/man8/nuseradd.1
       gzip /usr/local/man/man8/nuseradd.1
       man nuseradd
       Or use the install command as follows
       install -g 0 -o 0 -m 0644 nuseradd.1 /usr/local/man/man8/
       gzip /usr/local/man/man8/nuseradd.1
*** x-window
**** how to 启动x-windows系统
startx -- :1
start another X-windows session on display 1.(the default is opened on display 0)
** user management
*** talk and write
     username
     talk to another user currently logged on your machine(or "tal username@machinename" to talk to a user on a different computer")
**** to refuse accepting messages.
mesg n
**** to allow accepting messages
mesg y
*** who and rwho
     who and rwho to determine the users who are currently logged in.
     ps2pdf, pdf2ps,pdftex,ps2eps	## conversion between different filetypes
     change prompt and add color:
     export PS1="\e[0;31m[\u@\h:\w]$\e[m"	##\u current user name
     ##\h hostname
     ##\w working directory
     ##\e[ start color scheme
     ##x;y coloar pair: x=0,y=30 for black,y=34 for blue,32 for green,36 for cyan 31 for red,35 for purple,33 or 34 for brown,34 for blue
     ##\e[m stop color scheme

     xlsfonts	## This displays system fonts
     xfd -fn fontname## This pops up a window displaying it
     xfd -fn $(xlsfonts | grep courier-medium-r | grep .-18- | grep 59-1$)

*** examine the log-in information
     look at 
     /var/log/auth.log file
     /var/log/auth.log.1 file is the log of yesterday.** adduser user_name
      Create a new account (you must be root). E.g.,  adduser barbara  Don't forget to set up the password for the new user in the next step. The user home directory is /home/user_name.
*** useradd user_name
     The same as the command " adduser user_name ".
*** userdel user_name
     Remove an account (you must be a root). The user's home directory and the undelivered mail must be dealt with separately (manually because you have to decide what to do with the files).
*** groupadd group_name
     Create a new group on your system. Non-essential but can be handy even on a home machine with a small number of users.
*** passwd
     Change the password on your current account. If you are root, you can change the password for any user using:  passwd user_name
*** chown
chown wanding:wanding somefile
*** chmod perm filename
     (=change mode) Change the file access permission for the files you own (unless you are root in which case you can change any file). You can make a file accessible in three modes: read (r), write (w), execute (x) to three classes of users: owner (u), members of the same group as the owner (g), others on the system (o). Check the current access permissions using:
*** chmod a+r junk
     This command will remove the permission to execute the file junk from others:
*** chmod o-x junk
     Also try here for more info.
     You can set the default file permissions for the news files that you create using the command umask (see man umask).
*** chgrp new_groupname filename
     Change the file owner and group. You should use these two commands after you copy a file for use by somebody else.
*** su
     (=substitute user id) Assume the superuser (=root) identity (you will be prompted for the password). Type "exit" to return you to your previous login. Don't habitually work on your machine as root. The root account is for administration and the su command is to ease your access to the administration account when you require it. You can also use "su" to assume any other user identity, e.g. su barbara will make me "barbara" (password required unless I am a superuser).
** kernel/hardware
*** mount
     See here for details on mounting drives.  Examples are shown in the next commands.
**** mount -t auto /dev/fd0 /mnt/floppy
      (as root) Mount the floppy. The directory /mnt/floppy must exist, be empty and NOT be your current directory.
**** mount -t auto /dev/cdrom /mnt/cdrom
      (as root) Mount the CD. You may need to create/modify the /dev/cdrom file depending where your CDROM is. The directory /mnt/cdrom must exist, be empty and NOT be your current directory.
**** mount /mnt/floppy
      (as user or root) Mount a floppy as user. The file /etc/fstab must be set up to do this. The directory /mnt/floppy must not be your current directory.
**** mount /mnt/cdrom
      (as user or root) Mount a CD as user. The file /etc/fstab must be set up to do this. The directory /mnt/cdrom must not be your current directory.
**** umount /mnt/floppy
      Unmount the floppy. The directory /mnt/floppy must not be your (or anybody else's) current working directory. Depending on your setup, you might not be able to unmount a drive that you didn't mount.
**** 挂载卸载光盘
      mount /dev/cdrom
      umount /media/cdrom0
      注意 /cdrom 和 /media/cdrom 是 /media/cdrom0 的链接
      弹出 eject /dev/cdrom
*** printer
**** lpc(as root)
      check and control the printer(s)
**** lpq
      show the content of the printer queue.
**** lprm job_numer
      Remove a printing job"job_number" from the queue.
*** format a filesystem
find your current format and partition:
df -T

see mount point information:
mount

first unmount
sudo umount /dev/sdb1

to format /dev/sdb1:
mkfs.ext4 /dev/sdb1
or
mke2fs -t ext4 /dev/sdb1

to mount a file system
mount /dev/sdb1 /wherever
or
mount -t ext4 /dev/sdb1 /wherever
(-t ext4 is needed only on old Oss)

list all devices on your system
dmesg

list USB devices
sudo fdisk -l
or in short
fdisk -l | grep '^Disk'

check label
sudo e2label /dev/sdb1
change label
sudo e2label /dev/sdb1 ExternalDrivLarg

*** kernelcfg
     (as root in X terminal). GUI to to add/remove kernel modules. You can do the same from the command line using the command "insmod", but "insmode" is less "newbie-friendly".
*** lsmod
     List currently loaded kernel modules. A module is like a device driver--it provides operating system kernel support for a particular piece of hardware or feature.
*** modprobe -l |more
     List all the modules available for your kernel. The available modules are determined by how your Linux kernel was compliled. Every possible module/feature can be compiled on linux as either "hard wired" (fast, non-removable), "module" (maybe slower, but loaded/removable on demand), or "no" (no support for this feature at all).
*** insmod parport
*** insmod ppa
(as root) Insert modules into the kernel (a module is roughly an equivalent of a DOS device driver). This example shows how to insert the modules for support of the external parallel port zip drive (it appears to be a problem to get the external zip drive to work  in any other way under RH6.0 ).
*** rmmod module_name
     (as root, not essential). Remove the module module_name from the kernel.
*** setserial /dev/cua0 port 0x03f8 irq 4
     (as root) Set a serial port to a non-standard setting. The example here shows the standard setting for the first serial port (cua0 or ttyS0). The standard PC settings for the second serial port (cua1or ttyS1) are: address of i/o port 0x02f8, irq 3. The third serial port (cua2 or ttyS2): 0x03e8, irq 4. The forth serial port (cua3 or ttyS3): 0x02e8, irq 3. Add your setting to /etc/rc.d/rc.local if you want it to be set at the boot time. See man setserial for good a overview.

*** depmod -a
     (as root) Build the module dependency table for the kernel. This can, for example, be useful after installing and booting a new kernel. Use "modprobe -a" to load the modules.

*** mknod /dev/fd0 b 2 0
     (=make node, as root) Create a device file. This example shows how to create a device file associated with your first floppy drive and could be useful if you happened to accidentally erase it. The options are: b=block mode device (c=character mode device, p=FIFO device, u=unbuffered character mode device). The two integers specify the major and the minor device number.
*** make xconfig
     (as root in X terminal). Nice GUI front-end for configuration of the kernel options in preparation for compilation of your customized kernel.  (The directory name contains the version of your Linux kernel so you may need to modify the directory name if your Linux kernel version is different than 2.0.36 used in this example. You also need the "Tk" interpreter and the kernel source code installed. ) The alternatives to "make xconfig" are: "make config"  (runs a scripts that asks you questions in the text mode) and "make menuconfig" (runs a text-based menu-driven configuration utility). Try: less /usr/doc/HOWTO/Kernel-HOWTO for more information.
     After the configuration,  you may choose to proceed with kernel compilation of the new kernel by issuing the following commands:
     make dep
     make zImage
     The last command will take some time to complete (maybe 0.5 h, depending on your hardware). It produces the file "zImage", which is your new Linux kernel. Next:
     make modules
     make modules_install
     Read: /usr/doc/HOWTO/Kernel-HOWTO for information on how to install the new kernel. You will probably also find it useful to read "man depmode". Configuration, compilation and installation of a new kernel is not difficult but it CAN lead to problems if you don't know what you are doing.
     Compilation of a kernel is a good way to test your hardware, because it involves a massive amount of computing. If your hardware is "flaky", you will most likely receive the "signal 11" error (read the beatiful /usr/doc/FAQ/txt/GCC-SIG11-FAQ). See this for details on kernel upgrade.

*** fdformat /dev/fd0H1440
*** mkfs -c -t ext2
     (=floppy disk format, two commands, as root) Perform a low-level formatting of a floppy in the first floppy drive (/dev/fd0), high density (1440 kB). Then make a Linux filesystem (-t ext2), checking/marking bad blocks (-c ). Making the files system is an equivalent to the high-level format.
*** badblocks /dev/fd01440 1440
     (as root) Check a high-density floppy for bad blocks and display the results on the screen. The parameter "1440" specifies that 1440 blocks are to be checked. This command does not modify the floppy.

*** fsck -t ext2 /dev/hda2
     (=file system check, as root) Check and repair a filesystem. The example uses the partition hda2, filesystem type ext2.
*** the nvdia-setting save problem
   1) open a terminal
   2) sudo mv -i /etc/X11/xorg.conf /etc/X11/xorg.conf.backup
   3) sudo touch /etc/X11/xorg.conf
   4) hit save configuration
or run sudo nvidia-xconfig
run sudo nvidia-setting
*** how to check SSD health
install smartmontools
sudo smartctl -a /dev/sdb
check this post
===
As you already know, SSDs degrade over time, as its memory cells support a limited amount of writes (a sacrifice that many people, including myself, find worthy in exchange to get rid of those slow and noisy mechanical disks).

This morning, while talking to a colleague, an interesting question was raised: how do we know when our SSD is near to day of its defunction?

Expensive, enterprise-grade SSD cards, like the ones from TMS, come with monitoring tools and a nice entry in "/proc" that can be easily checked by a script. But I had no idea if there's some similar for consumer SSDs.

Turned out it was quite easy. Most models provide health info via the S.M.A.R.T. feature, so you can obtain it with smartctl (on Fedora, this utility came into the package smartmontools):

[slopez@slp-work ~]$ sudo smartctl -a /dev/sda
(...)
SMART Attributes Data Structure revision number: 1
Vendor Specific SMART Attributes with Thresholds:
ID# ATTRIBUTE_NAME FLAG VALUE WORST THRESH TYPE UPDATED WHEN_FAILED RAW_VALUE
9 Power_On_Hours 0x0032 099 099 --- Old_age Always - 23
12 Power_Cycle_Count 0x0032 099 099 --- Old_age Always - 34
177 Wear_Leveling_Count 0x0013 099 099 --- Pre-fail Always - 1
178 Used_Rsvd_Blk_Cnt_Chip 0x0013 077 077 --- Pre-fail Always - 458
190 Airflow_Temperature_Cel 0x0022 067 051 --- Old_age Always - 33
235 Unknown_Attribute 0x0012 099 099 --- Old_age Always - 10
(...)
The most relevant attribute while checking the health of our SSD, is Wear Leveling Count (more info on Wear Leveling on Wikipedia). But don't get fooled by its name, it's not really a count, but an indicator of how healthy are the cells in your disk, where 100 is the best, and 0 the worst.

When this value falls below 20, you should start considering backing up your data and buying a new disk.
===
** network
*** ssh/scp
**** ssh relay through proxy

in my ~/.ssh/config, put
#+BEGIN_EXAMPLE
Host dav
     User wz4
     HostName davinci.rice.edu
     ProxyCommand ssh wanding@wallace.cs.rice.edu nc %h %p
#+END_EXAMPLE
This is to tunnel to davinci through wallace

**** ssh-keygen and ssh config file
ssh-keygen 
ssh-keygen -t rsa
scp /home/wanding/.ssh/id_rsa.pub wz4@davinci.rice.edu:/home/wz4/.ssh

on remote server:
cd ~/.ssh
cat id_rsa.pub >>authorized_keys

.ssh/config file
see the following tutorial
http://nerderati.com/2011/03/simplify-your-life-with-an-ssh-config-file/
e.g., 
–
Host ams
     HostName ec2-107-22-102-40.compute-1.amazonaws.com
     Port 22
     User ubuntu
     IdentityFile ~/Dropbox/personal/AMS/wandingcloud.pem
–

remove old host information
ssh-keygen -f "/home/wanding/.ssh/known_hosts" -R mammoth.rice.edu
**** my setup for dqsfacpriv03 passwordless ssh
ssh-keygen -t rsa
scp ~/.ssh/id_rsa.pub wzhou1@dqsfacpriv03.mdanderson.edu:/home/wzhou1/.ssh (need to mkdir .ssh on the remote site)
cd .ssh
rm known_hosts
echo "StrictHostKeyChecking no" >>config

on the remote side
cat id_rsa.pub >>authorized_keys
cat id_rsa.pub >>authorized_keys2
both lines are necessary to ensure compability of ssh1.5 and openssh2.0 protocols** netconf
   (as root) A very good menu-driven setup of your network.
**** troubleshoot
http://askubuntu.com/questions/69433/ssh-no-longer-allows-public-key-authentication
make sure that on the server side, your home directory (~), the ~/.ssh directory, and the ~/.ssh/authorized_keys file, are all writable only by their owner. In particular, none of them must be writable by the group (even if the user is alone in the group). chmod 755 or chmod 700 is ok, chmod 770 is not.
*** ping machine_name
Check if you can contact another machine (give the machine's name or IP), press <Ctrl>C when done (it keeps going).
ping tells if a certain host is reachable or not from your web
to ping a router: ping 192.168.1.1
to ping a website: ping www.ebay.com
use Ctrl C to end ping
the lower the round trip number in millisecond, the better, the higher the round trip number the higher the lacency, which may indicates a network problem between your computer and the server you pinged.
TTL: time to live. this indicates the amount of time after which a package would be discarded after staying in the web.
*** route -n
     Show the kernel routing table.
*** nslookup host_to_find
     Query your default domain name server (DNS) for an Internet name (or IP number) host_to_find. This way you can check if your DNS works. You can also find out the name of the host of which you only know the IP number.
*** traceroute host_to_trace
     Have a look how you messages trave to host_to_trace (which is either a host name or IP number).
*** ipfwadm -F -p m
     (for RH5.2, seen next command for RH6.0) Set up the firewall IP forwarding policy to masquerading. (Not very secure but simple.) Purpose: all computers from your home network will appear to the outside world as one very busy machine and, for example, you will be allowed to browse the Internet from all computers at once.
*** echo 1 > /proc/sys/net/ipv4/ip_forward
*** ipfwadm-wrapper -F -p deny
*** ipfwadm-wrapper -F -a m -S xxx.xxx.xxx.0/24 -D 0.0.0.0/0
     (three commands, RH6.0). Does the same as the previous command. Substitute  the "x"s  with digits of your class "C" IP address that you assigned to your home network. See here for more details. In RH6.1, masquarading seems broken to me--I think I will install Mandrake Linux:).
*** ifconfig
     (as root) Display info on the network interfaces currently active (ethernet, ppp, etc). Your first ethernet should show up as eth0, second as eth1, etc, first ppp over modem as ppp0, second as ppp1, etc. The "lo" is the "loopback only" interface which should be always active. Use the options (see ifconfig --help) to configure the interfaces.
*** ifup interface_name
     (/sbin/ifup to it run as a user) Startup a network interface. E.g.:
*** ifup eth0
*** ifup ppp0
     Users can start up or shutdown the ppp interface only when the right permission was checked during the ppp setup (using netconf ). To start a ppp interface (dial-up connection), I normally use kppp available under kde menu "internet".
*** ifdown interface_name
     (/sbin/ifdown to run it as a user). Shut down the network interface. E.g.: ifdown ppp0 Also, see the previous command.
*** netstat | more
     Displays a lot (too much?) information on the status of your network.
*** wireless card troubleshooting
**** solve the wireless card problem of HP desktop
      http://askubuntu.com/questions/38143/ralink-5390-card-in-laptop-does-not-work-after-installing-driver  
**** wireless network debug
**** I - Learning the basics of your Wireless Device

      Discovering the chipset of the wireless device is the first step in installation. 
      For networking cards or internal wireless devices (even those not connected to the internet or without a driver installed), at the command line type:
      Code:
      lspci -nn
      This command lists (ls) devices connected to the pci (pci) bus (Hence the command lspci).
      
      For USB wireless dongles, there is an equivalent lsusb command, however issuing this command does not reveal the wireless chipset of the device. Often times you will either have to discover the chipset by either consulting the documentation that comes with the device, or googling the device to discover the chipset.

**** II - Setting up your Network Driver

      Unlike windows plug and play system were many network drivers are shipped with the operating system, ubuntu often requires installation of networking drivers. As of 7.10 there a few open source network drivers that are "built into the operating system" -- These are the following:
      a. Ra chipset drivers - rt2500, rt61, rt73, rt2400, rt2570
      b. Broadcom chipset - the bcm43xx driver
      c. Orinico, prism devices
      d. Certain Atheros chipsets - via the madwifi drivers - consult the madwifi website directly to see if your atheros chipset is supported: http://madwifi.org/wiki/Compatibility. 
      
      Broadcom chipsets -- in order to successfully use the built-in bcm43xx restricted driver, a working internet connection is required -- in most cases a wired connection. Ndiswrapper is another (and in my opinion a preferred) installation method
      Atheros chipset - the madwifi drivers can be installed via synaptic and searching for the linux restricted package.
      Ra chipsets - In many cases I'm finding the built-in drivers shipped in Ubuntu 7.10 are often problematic. If problematic, compilation and installation of the serial monkey drivers for ra based chipsets, or installation via ndiswrapper are two alternative installation methods.
      
      In cases where the wireless driver is not contained in contained in the default installation, or in cases where the built-in drivers do not function appropriately, drivers either need to be:
      Downloaded
      Downloaded, compiled and installed from source
      Wiindows drivers installed via the ndiswrapper tool
      
      Below is a list of posts that I have found very useful in my experience to aid users in alternative wireless driver installation.
      
      Ra chipsets - rt2500, rt73, rt61, rt2570 drivers - http://ubuntuforums.org/showthread.p...=serial+monkey - Author diepruis
      Rt2500 driver installation in Gusty http://ubuntuforums.org/showthread.php?t=584657 - Author - zoiks
      Ndiswrapper installation for Broadcom chipsets - http://ubuntuforums.org/showthread.php?t=475963 - Author Jamie Jackson
      Ndiswrapper General Installation Guide - SVN, Troubleshooting Tips (My Personal Guide) - http://ubuntuforums.org/showthread.php?t=574501 - Author KevDog
      Madwifi website for certain Atheros Chipsets - http://madwifi.org/ -- If your Atheros chipset is listed on this website - it should work out of the box with installation of the linux restricted drivers package for your kernel version
      Realtek win98 driver - http://www.majorgeeks.com/Realtek_RT...0XP_d5165.html - For use with ndiswrapper if native r818x, r8187 driver is buggy
      Realtek win98 driver installation - http://ubuntuforums.org/showthread.p...highlight=8187 - Author Panurge
      Realtek - Installation with Native Driver - http://ubuntuforums.org/showthread.php?t=567505

**** III - Using your Wireless Card

      Once wireless drivers are installed, often times a wireless GUI is needed to configure the wireless device and connect to the wireless network. 

      Network Manager is the default wireless GUI shipped by default with Ubuntu. In some cases however network manager does not function appropriately with certain setups (ra based chipsets). Alternatives do exist and are oftentimes necessary. These alternatives include

      The command line (configuring and connecting manually) - For all chipsets
      WICD - For all chipsets. For any WICD installation I recommend installing the latest testing version over the stable version
      Rutilt - For ra based chipsets

      References:
      Connecting Wireless at the Command Line - http://ubuntuforums.org/showthread.php?t=571188 - Author Kevdog
      Rutilt - A Network Manager Like GUI for Ra Chipsets - http://ubuntuforums.org/showthread.p...ghlight=rutilt - Author sulilogs
      WICD http://wicd.sourceforge.net/download.php

**** IV - Wireless Security

      WEP - In most cases WEP is built-in to the wireless management GUI. 
      WPA - For cases other than ra-based chipsets, the wpasupplicant package needs to be installed (via synaptic, apt, aptitude) prior to utilizing WPA.
      
**** V - Helpful Command-Line Wireless Commands

      ifconfig - lists IP address (similar to ipconfig in Windows)
      iwlist scan - shows wireless networks that are available in the area along with basic encryption information
      iwconfig wlan0 - do some work, but don't know exactly what it is.
      lshw -C network - Shows interface and driver associated with each networking device
      lspci -nn - Shows hardware connected to the pci bus
      lsusb - Shows USB connected hardware
      lshw -C usb - Additional info on USB related hardware (good for USB dongles)
      cat /etc/modprobe.d/blacklist - List modules that will not be loaded by the Operating System at boot time
      lsmod - lists currently loaded kernel modules. (Example usage - lsmod | grep ndiswrapper)
      route -n - Lists kernel IP routing table -- Good for troubleshooting problems with the gateway (netstat -rn = equivalent command)
      sudo route add default gw 192.168.1.1 - Example of how to set the default gateway to 192.168.1.1 
      sudo route del default gw 192.168.1.1 - Example of how to delete the default gateway setting
      sudo modprobe ***** - Loads the kernel module **** . (Example usage - sudo modprobe ndiswrapper, sudo modprobe r818x, sudo modprobe ath_pci)
      sudo modprobe -r **** - Unloades the kernel module ****. (Example usage - sudo modprobe -r ndiswrapper)
      sudo ifup/ifdown <interface> - Brings up/down the interface and clears the routing table for the specified interface
      sudo ifconfig <interface> up/down - Brings up/down the interface for the specified interface 
      sudo dhclient <interface> - Request IP address from DNS server for specified interface
      sudo dhclient -r <interface> - Release IP address associated with specified interface
      sudo iptables -L - Lists firewall rules 
      dmesg | more - Lists boot log -- good for troubleshooting problems with modules/drivers not being loaded
      uname -r - Displays kernel version
      /etc/iftab (Feisty and pre-releases (Edgy, etc)) - /etc/udev/rules.d/70-persistent-net.rules (Gutsy) - File which assigns logical names (eth0, wlan0, etc) to MAC addresses
      cat /etc/resolv.conf - Lists DNS servers associated with network connections (Network Manager)
      /etc/dhcp3/dhclient.conf - File which sets or modifies dns (domain name servers) settings

**** VI- Other Useful Guides

      DNS related problems?? - Configuration for OpenDNS servers - http://ubuntuforums.org/showthread.php?t=543659 - Author noob12
      Turn off/Disable IPv6 - http://ubuntuforums.org/showthread.php?t=6841 - Author mark
      Quit Having Network Manager asking for a Password to Connect to Known Wireless Networks - http://ubuntuforums.org/showthread.p...o+login&page=8 - Author Blueshift
* AWK
** how to add commas and convert column to row
# add commas and print in one line
echo $(awk 'NR > 1{print line", "}{line=$0;}END{print $0" "}' file)
I Love Bash, I Love Bash, I Love Bash

** source awk library and mingle with command line
*** -f filename -e '' [only AFTER awk4.01]
awk -f [library_path] -e [command_line]
E.g.,
awk -f /home/wanding/wzlib/wanding.awk -F"\t" -e '$9!="."{match($9, /^(\S+) \(([+-]), coding\)/, a); match($3, /ins([ATGC]+)/, b); print $3"\t"$5"\t"a[2]"\t"b[1]"\trev\t"dnarev(b[1])}' cosmic_insertion_tnuc_revanno 
*** setup AWKPATH
AWKPATH is the environmental variable where awk will look for files
*** '@include "filename";'
in .bashrc
export AWKPATH=~/.awk
in ~/.awk create awkfun
add 
function abs(value) {
return (value<0?-value:value)
}

echo "-10" | awk '@include "awkfun"; {print $1, abs($1)}'
** awk match remember use // instead of ""
// is regular expression
"" is regarded as string
** gensub vs gsub vs sub                                          :memorize:
*** gensub(regexp, replacement, how [, target])
awk -v OFS="\t" '{$7=gensub(/([[:digit:].]*)%/, "\\1", 1, $7) / 100.0; print $0}' varscan_snp_cms46.overlap

*** sub(regexp, replacement, [, target])
search target which is treated as a string for the leftmost, longest substring matched by the regular expression regexp. Modify the entire string by replacing the matched text with replacement. The modified string becomes the new value of target. Return the number of substitutions made (zero or one). 

Example 1: 
str = "water, water, everywhere"
sub(/at/, "ith", str)
sets str to 'wither, water, everywhere'

Example 2:
the character '&' is special, means the substring that matches regexp
str = "daabaaa"
sub(/a+/, "C&C", str)
sets str to "dCaaCbaaa"
** awk uses pipe and redirection in print                          :unknown:
https://www.gnu.org/software/gawk/manual/html_node/Redirection.html
print items > "output-file"
print items >> "output-file"
print $1 | "sort -r > names.sorted"

** how to specify the maximum length in regular expression
[A-Z]{3,7} range from 3 to 7
[A-Z]{3,} minimum length 3
[A-Z]{,7} maximum length 7

** install a new version in a local path
wget http://git.savannah.gnu.org/cgit/gawk.git/snapshot/gawk-gawk-4.0.1.tar.gz
./configure --prefix /home/wzhou/tools/awk4.01/gawk-4.01-install
make
make install
** basics
#+BEGIN_SRC 
awk 'BEGIN {<initializations>} 
<search pattern 1> {<program actions>}
<search pattern 2> {<program actions>} 
...
END {<final actions>}'
#+END_SRC

** How to specify multiple field separators (FS)?
awk -F"[ ,]+" '/^Inference/{print $6}' RAxML_info.optimize
space and ',' are FS here
** What are the variants of regular expressions? How to use regular expressions?
# POSIX regular expression
#+BEGIN_SRC 
sed -n '/^st/,/^binaries/p' problem_moma.lp | awk '/v[[:digit:]]+w/'
#+END_SRC

# normal regular expression
#+BEGIN_SRC 
sed -n '/^st/,/^binaries/p' problem_moma.lp | awk '/v[0-9]+w/'
#+END_SRC

# Perl style regular expression such as '\d' is not usable in awk.
#+BEGIN_SRC 
awk '!/v[0-9]+( |$)/' problem.lp > problem3.lp
LC_ALL=en_US.utf8 awk --re-interval '{sub(/^([^[:space:]]+[[:space:]]+){3}/,"")}1' file
#+END_SRC

** string operation
*** Split
Split string into an array
split(s, a, sep)
splits the string s into array a using delimiter sep
*** How to extract substring using substr?
substr(s,a,b)
return the substring of length b of string s starting from a

** use RS
to pull out things
awk -v RS='=' '!(NR%2)'
# awk -v RS='=' '!(NR%2){gsub(/\n/," ");print}' # if you want to reformat embedded newlines

** use RS and RT
to parse <tag>something</tag>
After the end of the record has been determined, gawk sets the variable RT to the text in the input that matched RS
gawk -v RS='</?tag>' 'RT=="</tag>"'
gawk -v RS='</?tag>' '!(NR%2)'
gawk -v RS='--[0-9]+--' 'RT{gsub(/--/,"",RT);print RT}'

** sprintf
# input: 2.5943 10
awk '{$1=sprintf("%d",$1); # truncates decimals, but also explicitly turns $1 into a string!
if($1 > $2) print "something went wrong!" } # this is printed

** Force type conversion
var""
Force number to string, the "" forces awk to evaluate the variable as a string
awk -F ',' -v OFS=',' '{if ($4) $6="X"}1'
awk -F ',' -v OFS=',' '{if ($4"") $6="X"}1'

var+0
Force string to number
awk '/foo/{tot++} END{print tot}'
awk '/foo/{tot++} END{print tot+0}'

** check identity
Check if two files are same set of lines, assuming the lines do not have duplicates in their own files.
awk '!($0 in a) {c++;a[$0]} END {exit(c==NR/2?0:1)}' file1 file2

** Parse csv
see http://backreference.org/2010/04/17/csv-parsing-with-awk/
awk -F '^ *| *, *| *$' ...
awk -F '^ *"|" *, *"|" *$' ...

"field1 , field2 , field3 , field4"
# FS=','
for(i=1;i<=NF;i++){
gsub(/^ *| *$/,"",$i);
print "Field " i " is " $i;}

"field1","field2","field3","field4"
# FS=','
for(i=1;i<=NF;i++){
gsub(/^ *"|" *$/,"",$i);
print "Field " i " is " $i;}

field1, "field2,with,commas" , field3 , "field4,foo"
$0=$0","; # yes, cheating
while($0) {
match($0,/[^,]*,| *"[^"]*" *,/); 
sf=f=substr($0,RSTART,RLENGTH); # save what matched in sf
gsub(/^ *"?|"? *,$/,"",f); # remove extra stuff
print "Field " ++c " is " f;
sub(sf,""); # "consume" what matched}

** Locale
echo 'èòàù' | LC_ALL=en_US.utf8 awk '/[a-z]/'
will output èòàù

** Split files
first way, works with all versions of awk
awk -v n=1 '/^FOO[0-9]*/{close("out"n);n++;next} {print > "out"n}' file
-v n=1 is initialization.
another way, needs GNU awk
LC_ALL=C gawk -v RS='FOO[0-9]*\n' -v ORS= '{print > "out"NR}' file

** Interesting trick/tips link
http://www.catonmat.net/blog/ten-awk-tips-tricks-and-pitfalls/

** print a range of a file
*** prints lines from /beginpat/ to /endpat/, not inclusive
#+begin_src sh
awk '/beginpat/,/endpat/{if (!/beginpat/&&!/endpat/)print}'
#+end_src
*** prints lines from /beginpat/ to /endpat/, not including /beginpat/
#+begin_src sh
awk '/beginpat/,/endpat/{if (!/beginpat/)print}'
#+end_src
*** prints lines from /beginpat/ to /endpat/, not inclusive
#+begin_src sh
awk '/endpat/{p=0};p;/beginpat/{p=1}'
#+end_src
*** prints lines from /beginpat/ to /endpat/, excluding /endpat/
#+begin_src sh
awk '/endpat/{p=0} /beginpat/{p=1} p'
#+end_src
*** prints lines from /beginpat/ to /endpat/, excluding /beginpat/
#+begin_src sh
awk 'p; /endpat/{p=0} /beginpat/{p=1}'
#+end_src
*** prints lines from /beginpat/ to /endpat/, inclusive
#+begin_src sh
awk '/beginpat/{p=1};p;/endpat/{p=0}'
#+end_src

** toupper(str) and tolower(str)
only exist in gawk, convert str to upper/lower cases.
somecommand | awk 'NR>1 && /foo/{sub(/foo/,"bar"); print toupper($2)}'

** Process two files one by one.
prints lines that are both in file1 and file2 (intersection)
#+BEGIN_SRC
awk 'NR==FNR { # some actions; next} # other condition {# other actions}'
#+END_SRC

Use one file to make a map and substitute the other file:
#+BEGIN_SRC awk
awk 'NR==FNR{a[$0];next} $0 in a' file1 file2
#+END_SRC

read the same file twice
#+BEGIN_SRC awk
awk 'NR==FNR{a[$1]=$2;next} {$3=a[$3]}1' mapfile datafile
#+END_SRC

replace each number with its difference from the maximum
#+BEGIN_SRC awk
awk 'NR==FNR{if($0>max) max=$0;next} {$0=max-$0}1' file file
#+END_SRC

** awk special variables
FS: Input field separator
OFS: Output Field Separator
RS: Record Separator
ORS: Output Record Separator
NR: Number of Records
NF: Number of Fields
FILENAME: Name of the current input file
FNR: Number of Records relative to the current input file

** match and result fetching
#+BEGIN_SRC 
match(string, regexp, result_array)
#+END_SRC

If array is present, it is cleared, and then the 0th element of array is set to the entire portion of string matched by regexp. If regexp contains parentheses, the integer-indexed elements of array are set to contain the portion of string matching the corresponding parenthesized subexpression.
#+BEGIN_SRC 
for i in 1 2; do
  gawk ’match($0,/<reaction id="([^"]+)"/,A){r=A[1];next}
  match($0,/<parameter id="FLUX_VALUE" value="([^"]+)"/,A){if (A[1]!=0)print r,A[1];next}’ Ec_iAF1260_flux$i.xml > flux$i.val
done
#+END_SRC
and
#+BEGIN_SRC 
awk 'match($0, /Inference\[([0-9]+)\]/, A){print A[1]}' hard/COG0642/RAxML_info.optimize
#+END_SRC

** getline
while((getline<D)>0){... $0, $1 etc. are all reloaded as in the main stream ...} BEGIN{D="reaids.fgf";while((getline<D)>0){if (match($1,/_tr$/)) continue; R[++r]=$1;Rn[$1]=r} close(D);
the "1" below has the effect of "print;"

** substitution
#+BEGIN_SRC 
awk '{sub(/pattern/,"foobar")}1'
#+END_SRC
operate only on some lines of the input (according to some condition), but also want to print all the lines, regardless of whether they were affected by your operation or not

** counting
awk 'END {print NR,"coins"}' coins.txt
print out how many coins in the text.

** line filtering
conditions (use == instead of =)
find . -name '*_info.bootstrap' -exec tail -n1 '{}' \; | awk '$1=="All" && $2==30'
awk '/gold/' coins.txt
the same as 
awk '/gold/ {print}'
awk '/gold/ {print $0}'
awk '/gold/ {print $5,$6,$7,$8}' coins.txt
awk '{if ($3 < 1980) print $3, " ",$5,$6,$7,$8}' coins.txt
awk '(NR%2 && /pattern/) || (!(NR%2) && /anotherpattern/)'
That prints odd lines that match /pattern/, or even lines that match /anotherpattern/.

awk '/gold/ {ounces += $2} END {print "value = $" 425*ounces}' coins.txt
"ounces" is a "user defined" variable, as opposed to the "standard" pre-defined variables. Almost any string of characters can be used as a variable name in Awk, as long as the name doesn't conflict with some string that has a specific meaning to Awk, such as "print" or "NR" or "END". There is no need to declare the variable, or to initialize it. A variable handled as a string variable is initialized to the "null string", meaning that if we try to print it, nothing will be there. A variable handled as a numeric variable will be initialized to zero.

qstat | grep 'wz4' | awk '{ if ($5=="R") {print ;} }'
awk 'NR % 6'
prints all lines except those divisible by 6
awk 'NR > 5'
prints from line 6 onwards (like tail -n +6, or sed '1,5d')
awk '$2 == "foo"'
prints lines where the second field is "foo"
awk 'NF >= 6'
prints lines with 6 or more fields
awk '/foo/ && /bar/'
prints lines that match /foo/ and /bar/, in any order
awk '/foo/ && !/bar/'
prints lines that match /foo/ but not /bar/
awk '/foo/ || /bar/'
prints lines that match /foo/ or /bar/ (like grep -e 'foo' -e 'bar')
awk '/foo/,/bar/'
prints from line matching /foo/ to line matching /bar/, inclusive
awk 'NF'
prints only nonempty lines (or: removes empty lines, where NF==0)
awk 'NF--'
removes last field and prints the line awk '$0 = NR" "$0' # prepends line numbers (assignments are valid in conditions)

** file spacing
*** double space a file
#+begin_src sh
awk 'BEGIN { ORS="\n\n" }; 1'
#+end_src
BEGIN is a special kind of pattern which is not tested against the input. It is executed before any input is read. This one-liner double-spaces the file by setting the ORS variable to two newlines. As I mentioned previously, statement "1" gets translated to "{ print }", and every print statement gets terminated with the value of ORS variable.
*** another way to double-space a file.
#+begin_src sh
awk '1; { print "" }'
# equivalent to
awk '{ print } { print "" }'
#+end_src
Every print statement in Awk is silently followed by an ORS - Output Record Separator variable, which is a newline by default. The first print statement with no arguments is equivalent to "print $0", where $0 is a variable holding the entire line. The second print statement prints nothing, but knowing that each print statement is followed by ORS, it actually prints a newline.
*** double-space a file so that no more than one blank line appears between lines of text
#+begin_src sh
awk 'NF { print $0 "\n" }'
#+end_src
The one-liner uses another special variable called NF - Number of Fields. It contains the number of fields the current line was split into. For example, a line "this is a test" splits in four pieces and NF gets set to 4. The empty line "" does not split into any pieces and NF gets set to 0. Using NF as a pattern can effectively filter out empty lines. This one liner says: "If there are any number of fields, print the whole line followed by newline."
** line numbering
*** Number lines in each file separately.
#+begin_src sh
 awk '{ print FNR "\t" $0 }'
#+end_src
This Awk program appends the FNR - File Line Number predefined variable and a tab (\t) before each line. FNR variable contains the current line for each file separately. For example, if this one-liner was called on two files, one containing 10 lines, and the other 12, it would number lines in the first file from 1 to 10, and then resume numbering from one for the second file and number lines in this file from 1 to 12. FNR gets reset from file to file.
*** Number lines for all files together.
 #+begin_src sh
 awk '{ print NR "\t" $0 }'
 #+end_src
This one works the same as the case that uses FNR except that it uses NR - Line Number variable, which does not get reset from file to file. It counts the input lines seen so far. For example, if it was called on the same two files with 10 and 12 lines, it would number the lines from 1 to 22 (10 + 12).
* Sed
** use sed -E to allow [ [:space:]] like character class
** basic concept
*** the four spaces
It was the four spaces of sed -- input stream, output stream, pattern space, hold buffer. Sed operates on input stream and produces an output stream. The lines from input stream are placed into the pattern space where they are modified. The hold buffer can be used for temporary storage. These four spaces changed the way I think about sed.
*** the way sed operates
sed operates by performing the following cycle on each line of input: first, sed reads one line from the input stream, removes any trailing newline, and places it in the pattern space. Then commands are executed; each command can have an address associated to it: addresses are a kind of condition code, and a command is only executed if the condition is verified before the command is to be executed.
When the end of the script is reached, unless the -n option is in use, the contents of pattern space are printed out to the output stream, adding back the trailing newline if it was removed.3 Then the next cycle starts for the next input line.
Unless special commands (like ‘D’) are used, the pattern space is deleted between two cycles. The hold space, on the other hand, keeps its data between cycles (see commands ‘h’, ‘H’, ‘x’, ‘g’, ‘G’ to move data between both buffers). 
** s command
#+begin_src sh
sed 's/day/night/' <old >new
#+end_src
*** choice of delimiter (doesn't have to be "/")
#+begin_src sh
# quote the / by \/
sed 's/\/usr\/local\/bin/\/common\/bin/' <old >new
# or use _
sed 's_/usr/local/bin_/common/bin_' <old >new
# or use :
sed 's:/usr/local/bin:/common/bin:' <old >new
# or use |
sed 's|/usr/local/bin|/common/bin|' <old >new
#+end_src
*** fetch the matched found using &
#+begin_src sh
sed 's/[a-z]*/(&)/' <old >new
% echo "123 abc" | sed 's/[0-9]*/& &/'
123 123 abc
#+end_src
*** + in regular expression
The original sed did not support the "+" metacharacter. GNU sed does if you use the "-r" command line option, which enables extended regular expressions. The "+" means "one or more matches". So the above could also be written using
#+begin_src sh
% echo "123 abc" | sed -r 's/[0-9]+/& &/'
123 123 abc
#+end_src
*** \1 to fetch pattern in ()
The escaped parentheses (that is, parentheses with backslashes before them) remember a substring of the characters matched by the regular expression. You can use this to exclude part of the characters matched by the regular expression. 
The "\1" is the first remembered pattern, and the "\2" is the second remembered pattern. Sed has up to nine remembered patterns. 
#+begin_src sh
sed 's/\([a-z]*\).*/\1/'
#+end_src
** get a section without the first and last
sed -n '1,/PATTERN1/d;/PATTERN2/q;p' < input
** show the second line of a file
#+BEGIN_SRC sh
sed -n '2p' myfile
#+END_SRC

** -n no print
-n : no print
The -n option tells sed to print only those lines matching the pattern.
sed -n '1,100p' filename
** how to get \t in mac sed
the bsd sed is a little different from the gnu sed
brew install gnu-sed
and use gsed

or you could type control-v then tab for the tab

** print line with certain pattern
sed -n '/xzy/p' $filename
print line only containing "xzy"

** print from line 4 to line 10
head -n 4 file | tail -n1
get the 4th line, equivalent to
sed -n '4,10p' file

** print a section of a file
sed -n '/start_pattern/,/end_pattern/p' file_name
sed -n '/^st/,/^binaries/p' problem_moma.lp

** multiple commands with -e
sed -e 's/a/A/' -e 's/b/B/' <old >new
find . -type f -exec sed -i '' -e s/Red/$color1/g -e s/Blue/$color2/g {} \;
** edit file in place
*** remove last line of a file
#+BEGIN_SRC sh
sed -i '$ d' foo.txt
#+END_SRC

** how to print a non-sequential number of lines
$ sed -n -e 1,2p -e 4p somefile.txt
Line 1
Line 2
Line 4

** how to add some text to a given line                            :unknown:
add new text in the first line
sed -e '1s/$/new text/' yourfile.txt 
** how to delete first/last line
delete first line
tail -n +2 "$FILE"
or
sed -i -e "1d" $FILE
or
perl -ni -e 'print unless $. == 1' filename.txt

delete last line
sed -i '$ d' foo.txt
$ means "the last line in the file". When specifying a location (called "range" in sed lingo) before a command, that command is only applied to the specified location. So, this command explicitly says "in the range of the last line in a file, delete it
** print range but exclude patterns
exclude both starting and ending pattern
$ cat file
AIX
Solaris
Unix
Linux
HPUX
$ sed -n '/Solaris/,/HPUX/{//!p;}' file
Unix
Linux

exclude ending pattern
sed -n '/chrM/,/GL000207.1/{/GL000207.1/!p}' /scratch/bcb/wzhou1/reference/hs37d5/hs37d5.fa > hs37d5.fa.chrM
* C
** gdb
*** how to enable TUI
gdb -tui
or
"C-x C-a" inside a session
** define bit array in macro
define a bit array
#define BITOP(a,b,op) ((a)[(size_t)(b)/(CHAR_BIT*sizeof *(a))] op ((size_t)1<<((size_t)(b)%(CHAR_BIT*sizeof *(a)))))

#define BITMASK(b) (1 << ((b) % CHAR_BIT)) /* the mask on the char that is to be tested/altered */
#define BITSLOT(b) ((b) / CHAR_BIT) /* the index of the char where bth bit sit */
#define BITSET(a,b) ((a)[BITSLOT(b)] |= BITMASK(b))
#define BITCLEAR(a, b) ((a)[BITSLOT(b)] &= ~BITMASK(b))
#define BITTEST(a, b) ((a)[BITSLOT(b)] & BITMASK(b))
#define BITNSLOTS(nb) ((nb + CHAR_BIT - 1) / CHAR_BIT)calculate the number of slots necessary to hold nb bits
** C中各变量大小
char 8 bytes but allowed to be wider
unsigned char 0..255 
signed char -127..127

A plain char(without the prefix 'signed' or 'unsigned') is either the one or the other. The compiler chooses for you and it most likely bases that choice on the way both of them can be implemented on your hardware.


short: 2 bytes
int: 4 bytes
long: 4 bytes (on some machine,8 bytes)

float: 4 bytes
double: 8 bytes
** how to print a char array of length 3
printf("%.3s", char_array);
or for unknown length
printf("%.*s", num, char_array);
** how to define global variable (across files)
The clean, reliable way to declare and define global variables is to use a header file file3.h to contain an extern declaration of the variable. The header is included by the one source file that defines the variable and by all the source files that reference the variable. For each program, one source file (and only one source file) defines the variable. Similarly, one header file (and only one header file) should declare the variable.

file3.h
#+begin_src c
  extern int global_variable;  /* Declaration of the variable */
#+end_src

file1.c
#+begin_src c
  #include "file3.h"  /* Declaration made available here */
  
  /* Variable defined here */
  int global_variable = 37;    /* Definition checked against declaration */
  
  int increment(void) { return global_variable++; }
#+end_src

file2.c
#+begin_src c
  #include "file3.h"
  #include <stdio.h>
  
  void use_it(void)
  {
      printf("Global variable: %d\n", global_variable++);
  }
#+end_src

** type
*** c string (char *) to string
C++ strings have a constructor that lets you convert C-style strings:
#+begin_src c++
  char* myStr = "This is a C string!";
  std::string myCppString = myStr;
#+end_src
** IO
*** getopt - get command line options
the synopsis, note that the variable optind, opterr and optopt are extern
#+begin_src c
  #include <unistd.h>
  
  int getopt(int argc, char * const argv[],
             const char *optstring);
  
  extern char *optarg;
  extern int optind, opterr, optopt;
#+end_src

an example
#+begin_src c
  #include "unistd.h"             /* for getopt */
  while((c = getopt(argc, argv, "A:l:w:q:N:p:I:Q:L:brdR:c:M:h:k:")) >= 0){
    switch(c) {
    case 'A': estimate_max_ins = atoi(optarg);            break;
    case 'l': flanking_size = atoi(optarg);               break;
    default: fprintf(stderr, "Unrecognized option '-%c'.\n", c);
      return 1;
    }
   }
  
#+end_src

another example from wikipedia
#+begin_src c
  #include <stdio.h>     /* for printf */
  #include <stdlib.h>    /* for exit */
  #include <unistd.h>    /* for getopt */
  int main (int argc, char **argv) {
    int c;
    int digit_optind = 0;
    int aopt = 0, bopt = 0;
    char *copt = 0, *dopt = 0;
    while ( (c = getopt(argc, argv, "abc:d:012")) != -1) {
      int this_option_optind = optind ? optind : 1;
      switch (c) {
      case '0':
      case '1':
      case '2':
        if (digit_optind != 0 && digit_optind != this_option_optind)
          printf ("digits occur in two different argv-elements.\n");
        digit_optind = this_option_optind;
        printf ("option %c\n", c);
        break;
      case 'a':
        printf ("option a\n");
        aopt = 1;
        break;
      case 'b':
        printf ("option b\n");
        bopt = 1;
        break;
      case 'c':
        printf ("option c with value '%s'\n", optarg);
        copt = optarg;
        break;
      case 'd':
        printf ("option d with value '%s'\n", optarg);
        dopt = optarg;
        break;
      case '?':
        break;
      default:
        printf ("?? getopt returned character code 0%o ??\n", c);
      }
    }
    if (optind < argc) {
      printf ("non-option ARGV-elements: ");
      while (optind < argc)
        printf ("%s ", argv[optind++]);
      printf ("\n");
    }
    exit (0);
  }
#+end_src
*** getopt_long - gnu long option
#+begin_src c
  #include <stdio.h>     /* for printf */
  #include <stdlib.h>    /* for exit */
  #include <getopt.h>    /* for getopt_long; standard getopt is in unistd.h */
  int main (int argc, char **argv) {
    int c;
    int digit_optind = 0;
    int aopt = 0, bopt = 0;
    char *copt = 0, *dopt = 0;
    static struct option long_options[] = {
      {"add", 1, 0, 0},
      {"append", 0, 0, 0},
      {"delete", 1, 0, 0},
      {"verbose", 0, 0, 0},
      {"create", 1, 0, 'c'},
      {"file", 1, 0, 0},
      {NULL, 0, NULL, 0}
    };
    int option_index = 0;
    while ((c = getopt_long(argc, argv, "abc:d:012",
                            long_options, &option_index)) != -1) {
      int this_option_optind = optind ? optind : 1;
      switch (c) {
      case 0:
        printf ("option %s", long_options[option_index].name);
        if (optarg)
          printf (" with arg %s", optarg);
        printf ("\n");
        break;
      case '0':
      case '1':
      case '2':
        if (digit_optind != 0 && digit_optind != this_option_optind)
          printf ("digits occur in two different argv-elements.\n");
        digit_optind = this_option_optind;
        printf ("option %c\n", c);
        break;
      case 'a':
        printf ("option a\n");
        aopt = 1;
        break;
      case 'b':
        printf ("option b\n");
        bopt = 1;
        break;
      case 'c':
        printf ("option c with value '%s'\n", optarg);
        copt = optarg;
        break;
      case 'd':
        printf ("option d with value '%s'\n", optarg);
        dopt = optarg;
        break;
      case '?':
        break;
      default:
        printf ("?? getopt returned character code 0%o ??\n", c);
      }
    }
    if (optind < argc) {
      printf ("non-option ARGV-elements: ");
      while (optind < argc)
        printf ("%s ", argv[optind++]);
      printf ("\n");
    }
    exit (0);
  }
#+end_src
*** standard usage information
#+begin_src c
  namespace {
    const int DEFAULT_FLANKING_SIZE = 500; // l
    const int DEFAULT_ASSEMBLE_READ_QUAL = 1; // q
    const int DEFAULT_NUM_MISMATCH_POOR_MAP = 5; // N
    const int DEFAULT_HIGH_DEPTH_SKIP = 1000; // p
    const int DEFAULT_PAD_LOCAL_REF = 200; // w
    const int DEFAULT_MIN_SIZE_THRESHOLD = 3; // M skip those with input size smaller than 3
    const int DEFAULT_MAX_NODE = 100; // h
    const string DEFAULT_KMERS = "15,25";
  
    void usage() {
      fprintf(stderr, "\n./tigra_sv <SV file> <a.bam> <b.bam> ...\n\n");
      fprintf(stderr, "\n Or: ./tigra_sv <SV file> <bam_list_file>\n\nOptions: \n");
      fprintf(stderr, "    -l INT     Flanking size for assembly [%d] bp\n", DEFAULT_FLANKING_SIZE);
      fprintf(stderr, "    -c STR     Only assemble calls on specified chromosome\n");
      fprintf(stderr, "    -R STR     Reference file location with the full path\n");
      fprintf(stderr, "    -q INT     Only assemble reads with mapping quality > [%d]\n", DEFAULT_ASSEMBLE_READ_QUAL);
      fprintf(stderr, "    -N INT     Number of mismatches required to be tagged as poorly mapped [%d]\n", DEFAULT_NUM_MISMATCH_POOR_MAP);
      fprintf(stderr, "    -p INT     Ignore cases that have average read depth greater than [%d]\n", DEFAULT_HIGH_DEPTH_SKIP);
      fprintf(stderr, "    -r         Write local reference to a file with .ref.fa as the suffix\n");
      fprintf(stderr, "    -d         Dump reads to fasta files\n");
      fprintf(stderr, "    -I STR     Save output files into an existing directory\n");
      fprintf(stderr, "    -w INT     Pad local reference by additional [%d] bp on both ends\n", DEFAULT_PAD_LOCAL_REF);
      fprintf(stderr, "    -b         Check when the input format is breakdancer\n");
      fprintf(stderr, "    -M INT     Skip those calls with input size smaller than [%d]\n", DEFAULT_MIN_SIZE_THRESHOLD);
      fprintf(stderr, "    -h INT     Maximum node to assemble, by default [%d]\n", DEFAULT_MAX_NODE);
      fprintf(stderr, "    -k STR     List of kmer sizes to use as a comma delimited string [%s]\n", DEFAULT_KMERS.c_str());
      fprintf(stderr, "Version: %s (commit %s)\n", __g_prog_version, __g_commit_hash);
    }
  }
  
#+end_src

** C string
*** in-place string swapping using XOR(^)
#+begin_src c
  #include <stdio.h>
  
  void strrev(char *p)
  {
    char *q = p;
    while(q && *q) ++q;
    for(--q; p < q; ++p, --q)
      *p = *p ^ *q,
      *q = *p ^ *q,
      *p = *p ^ *q;
  }
  
  int main(int argc, char **argv)
  {
    do {
      printf("%s ",  argv[argc-1]); strrev(argv[argc-1]);
      printf("%s\n", argv[argc-1]);
    } while(--argc);
  
    return 0;
  }
#+end_src
** shared library
*** introduction - 三种shared library names
      http://tldp.org/HOWTO/Program-Library-HOWTO/shared-libraries.html
      1. so name : /usr/lib/libreadline.so.3
       	 this is a linker to real name (set by ldconfig)
       	 this is the name that a user program would give
       	 现存的程序通常记录一个major version number。
       	 每次load的时候总是load这个so name
      2. real name : /usr/lib/libreadline.so.3.0
       	  this is the name of the file created by the developer
       	  of the library.
      3. linker name : /usr/lib/libreadline.so
       	  this is a linker to so name (done during the library installation)
       	  usually the latest so name
       	  编译一个新程序，总是采用linker name，因为它总是指向最新的so name。
*** ELF (Executable and Linkable Format) - the real .so format
In fact, ELF format is also the format for all binary files, executables and core dumps
http://en.wikipedia.org/wiki/Executable_and_Linkable_Format

Its components include:
#+BEGIN_EXAMPLE
ELF header
Program header table, describing zero or more segments
Section header table, describing zero or more sections
Data referred to by entries in the program header table or section header table
#+END_EXAMPLE

*** find what is in the shared library
**** a brute-force way by using "strings" (NOT recommended)
#+BEGIN_SRC sh
strings /usr/lib64/libstdc++.so.6 | grep 'GLIBCXX'
#+END_SRC
**** objdump - display information from object files
***** see private header using "objdump -p"
#+BEGIN_SRC sh
objdump /usr/lib64/libstdc++.so.6 -p
#+END_SRC
**** readelf - display information from ELF files
#+BEGIN_SRC sh
# display all info
readelf -a mylib.so

# display file header only
readelf -h mylib.so 

# display version info
readelf -V mylib.so
#+END_SRC

**** file - display some of the library infomation
*** ldd 显示一个程序所使用的所有.so
例如 ldd /bin/ls
for safety's sake, don't use ldd on programs you don't trust.
ldd命令等同于 /lib/ld-linux.so.2 --list
#+BEGIN_SRC sh
ldd -r -v my_binary
#+END_SRC
-v prints the version info of the shared library
-r prints the object
*** what's GLIBCXX_3.4?
this is the version reference
can be seen using "objdump -p"
*** find out which shared library is installed?
rpm -q libstdc++
or rpm -qa | grep libstdc++
*** compile shared library using gcc
    gcc -shared -Wl,-soname,your_soname -o real_name file_list library_list
    注意-Wl,-soname,[yourname] 之间只有逗号，不能有空格。
    避免使用-fomit-frame-pointer
    -fPIC 产生 position independent code
    -fpic 相同，但是为本地平台优化，速度更快，但不推荐。

#+BEGIN_SRC sh
  gcc -fPIC -g -c -Wall a.c
  gcc -fPIC -g -c -Wall b.c
  gcc -shared -Wl,-soname,libmystuff.so.1 -o libmystuff.so.1.0.1 a.o b.o -lc
#+END_SRC
*** ldconfig - install shared library
(as root) Re-create the bindings and the cache for the loader of dynamic libraries ("ld"). You may want to run ldconfig after an installation of new dynamically linked libraries on your system. (It is also re-run every time you boot the computer, so if you reboot you don't have to run it manually.)

ldconfig 用来更新 /etc/ld.so.cache。详情见下:
When you install a new version of a library, you install it in one of a few special directories and then run the program ldconfig(8). ldconfig examines the existing files and creates the sonames as symbolic links to the real names, as well as setting up the cache file /etc/ld.so.cache.
Searching all of these directories at program start-up would be grossly inefficient, so a caching arrangement is actually used. The program ldconfig(8) by default reads in the file /etc/ld.so.conf, sets up the appropriate symbolic links in the dynamic link directories (so they'll follow the standard conventions), and then writes a cache to /etc/ld.so.cache that's then used by other programs. This greatly speeds up access to libraries. The implication is that ldconfig must be run whenever a DLL is added, when a DLL is removed, or when the set of DLL directories changes; running ldconfig is often one of the steps performed by package managers when installing a library. On start-up, then, the dynamic loader actually uses the file /etc/ld.so.cache and then loads the libraries it needs.
***** 设定环境变量以暂时改变以上默认行为（debug常用）
****** LD_LIBRARY_PATH
       	 a set of directory where libraries should be searched for first, before the standard set of directories;
       	 LD_LIBRARY_PATH=.:$LD_LIBRARY_PATH my_program
       	 see "Why LD_LIBRARY_PATH is bad?"
       	 http://www.visi.com/~barr/ldpath.html
****** LD_RELOAD
       	 override /etc/ld.so.preload
****** LD_DEBUG
       	 产生debug信息，可设为"files", "bindings","libs","versions", "help"
       	 =======================
       	 export LD_DEBUG=files
       	 command_to_run
       	 =======================
***** 直接调用program loader （debug常用）
       	/lib/ld-linux.so.2 --library-path PATH EXECUTABLE
**** 安装shared library
       ldconfig -n directory_with_shared_libraries.
       -n 只更新指定的目录
*** Procedure Linkage Table (PLT)
程序中动态链接库集中放置的地方。
a table in the program that lists every function that a program calls. When the program is started, the PLT contains code for each funcion to query the runtime linker for the address at which it has loaded a function. It then fills in that entry in the table and jumps there. As each function is called, its entry in the PLT is smplified into a direct jump to the loaded function.
*** program loader /lib/ld-linux.so.X
searches /etc/ld.so.conf for list of directories for libraries that needs loading.
/etc/ld.so.conf usually include /etc/ld.so.conf.d/*.conf
*** object file (.o)
gdb -q myfile 查看.o文件的内存映像
*** c++filt - demangle c++ and Java symbols
#+BEGIN_SRC 
readelf -aW <binary> | c++filt
#+END_SRC
* R
:PROPERTIES:
:VISIBILITY: children
:END:
** Bioconductor
*** basics
**** install bioconductor
source("http://bioconductor.org/biocLite.R")
biocLite()
**** install bioconductor packages
biocLite(c("GenomicFeatures", "AnnotationDbi"))
**** upgrade installed package (require R version > 2.15)
source("http://bioconductor.org/biocLite.R")
biocLite('BioUpgrade')
**** recompile installed package
source("http://bioconductor.org/biocLite.R")
pkgs <- rownames(installed.packages())
biocLite(pkgs, type="source")
*** methylumi
**** tryout
suppressPackageStartupMessages(library(methylumi,quietly=TRUE))
samps <- read.table(system.file("extdata/samples.txt", package = "methylumi"),sep="\t",header=TRUE)
mldat <- methylumiR(system.file('extdata/exampledata.samples.txt',package='methylumi'), qcfile=system.file('extdata/exampledata.controls.txt',package="methylumi"), sampleDescriptions=samps)

md <- cmdscale(dist(t(exprs(mldat)[fData(mldat)$CHROMOSOME=='X',])),2)
plot(md,pch=c('F','M')[pData(mldat)$Gender],col=c('red','blue')[pData(mldat)$Gender])

# system.file find file relative to package installation
**** assessor functions
exprs(ob)
betas(ob)
pvals(ob)
methylated(ob)
unmethylated(ob)
QCdata(ob)
getHistory(ob)
**** example - QC
#+BEGIN_SRC R
qcplot(mldat, 'FIRST HYBRIDIZATION')
#+END_SRC
**** example - normalization
only for GoldenGate platform
#+BEGIN_SRC R

mldat.norm <- normalizeMethyLumiSet(mldat)
# looks at the median intensities in the methylated and unmethylated channels at very low and high beta values and sets these medians equal.
#+END_SRC

**** example - test whether expression differs between males and females

see "an introduction to methylumi package"

#+BEGIN_SRC R

library(limma)

design_matrix <- model.matrix(~1+Gender, data=pData(mldat.norm))
# pData returns the "phenotypic" data in a data.frame
# "Gender" is a column in the returned data.frame
# model.matrix creates the "design matrix" which was usually used for the linear regression analysis
# In this case, however, the design matrix can be for limma (see limma paper for the definition of the design matrix, in brief, the rows are array experiments the columns are contrast coefficient)
# the "1+" adds (which is by default on too, so one could just use model.matrix(~Gender) in principle) the intercept term into the design matrix. The intercept term is a column of 1's.

fit <- lmFit(exprs(mldat), design_matrix)
# raw fit

fit <- eBayes(fit)
# empirical Bayes smoothing
# which estimates the prior from the raw fit, then estimate the posterior fit, then iterate.

tt <- topTable(fit, coef=2, genelist=fData(mldat)[,c('SYMBOL', 'CHROMOSOME')], number=1000)
# get the top differentially expressed genes

x <- aggregate(tt$adj.P.Val, by=list(tt$CHROMOSOME), median)
# split the genes by chromosome, compute the median adjusted P-value
colnames(X) <- c('Chromosome', 'Median adjusted P-value')
# set title

library(xtable)
xt <- xtable(x, label='tab:chromosomepvals', caption='The median adjusted P-value for each gene')
digits(xt) <- 6
print(xt, include.rownames=FALSE, align='cr')
# print LaTeX table.
#+END_SRC

*** illuminaio
*** Biobase

exprs accesses expression from eSet class
se.exprs access standard error of expression from eSet class

fData retrieves features
** basics
*** what is S3 scheme of method dispatching?
S3 is a naming scheme of method dispatching (or method overload, or polymorphism or whatever you call it)

S3 method means one have different method instantiation when applied to different S3 classes.

it works by:
setting the return value of a call to method "glm" to be "glm"
print.glm is the specific version of print when it applies to object of class "glm"

For example, if you type "median", you see "UseMethod("median"). That means it is an S3 method. To list all the S3 classes
#+BEGIN_SRC R
methods(median)
methods(print)
#+END_SRC

Notice that some of the methods have *s next to their name. That means that they are hidden inside some package's namespace. Use find to find out which package they are in. For example
#+BEGIN_SRC R
find("acf")  #it's in the stats package
stats:::print.acf
#+END_SRC

*** S4 class

see http://www.cyclismo.org/tutorial/R/s4Classes.html

S4 objects are created using setClass
S4 methods are created using setGeneric + setMethod
S4 data members (called 'slot's) are accessed from '@' instead of '$'
is.object(a) tells whether a is an object
isS4(a) tells whether a is an S4 object
"contains" argument specify base class of inheritance
"callNextMethod" is used to call the same version in the base class.

other methods:
slotNames(a)
slotNames('MethyLumiSet')
getSlots('MethyLumiSet') returns the types (search getSlots in the link for detail)
getslot(a, 'slotname') == a@slotname

"The basic idea is that if the name of a function has not been defined, the name must first be reserved using the setGeneric function."

In setMethod, the first component of "signature" argument must be the same as the name of the class object. additional components can be different to allow overloading. (search overloading)

validity function can be explicitly called through validObject(theObject);

*** environment function

http://adv-r.had.co.nz/Environments.html

objects in the environment is like pointers in C.

environments' members are accessed via e$member or e[ [member]] or get('member', envir=e)

environment is like a list, but different in other ways:
 + Every object in an environment has a unique name.
 + The objects in an environment are not ordered (i.e., it doesn’t make sense to ask what the first object in an environment is).
 + An environment has a parent.

lexical scoping: if a name is not found in an environment, R will look in the parent environment.

only empty environment does not have parent

environment = frame + parent environment

ls(e) list bindings in the environment, omitting names begin with '.'.
ls(e, all.names=True) show all the names
create a new binding like a list: e$a <- 1
remove binding: rm('a', envir=e)
check existence of binding: exists('x', envir=e)
consider inherits: exists('x', envir, inherits=TRUE)

where environment is defined?
library(pryr)
where('x')
<environment: R_GlobalEnv>

 + globalenv() interactive workspace
 + baseenv() base environment, on top of emptyenv()
 + emptyenv() empty environment, ultimate ancestor of all environments
 + environment() current environment

[environment loaded with "library"] => globalenv() => search path .. => baseenv() => emptyenv()

The enclosing environment determines how the function finds values; the binding environments determine how we find the function.

** Data IO
see R wiki for more options
http://en.wikibooks.org/wiki/R_Programming/Importing_and_exporting_data
*** read ods file
library("gnumeric")
df <- read.gnumeric.sheet(file = "df.ods", head = TRUE, sheet.name = "Feuille1")
*** read.csv
read.csv is identical to read.table except that the default separator is “,” and header=TRUE
t<-read.csv('2012_04_16_endosymbiosis_in_gamma_proteobacteria.csv', header=TRUE)

*** handle large dataset by doing a pilot study of the type of each column
initial <- read.table(“datatable.txt”, nrows=100) # try the first 100 rows first
classes <- sapply(initial, class)
tabAll <- read.table(“datatable.txt”, colClasses=classes)

*** dget and dput
read and write R code files, contain the metadata from R for a single object
> y<-data.frame(a=1,b='a')
> dput(y) # dput to the console
structure(list(a = 1, b = structure(1L, .Label = "a", class = "factor")), .Names = c("a", 
"b"), row.names = c(NA, -1L), class = "data.frame")
> dput(y,file='y.R') # dput to a file
> new.y <- dget('y.R')
> new.y
  a b
1 1 a

*** source and dump
read and  write R code files, contain the metadata from R, possibly for multiple objects
> x<-'foo'
> y<-data.frame(a=1,b='a')
> dump(c('x','y'), file='data.R')
> rm(x,y)
> source('data.R')

*** sink
sink("/home/wanding/myfile.csv")
datafile
sink()
完成

*** write.table
write.table(datafile,file="/home/wanding/myfile.csv")

*** load and save
read and write workspaces
*** serialize and unserialize
write and read single R object in binary form

*** file connections
file, opens a connection to a file
gzfile, opens a connection to a gzip file
bzfile, opens a connection to a bzip2 file
url, opens a connection to a webpage
> str(file)
function (description = "", open = "", blocking = TRUE, encoding = getOption("encoding"),  raw = FALSE)  
str(file) see the description of a command (in this case “file”)
open is a code indicating
'r' read only
'w' writing
'a' appending
'rb','wb','ab' for binary
con ← file(“foo.txt”, 'r')
data ← read.csv(con)
close(con)
the same as 
data ← read.csv('foo.txt')

**** readLines and writeLines
> con ← gzfile(“words.gz”)
> x ← readLines(con,10) # read in 10 lines.

**** Read in lines of a webpage
con ← url(“http://www.jhsph.edu”,'r')
x ← readLines(con)

*** Print
**** Print without quote “”
> print("a")
[1] "a"
> print("a",quote=FALSE)
[1] a

**** print without array index using cat
> cat("a\n")
a
but in this case, one needs to append “\n”.

the best solution to use C-type printf is to use cat + sprintf
cat(sprintf('%1.3f\t%s\n', a.sorted[i], names(a.sorted)[i]))

** Manipulate Data
**** cut
group data into intervals
> a=c(3,7,2,5,7,12,15,13,17)
> cut(a, br=c(1,5,10,15))
[1] (1,5]   (5,10]  (1,5]   (1,5]   (5,10]  (10,15] (10,15] (10,15] <NA>   
Levels: (1,5] (5,10] (10,15]
> ac=cut(a, br=c(1,5,10,15))
> class(ac)
[1] "factor"
> as.character(ac)
[1] "(1,5]"   "(5,10]"  "(1,5]"   "(1,5]"   "(5,10]"  "(10,15]" "(10,15]" "(10,15]"
[9] NA

***** specify labels
> cut(a, br=c(1,5,10,15),labels=c('1-5','5 to 10','10~15'))
[1] 1-5     5 to 10 1-5     1-5     5 to 10 10~15   10~15   10~15   <NA>   
Levels: 1-5 5 to 10 10~15

**** assign()
assign("x", function(arguments)) # Has the same effect, but uses the assignment function instead of the assignment operator.
x<-function(arguments)

**** how to round a number using floor and ceiling
sapply(vec, floor)
trunc
round
signif
**** word wrap using strwrap function
sapply(names(t), function(x) paste(strwrap(x,width=40),collapse="\n"))

**** view the class (type) of an object
class(x)
in fact “class” is an attribute of x, see below.

**** names()
> x<-1:3
> names(x) # no names by default
NULL
> names(x)<-c('foo','bar','norf')
> x
 foo  bar norf 
  1    2    3 
> names(x)
[ 1 ] "foo"  "bar"  "norf"
# note that even with names, x remains a vector, not a data.frame
**** explicit type coersion
as.numeric()
as.character()
as.logical()
as.complex()

**** integer and numeric
numeric is equivalent to "double"
“1” is numeric and “1L” is integer.

**** NULL, NaN and NA
***** basics
NULL is nothing. is.null()

NA has a class too, there's integer NA and character NA.
default is a logical NA
NaN is NA, but NA is not NaN.
> x<-c(1,2,NA,10,3)
> is.na(x)
[ 1 ] FALSE FALSE  TRUE FALSE FALSE
> is.nan(x)
[ 1 ] FALSE FALSE FALSE FALSE FALSE
> x<-c(1,2,NaN,NA,4)
> is.na(x)
[1] FALSE FALSE  TRUE  TRUE FALSE
> is.nan(x)
[1] FALSE FALSE  TRUE FALSE FALSE
***** omit all NA in a data.frame (this omit any row with at least 1 NA)
na.omit(dat)
***** omit rows with all NA in a data.frame

df2 <- df[rowSums(is.na(df)) == 0,];
cat(sprintf('Removed %d rows from %d.\n', dim(df)[1]-dim(df2)[1], dim(df)[1]));
return(df2);

***** filter out NA - is.na
> x<-c(1,2,NA,4,NA,5)
> x[!is.na(x)]
[ 1 ] 1 2 4 5

***** filter out NA for two vectors simultaneously using complete.cases.
> x<-c(1,NA,2,4,NA,5)
> y<-c('a','b',NA,'d',NA,'f')
> good<-complete.cases(x,y)
> good
[ 1 ]  TRUE FALSE FALSE  TRUE FALSE  TRUE
> x[good]
[ 1 ] 1 4 5
> y[good]
[ 1 ] "a" "d" "f"

**** concatenate two string - paste
paste("wan","ding")
this would insert a " "

paste0("wan", "ding")
this results in "wanding"

**** trim whitespace
library(stringr)
str_trim(string)
**** substring
substring:
substring("wanding",1,3)
gives "wan"
*** attributes of an R object
every R object can have attributes attached to it

attributes are named list of R

to list all attributes
attributes(obj)

to access a specific attributes
attr(obj,'aaa')
or
aaa(obj)

"class" is an attribute of obj

structure() function returns a new object with modified attributes
structure(1:10, my_attribute='This is a vector')

By default, most attributes are lost when modifying a vector
Attributes that are NOT lost:
names
dimensions
class

factors can be seen as vector with "level" attribute.


*** vector
numeric, character, integer are all vectors
**** create a numeric vector - numeric()
e<-numeric()		create an empty numeric vector
**** create a character vector - character()
as.character(z)	convert to character
**** create an integer vector - integer()
as.integer(z)	convert to integer
**** create empty vector
list of empty character string vector is listed as character(0) and the empty numeric vector as numeric(0)
**** take absolute value
y[y<0]<- -y[y<0]
has the same effect as y<- abs(y)
**** repeat
c(2,3)[rep(c(2,1),times=2)] gives 3 2 3 2
y <- rep(x,times=5)
y<- rep(x,each=5)

**** how to create X1, Y2, X3, Y4 ... quickly
labs<-paste(c("X","Y"),1:10,sep"")
== c("X1", "Y2", "X3", "Y4", "X5", "Y6", "X7", "Y8", "X9", "Y10")
the recycling of short lists takes place here.

**** delete elements from vector
x[-c(1,2,3)]	删掉第1，2，3个元素
**** how to generate (1,1,1,2,2,2,3,3,3) from (1,2,3)
> rep(c("g1","g2","g3"),times=c(1,3,4))
[1] "g1" "g2" "g2" "g2" "g3" "g3" "g3" "g3"
**** Createpr an empty vector
x ← vector(“numeric”, length=10)
vector elements are initialized to 0.0
logical vector
x ← c(T,F)
x ← c(TRUE, FALSE)

**** range
x <- 1:20
seq
seq(from = 1, to = 1, by = ((to - from)/(length.out - 1))
seq(-5, 5, by = .2)
***** sample 51 points from -5 by interval 0.2
seq(length = 51, from = -5, by = 0.2)
***** sample 50 points from 3 to 7 evenly
***** seq_along - the counterpart of python enumerate
seq_along(x) is equivalent to seq(along=x)
**** type coersion in vectors
elements in a vector must be of the same type, there will be implicit type coersion when different types are mixed.
y ← c(1.7, “a”) ## character 1.7 → “1.7”
y ← c(TRUE,2) ## numeric TRUE → 1
y ← c(“a”, TRUE) ## character TRUE → “TRUE”

**** subset a vector
# different from python, 1-based, last element included.
x[1:10] select the first 10 components of x
y<-x[x<10]
y<-x[!is.na(x)]
(x+1)[(!is.na(x)) & x>0] ->z
A<-c(1,2,3)
B<-c(0,1,0)
change A's element to 0 if B's element is 1: A[B==1]<-0

y<-x[-(1:5)] gives y all but the first five elements of x

> x<-c('a','b','c','c','d','a')
> x[ 2 ]
[ 1 ] "b"
> x[1:4]
[ 1 ] "a" "b" "c" "c"
> x[x>"a"]
[ 1 ] "b" "c" "c" "d"
> u<-x > 'a'
> u
[ 1 ] FALSE  TRUE  TRUE  TRUE  TRUE FALSE
> x[u]
[ 1 ] "b" "c" "c" "d"

**** vectorized operations
> x <- 1:4; y <- 6:9
> x+y
[1]  7  9 11 13
> x>=2
[1] FALSE  TRUE  TRUE  TRUE
> y==9
[1] FALSE FALSE FALSE  TRUE
> x*y
[1]  6 14 24 36
> x/y
[1] 0.1666667 0.2857143 0.3750000 0.4444444

**** count using sum
> sum(x>4)
count the number of elements in x that are larger than 4

**** vector type check
is.vector()

**** use match to find mapping between two vectors
ma=match(cats, names(t))

**** reverse the vector
rev(vec)
**** get index of an element in a vector
which(vector == element_value)
*** matrix
**** make a matrix
> m ← matrix(nrow=2, ncol=3)
initialized to “NA”
> m ← matrix(1:6, nrow=2, ncol=3)
> m
     [,1] [,2] [,3]
[1,]    1    3    5
[2,]    2    4    6
matrix initialization is “column first” or “columnwise”
> m ← 1:10
> dim(m) ← c(2,5)
> m
     [,1] [,2] [,3] [,4] [,5]
[1,]    1    3    5    7    9
[2,]    2    4    6    8   10

**** create a matrix
diag(v) gives a diagonal matrix with elements of the vector as the diagonal entries.
diag(M) gives the vector of main diagonal entries of M.
diag(k) gives a k by k identity matrix
**** create a matrix by concatenation
cbind(arg_1,arg_2,arg_3,...) forms matrices by binding together matrices horizontally, or column-wise.
rbind(arg_1,arg_2,arg_3,...) forms matrices by binding together matrices vertically, or row-wise.
**** matrix multiplication
"*" normal multiplication
"%*%" matrix multiplication

**** matrix transposition
t() matrix transposition

**** cross product two vector
crossprod(x,y)==t(x)%*%y
**** dimension of a matrix
> dim(m)
[1] 2 5

**** remove a column/row of a matrix
d[,-5] deletes column 5
mat[-3,] remove the 3rd row of the matrix
t[-1,] remove the first row

**** remove several rows
df[-(416:472), ]

**** Sort a data frame by a certain column
sortd=d[with(d, order(-ratio)),]

**** view attributes of objects, e.g., names, dimnames, dimensions, class, length are all attributes.
> attributes(m)
$dim
[1] 2 3

**** cbind and rbind
> x ← 1:3
> y ← 10:12
> cbind(x,y)
     x  y
[1,] 1 10
[2,] 2 11
[3,] 3 12
> rbind(x,y) 
  [,1] [,2] [,3]
x    1    2    3
y   10   11   12

**** add two columns rowwise using cbind
ctbl = cbind(tbl[,"Freq"], tbl[,"None"] + tbl[,"Some"])

**** matrix with dimnames (row names, column names)
> m<-matrix(1:4, nrow=2,ncol=2)
> dimnames(m)<-list(c('a','b'),c('c','d'))
> m
  c d
a 1 3
b 2 4

**** get or set the names of rows and columns
rownames(matrix)
colnames(matrix)

can also use dimnames(matrix),where
"rownames()" is equivalent to "dimnames(matrix)[ [1] ]"
"colnames()" is equivalent to "dimnames(matrix)[ [2] ]"
actually [[]] can be substituted by []
dimnames() is built for high dimensional matrix e.g. 3,4...

colnames(data)<-c('Apple','Orange')
colnames(data)<-"Q"

**** build matrix by column
matrix <- cbind(column1 = 3, column2 = c(4:1,2:5))

**** apply on rows/columns
col.sums <- apply(matrix,2,sum)
row.sums <- apply(matrix,1,sum)

**** use sweep to normalize data

#+BEGIN_SRC R
x <- sweep(x, 2, rowMeans(x, na.rm = na.rm))
sx <- apply(x, 1, sd, na.rm = na.rm)
x <- sweep(x, 1, sx, "/")
#+END_SRC

**** subset a matrix
> x<-matrix(1:6,2,3)
> x[1,2]
[1] 3
> x[2,1]
[1] 2
> x[1,]
[1] 1 3 5
> x[,2]
[1] 3 4

by default when a matrix is subsetted, one get a vector instead of a matrix, if one wants a submatrix, specify drop=FALSE
> x[1,]
[1] 1 3 5
> x[1,,drop=FALSE]
     [,1] [,2] [,3]
[1,]    1    3    5

**** vectorized operations
> x <- matrix(1:4, 2, 2); y <- matrix(rep(10, 4), 2, 2) 
> x * y ## element-wise multiplication 
   [,1] [,2] 
[1,] 10 30 
[2,] 20 40 
> x / y 
   [,1] [,2] 
[1,] 0.1 0.3 
[2,] 0.2 0.4 
> x %*% y ## true matrix multiplication 
   [,1] [,2] 
[1,] 40 40 
[2,] 60 60

*** list
List can mix objects of different types
> x ← list(1, “a”, TRUE, 1+4i)
> x
[[1]]
[1] 1

[[2]]
[1] "a"

[[3]]
[1] TRUE

[[4]]
[1] 1+4i

**** A list with name
> x<-list(a=1,b=2,c=3)
> x
$a
[1] 1

$b
[1] 2

$c
[1] 3

**** subset a list
> x<-list(foo=1:4,bar=0.6)
> x[1] # single bracket always return object of the same class, in this case a list.
$foo
[1] 1 2 3 4

> x[ [1] ]
[1] 1 2 3 4
> x$bar
[1] 0.6
> x[ ["bar"] ]
[1] 0.6
> x["bar"]
$bar
[1] 0.6

**** extract multiple elements
> x<-list(foo=1:4,bar=0.6,baz='hello')
> x[c(1,3)]
$foo
[1] 1 2 3 4

$baz
[1] "hello"

**** [[]] can be used to subset elements whose name is determined at runtime
> x<-list(foo=1:4, bar=0.6, baz='hello')
> name<-'foo'
> x[ [name] ]
[1] 1 2 3 4
> x$name
NULL
> x$foo
[1] 1 2 3 4

**** create a list
Lst<-list(name_1=object_1,...,name_m=object_m)	construct a new list

Lst<-c(list.A, list.B, list.C)	concatenate lists

Lst<-list(name="Fred",wife="Mary",no.children=3,child.ages=c(4,7,9))
Lst[ [ 1 ] ] is equivalent to Lst$name.
Lst[ [ 2 ] ] is equivalent to Lst$wife.
Lst[ [ 3 ] ] is equivalent to Lst$no.children
Lst[ [ 4 ] ][1] is equivalent to Lst$child.ages[1]

**** double subset using [[]]
> x<-list(a=list(10,12,14),b=c(3.14,2.81))
> x[ [c(1,3)] ] # the third element of the first element
[1] 14
> x[ [1] ][ [3] ]
[1] 14
> x[ [c(2,1)] ]
[1] 3.14

**** partial matching
> x<-list(aardvark=1:5)
> x$a
[1] 1 2 3 4 5
> x[ ["a"] ]
NULL
> x[ ["a",exact=FALSE] ]
[1] 1 2 3 4 5

**** list coersion
as.list()

**** list type check
is.list()

**** append to list
# Initial list:
List <- list()

# Now the new experiments
for(i in 1:3){
  List[as.character(i)] <- list(sample(1:3))
}

*** factor

if something is a factor, then typing "something" will give levels information.Besides, typing "attributes(something)"will have "levels" option. So we can use levels(something)

**** create label from factor

> f <- factor(c('a','b','a','a','a'))
> f
[1] a b a a a
Levels: a b
> c('F','M')[f]
[1] "F" "M" "F" "F" "F"

This is useful when you want to create colors and label for plots, e.g.,
plot(md,pch=c('F','M')[pData(mldat)$Gender],col=c('red','blue')[pData(mldat)$Gender])

**** Ordered or unordered, represent categorical data
> x ← factor(c(“yes”, “no”, “yes”, “no”, “yes”))
> x
[1] yes no  yes no  yes
Levels: no yes

**** frequency count of factor levels
> table(x)
x
 no yes
 2   3

**** see the underlying representation of the factor (really an integer vector)
> unclass(x)
[1] 2 1 2 1 2
attr(,"levels")
[1] "no"  "yes"
> attr(x,'levels')
[1] "no"  "yes"

**** make something a factor:
something<-factor(something)

**** setup baseline level
setup the baseline level, the baseline level would be the first level that is encountered in the “levels” option, otherwise it is determined by the alphabetic order.
> x<-factor(c("yes","no", "yes", "no"), levels=c("yes","no"))
> x
[1] yes no  yes no 
Levels: yes no
if levels is not specified, “no” is before “yes” because “n” is before “y” in the alphabetic order.
> y<-factor(c("yes","no", "yes", "no"))
> y
[1] yes no yes no 
Levels: no yes

**** generate factor levels
> f<-gl(40,10)
generate forty factor levels (each labeled as an integer), each level is repeated 10 times.

**** convert factors to numeric
t3<-as.numeric(as.character(f))
Or more efficiently, as.numberic(levels(f))[as.integer(f)]
*** data frame
heterogenous columns, as opposed to matrix.
**** convert data frame to numeric matrix
data.matrix

**** remove rows with any value NA
#+BEGIN_SRC R
na.omit(x)
#+END_SRC

**** remove rows with all values NA
#+BEGIN_SRC R
x[apply(x, 1, function(y) !all(is.na(y))),]
or 
x[rowSums(is.na(x))!=num_columns, ]
#+END_SRC

**** count the number of NAs in each row
apply(x, 1, function(z) sum(is.na(z)))
**** remove rows with more than 3 NAs
numNAs = apply(x, 1, function(z) sum(is.na(z)))
x[!(numNAs > 3),]
**** accessing column of data frame
#+BEGIN_SRC R
data <- data.frame (id=1:3, weight=c(20,27,24))
# id weight
#  1     20
#  2     27
#  3     24


# Ways to add a column
data$size      <- c("small", "large", "medium")
data[["size"]] <- c("small", "large", "medium")
data["size"]   <- c("small", "large", "medium")
data[,"size"]  <- c("small", "large", "medium")
data$size      <- 0


# Ways to remove the column
data$size      <- NULL
data[["size"]] <- NULL
data["size"]   <- NULL
data[,"size"]  <- NULL
data[3]        <- NULL
data[,3]       <- NULL
data           <- subset(data, select=-size)
#+END_SRC

**** create a dataframe
bubba<-data.frame(first=a second=b f=levels)
**** add new column
>saida["MY_NEW_COLUMN"] <- NA # That creates the new column named "MY_NEW_COLUMN" filled with "NA"
>saida$MY_NEW_COLUMN <- saida$C - saida$D  # As an example, the new column receives the result of C - D
**** rownames (or row.names) and colnames
**** rename column of data frame
names(d)[4]='ratio'

**** build a data frame in a loop, adding row by row
***** method 1
d <- data.frame()
for (i in 1:20) {d <- rbind(d,c(i+i, i*i, i/1))}

similarly
d = NULL

And in the loop:

d = rbind(d, data.frame(x, y, z))
Note: this solution is not efficient.

***** method 2
Build vectors first, dataframe after the loop

x = NULL
y = NULL
z = NULL

in the loop:

x = append(x, runif(1))
...

After loop:

d = data.frame(x, y, z)

***** method 3 (best)

Build empty data frame of desired size beforehand and fill by row

E.g. for 10 rows:
d = data.frame( x=rep(0, 10), y=rep(0,10), z=rep(0,10))

And in the loop (index i):

d[i, ] = c(x, y, z)
**** see the beginning of the data frame
head(df)

**** filter rows of a data frame
df[apply(df > 5, 1, all)]
df[apply(df > 5, 1, any)]
1 means row and “all” and “any” are logical functions.

**** create a data frame
dat <- data.frame(x1 = c(1,2,3, NA, 5), x2 = c(100, NA, 300, 400, 500))
> x<-data.frame(foo=1:4, bar=c(T,T,F,F))
> x
  foo   bar
1   1  TRUE
2   2  TRUE
3   3 FALSE
4   4 FALSE
> nrow(x)
[1] 4
> ncol(x)
[1] 2
> attributes(x)
$names
[1] "foo" "bar"

$row.names
[1] 1 2 3 4

$class
[1] "data.frame"
> attr(x, "row.names")
[1] 1 2 3 4

**** subset out NA using complete.cases (remove NA)
> airquality[1:6,]
  Ozone Solar.R Wind Temp Month Day
1    41     190  7.4   67     5   1
2    36     118  8.0   72     5   2
3    12     149 12.6   74     5   3
4    18     313 11.5   62     5   4
5    NA      NA 14.3   56     5   5
6    28      NA 14.9   66     5   6
> good<-complete.cases(airquality)
> airquality[good,][1:6,]
  Ozone Solar.R Wind Temp Month Day
1    41     190  7.4   67     5   1
2    36     118  8.0   72     5   2
3    12     149 12.6   74     5   3
4    18     313 11.5   62     5   4
7    23     299  8.6   65     5   7
8    19      99 13.8   59     5   8

**** split a data frame by a column into lists
s<-split(airquality, airquality$Month)
str(s)

**** how to remove duplicate rows
df <- data.frame(a = c(1:4, 1:4), 
                 b = c(4:1, 4:1), 
                 d = LETTERS[1:8])

df[!duplicated(df[c("a", "b")]),]
**** number of rows and columns
nrow()
ncol()
higher dimensional matrix:
NROW()
NCOL()
**** randomly sample rows
df[sample(nrow(df), 10),]

Note that this is equivalent to (and is a convenient feature of sample)
df[sample(1:nrow(df), 10),]
or
df[sample(seq(1:nrow(df)), 10),]

**** Build contigency table
table(vec)
table(vec,vec2) 2-d contingency table(matrix)

**** merge two data frame on common column
merge(probes.sampled, sampledata, by.x="pname", by.y="Composite Element REF")
*** data.table - contingency table
**** table - create table from vector
a<-c("Sometimes","Sometimes","Never","Always","Always","Sometime","Sometimes","Never")
b<-c("Maybe","Maybe","Yes","Maybe","Maybe","No","Yes","No")
results2<-table(a,b)

**** as.table - convert matrix to table
sexsmoke<-matrix(c(70,120,65,140),ncol=2,byrow=TRUE)
rownames(sexsmoke)<-c("male","female")
colnames(sexsmoke)<-c("smoke","nosmoke")
sexsmoke <- as.table(sexsmoke)

**** xtabs - convert data.frame to table
** control structure
**** if structure
if (<condition1>) {
	## do something
} else if (<condition2>) {
	## do something different
} else {
	## do something different
}

y ← if(x>3) { 10 } else { 0 }

**** for structure
for(i in 1:10) {
	print (i)
}

x ← c(“a”,”b”,”c”,”d”)
for(i in seq_along(x)) {
	print(x[i])
}

for(letter in x) {
	print(letter)
}

for(i in 1:4) print(x[i]) # omit “{}” if only one statement exists

**** loop over a matrix
x ← matrix(1:6, 2, 3)

for(i in seq_len(nrow(x))) {
	for(j in seq_len(ncol(x))) {
		print(x[i,j])
	}
}

**** while
count ← 0
while(count < 10) {
	print(count)
	count ← count + 1
}

**** more than 1 conditions
z ← 5
while(z >= 3 && z <= 10) {
	print(z)
	coin ← rbinom(1,1,0.5)

	if(coin == 1) { ## random walk
		z ← z + 1
	} else {
		z ← z – 1
	}
}

**** repeat (and break)
x0 ← 1
tol ← 1e-8
repeat {
	x1 ← computeEstimate()
	
	if(abs(x1 – x0) < tol) {
		break
	} else {
		x0 ← x1
	}
}

**** next
for(i in 1:100) {
	if (I <= 20) {
		next # skip the first 20 iterations
	}
	# Do something here.
}

*** compact looping
**** lapply – loop apply
loop over a list or a vector and evaluate a function on each element
lapply(X, FUN, ...)
lapply takes a list X, a function FUN, and other arguments via …. if X is not a list, it will be coerced using as.list.
Always return a list, list names are preserved.
> x<-list(a=1:5,b=rnorm(10))
> lapply(x,mean)
$a

[1] 3

$b
[1] 0.3474802
generate uniform random variables using lapply
> x<-1:4
> lapply(x,runif,min=0,max=10)
[ [1] ]
[1] 3.472307

[ [2] ]
[1] 1.314423 3.744869

[ [3] ]
[1] 6.314202 3.900789 6.896278

[ [4] ]
[1] 6.894134 5.549006 4.296244 4.527201

***** use anonymous function
>lapply(x, function(elt) etl[,1])

**** sapply – simplified apply
same as lapply but try to simplify the result
If the result is a list where every element is length 1, then a vector is returned.
If the result is a list where every element is a vector of the same length (>1), a matrix is returned.
If else, a list is returned.
e.g.,
> x<-list(a=1:4, b=rnorm(10),c=rnorm(20,1),d=rnorm(100,5))
> lapply(x,mean)
$a
[1] 2.5

$b
[1] -0.006170834

$c
[1] 0.5080433

$d
[1] 5.022779

> sapply(x,mean)
           a            b            c            d 
 2.500000000 -0.006170834  0.508043328  5.022778795 
but direct application of mean doesn't work
> mean(x)
[1] NA
Warning message:
In mean.default(x) : argument is not numeric or logical: returning NA

**** apply
apply a function over the margins of an array
apply a function to the rows or columns of a matrix
can be used with general arrays, e.g., taking the average of an array of matrices
> str(apply)
function (X, MARGIN, FUN, …
MARGIN is the index of the dimension that will be preserved.

***** find the means of each column
> x<-matrix(rnorm(200),20,10)
> apply(x,2,mean)
 [1]  0.31764279 -0.20359102  0.27766089 -0.11381463 -0.24245208  0.08050356  0.04293477
 [8]  0.42504710  0.17117791 -0.36513407

***** find the sums of each row
> apply(x,1,sum)
 [1] -3.9535466  3.5378914  4.8657229  2.1557528 -1.2801329 -1.6267276  1.4089528
 [8] -5.7330351  0.2595242  3.4015684  3.6012107 -3.0530614  7.9383705  4.2204701
[15]  2.0523599 -4.0927801 -2.7655184 -0.8362684 -4.0963852  1.7951365

***** shortcut functions
Other shortcut functions (which are faster than “apply”)
rowSums = apply(x, 1, sum)
rowMeans = apply(x, 1, mean)
colSums = apply(x, 2, sum)
colMeans = apply(x, 2, mean)

***** quantiles of the rows
Find quantiles of the rows of a matrix, create a matrix of two rows and 20 columns
> apply(x,1,quantile, probs=c(0.25,0.75))
          [,1]       [,2]       [,3]       [,4]       [,5]       [,6]      [,7]
25% -0.9239855 -0.1499241 -0.4175811 -0.3613111 -0.7518307 -0.8027004 -0.915204
75%  0.2370467  0.9444296  1.1890847  0.8302415  0.2443323  0.5562284  1.166624
          [,8]       [,9]     [,10]     [,11]      [,12]       [,13]      [,14]
25% -1.1292202 -1.0858680 -0.609356 -0.291738 -1.5721484 -0.08286563 -0.5172696
75%  0.1851672  0.6926818  1.569856  1.019504  0.9787008  1.38228732  1.3543220
         [,15]      [,16]      [,17]      [,18]      [,19]      [,20]
25% -0.3594793 -1.3583341 -0.6312026 -0.6608781 -1.1903123 -0.4987485
75%  0.7911442  0.5423904  0.1620909  0.4539918  0.6021955  0.8130619

***** collapse the third dimension of a 3-dimensional array
> a<-array(rnorm(2*2*10),c(2,2,10))
> apply(a,c(1,2),mean)
            [,1]       [,2]
[1,]  0.42410042 -0.3661022
[2,] -0.07362168 -0.3439171
or
> rowMeans(a,dims=2)
            [,1]       [,2]
[1,]  0.42410042 -0.3661022
[2,] -0.07362168 -0.3439171

**** tapply – table apply
***** apply a function over subsets of a vector
>str(tapply)
function (X, INDEX, FUN = NULL, ..., simplify = TRUE)
X is a vector
INDEX is a factor or a list of factors (or else they are coerced to factors)
FUN is the function to apply
> x<-c(rnorm(10),runif(10),rnorm(10,1)) 
runif(10): 10 random variables from uniform distribution [0,1].
rnorm(10,1): 10 random variables from normal distribution with mean 1.
> f<-gl(3,10) 
gl: generate levels, in this case 3 levels each repeating 10 times.
> f 
 [1] 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 
Levels: 1 2 3 
> tapply(x,f,mean) 
         1          2          3 
-0.2476370  0.5634356  0.6739376 
“do not simplify” gives back a list:
> tapply(x,f,mean, simplify=FALSE) 
$`1` 
[1] -0.247637 

$`2` 
[1] 0.5634356 

$`3` 
[1] 0.6739376 

***** find group ranges (which returns two values for each factor):
> tapply(x,f,range) 
$`1` 
[1] -1.648938  1.321278 

$`2` 
[1] 0.03362691 0.98146410 

$`3` 
[1] -1.888952  2.737393 

**** split
> str(split) 
function (x, f, drop = FALSE, …)
split a vector into a list of vectors based on a factor
x: vector (or list) or data frame
f: factor or a list of factors
drop indicates whether empty factors levels should be dropped.
> x<-c(rnorm(10),runif(10),rnorm(10,1)) 
> f<-gl(3,10) 
> split(x,f) 
$`1` 
 [1]  0.248778156  1.334741866 -0.881660069  1.628081242  0.386855405 
 [6]  2.029278781 -0.581958243  0.585995546  0.001749236 -0.126356265 

$`2` 
 [1] 0.56698467 0.98970559 0.56405445 0.09636717 0.81920102 0.30759100 
 [7] 0.28073524 0.74154150 0.11624665 0.02004426 

$`3` 
 [1]  0.26474370  0.54080495  1.64569571 -0.03437529  1.76457570  1.21412163 
 [7] -0.30767157  3.32346077  1.78569698  2.65727270 

**** lapply + split (in this case, can be substituted by tapply)
> lapply(split(x,f), mean)
$`1` 
[1] 0.4625506 

$`2` 
[1] 0.4502472 

$`3` 
[1] 1.285433

**** classify and calculate mean
calculate the mean of Ozone and Solar.R based on month
> library(datasets) 
> head(airquality) 
  Ozone Solar.R Wind Temp Month Day 
1    41     190  7.4   67     5   1 
2    36     118  8.0   72     5   2 
3    12     149 12.6   74     5   3 
4    18     313 11.5   62     5   4 
5    NA      NA 14.3   56     5   5 
6    28      NA 14.9   66     5   6
split the entire table by Month
s<-split(airquality, airquality$Month)
> lapply(s, function(x) colMeans(x[,c("Ozone","Solar.R","Wind")])) 
$`5` 
   Ozone  Solar.R     Wind 
      NA       NA 11.62258 

$`6` 
    Ozone   Solar.R      Wind 
       NA 190.16667  10.26667 

$`7` 
     Ozone    Solar.R       Wind 
        NA 216.483871   8.941935 

$`8` 
   Ozone  Solar.R     Wind 
      NA       NA 8.793548 

$`9` 
   Ozone  Solar.R     Wind 
      NA 167.4333  10.1800

***** use sapply instead of lapply
> sapply(s, function(x) colMeans(x[,c("Ozone","Solar.R","Wind")])) 
               5         6          7        8        9 
Ozone         NA        NA         NA       NA       NA 
Solar.R       NA 190.16667 216.483871       NA 167.4333 
Wind    11.62258  10.26667   8.941935 8.793548  10.1800 
remove missing value when colMeans is called
> sapply(s, function(x) colMeans(x[,c("Ozone","Solar.R","Wind")],na.rm=TRUE)) 
                5         6          7          8         9 
Ozone    23.61538  29.44444  59.115385  59.961538  31.44828 
Solar.R 181.29630 190.16667 216.483871 171.857143 167.43333 
Wind     11.62258  10.26667   8.941935   8.793548  10.18000 

***** interaction - split on more than one factors
use interaction function, make cartesian product of the two factors.
> f1<-gl(2,5) 
> f2<-gl(5,2) 
> interaction(f1,f2) 
 [1] 1.1 1.1 1.2 1.2 1.3 2.3 2.4 2.4 2.5 2.5 
Levels: 1.1 2.1 1.2 2.2 1.3 2.3 1.4 2.4 1.5 2.5 
interaction (in this case produced by using “list” in split) can create empty levels
> str(split(x, list(f1, f2)))
List of 10 
 $ 1.1: num [1:6] 0.249 1.335 0.567 0.99 0.265 ... 
 $ 2.1: num(0) 
 $ 1.2: num [1:6] -0.8817 1.6281 0.5641 0.0964 1.6457 ... 
 $ 2.2: num(0) 
 $ 1.3: num [1:3] 0.387 0.819 1.765 
 $ 2.3: num [1:3] 2.029 0.308 1.214 
 $ 1.4: num(0) 
 $ 2.4: num [1:6] -0.582 0.586 0.281 0.742 -0.308 ... 
 $ 1.5: num(0) 
 $ 2.5: num [1:6] 0.00175 -0.12636 0.11625 0.02004 1.7857 ... 

drop empty levels
> str(split(x, list(f1, f2), drop=TRUE)) 
List of 6 
 $ 1.1: num [1:6] 0.249 1.335 0.567 0.99 0.265 ... 
 $ 1.2: num [1:6] -0.8817 1.6281 0.5641 0.0964 1.6457 ... 
 $ 1.3: num [1:3] 0.387 0.819 1.765 
 $ 2.3: num [1:3] 2.029 0.308 1.214 
 $ 2.4: num [1:6] -0.582 0.586 0.281 0.742 -0.308 ... 
 $ 2.5: num [1:6] 0.00175 -0.12636 0.11625 0.02004 1.7857 ... 

**** mapply – multivariate apply
multivariate version of lapply
lapply, sapply and tapply only apply a function to a single element. Mapply applies a function in parallel over a set of arguments.
> str(mapply) 
function (FUN, ..., MoreArgs = NULL, SIMPLIFY = TRUE, USE.NAMES = TRUE)
… contains arguments to apply over
MoreArgs is a list of other arguments to FUN
> mapply(rep,1:4,4:1) 
is equivalent to
> list(rep(1,4), rep(2,3), rep(3,2),rep(4,1))
[[1]]
[1] 1 1 1 1

[[2]]
[1] 2 2 2 

[[3]] 
[1] 3 3 

[[4]] 
[1] 4 
another examle of “instant vectorization”
> noise<-function(n,mean,sd){rnorm(n,mean,sd)}
> mapply(noise,1:5,1:5,2) 
[[1]] 
[1] -0.5250608 

[[2]] 
[1] 4.684903 1.778944 

[[3]] 
[1] 7.540316 5.528719 5.501906 

[[4]] 
[1] 5.190458 5.318942 2.206801 3.286727 

[[5]] 
[1] 2.245433 1.933995 8.561300 4.059340 2.559799 
is equivalent to
list(noise(1,1,2), noise(2,2,2), noise(3,3,2), noise(4,4,2),noise(5,5,2))

** Functions
*** see all formal arguments of a function
formals

*** check out what arguments are needed
args(function)

*** arguments matching
> args(sd)
function (x, na.rm = FALSE) 
> mydata ← rnorm(100)
> sd(mydata)
> sd(x = mydata)
> sd(x = mydata, na.rm = FALSE)
> sd(na.rm = FALSE, x = mydata)
> sd(na.rm = FALSE, mydata)

*** lazy evaluation
f ← function(a, b) {
	a^2
}
f(2) # function call OK!

f ← function(a, b) {
	print(a)
	print(b)
}
f(45) # function call Error!

… argument
… is often used when extending another function and you don't want to copy the entire argument list of the original function
myplot ← function(x, y, type=”l”, …) {
	plot(x, y, type = type, …)
}

*** generic function
> mean
function (x, ...) 
UseMethod("mean")

**** "..." usage
when the number of arguments is unknown one should use … and any arguments that appear after … on the argument list must be named explicitly and cannot be partially matched.
> args(paste)
function (..., sep = " ", collapse = NULL) 
> args(cat)
function (..., file = "", sep = " ", fill = FALSE, labels = NULL, 
    append = FALSE)

define a generator function to make an optimization function to feed “optim”, “nlm”, and “optimize”
the generator function fix the data argument
make.NegLogLik <- function(data, fixed=c(FALSE, FALSE)) {
	params <- fixed
	function(p) {
		params[!fixed] <- p
		mu <- params[1]
		sigma <- params[2]
		a <- -0.5 * length(data) * log(2*pi*sigma^2)
		b <- -0.5 * sum((data-mu)^2) / (sigma^2)
		-(a+b)
	}
}

**** to use generator function to make a negative log likelihood function
> set.seed(1); normals<-rnorm(100,1,2)
> nLL<-make.NegLogLik(normals)
> nLL
function(p) {
        params[!fixed] <- p
        mu <- params[1]
        sigma <- params[2]
        a <- -0.5 * length(data) * log(2*pi*sigma^2)
        b <- -0.5 * sum((data-mu)^2) / (sigma^2)
        -(a+b)
    }
<environment: 0xb647a480> # the pointer to the defining environment
> ls(environment(nLL))
[1] "data"   "fixed"  "params"

**** to use nLL, with initial value of mu and sigma
> optim(c(mu=0, sigma=1), nLL)$par
      mu    sigma 
1.218239 1.787343 

fixing sigma = 2
> nLL<-make.NegLogLik(normals, c(FALSE,2))
> optimize(nLL,c(-1,3))$minimum
[1] 1.217775

fixing mu = 1
> nLL<-make.NegLogLik(normals, c(1,FALSE))
> optimize(nLL,c(1e-6,10))$minimum
[1] 1.800596

**** plot log likelihood function against mean
> nLL<-make.NegLogLik(normals,c(1,FALSE))
> x<-seq(1.7,1.9,len=100)
> y<-sapply(x,nLL)
> plot(x,exp(-(y-min(y))),type='l')

**** plot log likelihood function against sigma
> nLL<-make.NegLogLik(normals, c(FALSE,2))
> x<-seq(0.5,1.5,len=100)
> y<-sapply(x,nLL)
> plot(x,exp(-(y-min(y))),type="l")

*** anonymous function
suppose we have x
> x<-list(a=matrix(1:4,2,2),b=matrix(1:6,3,2))
> x
$a
     [,1] [,2]
[1,]    1    3
[2,]    2    4

$b
     [,1] [,2]
[1,]    1    4
[2,]    2    5
[3,]    3    6
and we want to grab the first column of the two matrices, we can use an anonymous function
>lapply(x, function(elt) etl[,1])

use invisible to refrain auto-printing (but still return the object)
printmessage <- function(x) {
	if (x>0)
		print(“x is greater than zero”)
	else
		print(“x is less than or equal to zero”)
	invisible(x)
}

“print” function will return the string it prints
** object
unclass()	remove the class attribute of an object
*** set attributes
attr(z,"dim")<-c(10,10)		treat z as a 10 by 10 matrix
** Statistics

描述性统计量
sum mean median var sd min max range quantile summary 
max(x),min(x),mean(x),range(x),sum(x),prod(x),length(x),
var(x),sort(x),sort.list(),order(),quantile(x),sd(x)

Summary will print out the statistics of all columns in the dataframe
summary(argument)	print the five point descriptive statistics if the argument is a factor
summary(factor)	print the frequency of each key if the argument is NOT a factor

Wilcoxon
wilcox.test(t$enzymes[t$symbiotic.!='endosymbiotic'], t$enzymes[t$symbiotic.=='endosymbiotic'], alternative='greater')

wilcox.test(t$Qnorm[t$symbiotic.!='endosymbiotic'], t$Qnorm[t$symbiotic.=='endosymbiotic'], alternative='greater')

wilcox.test(t$V3[t$V2=='Obligate'], t$V3[t$V2=='Facultative'], alternative='less')

Pearson's Chi-squared test of independence on a contingency table
chisq.test(table(survey$Smoke, survey$Exer))

chisq.test(x[,2],p=dbinom(x[,1],5,0.5))

Spearman's Correlation
cor.test(method='spearman')

*** cmdscale
#+BEGIN_SRC R

# classical multidimensional scaling
md <- cmdscale(dist(t(exprs(mldat)[fData(mldat)$CHROMOSOME=='X',])),2)
plot(md,pch=c('F','M')[pData(mldat)$Gender],col=c('red','blue')[pData(mldat)$Gender])
#+END_SRC

*** sample random variables
set.seed
> set.seed(1)
> rnorm(5)
[1] -0.6264538  0.1836433 -0.8356286  1.5952808  0.3295078
> rnorm(5)
[1] -0.8204684  0.4874291  0.7383247  0.5757814 -0.3053884
> set.seed(1)
> rnorm(5)
[1] -0.6264538  0.1836433 -0.8356286  1.5952808  0.3295078

Binomial distribution
dbinom(x,size,prob)	size is the number of trials, prob is the probability of success of each trial
pbinom(q,size,prob)
qbinom(p,size,prob)	p is the number of successes among size
rbinom(p,size,prob)

Normal distribution:
help(Normal)
1. dnorm(0,mean=0,sd=1)	returns the height of the probability distribution at each point. Assume zero-mean and one-standard deviation
x<-seq(from=-10,to=10,by=0.1)
y<-dnorm(x,mean=0,sd=1)
plot(x,y)
2. pnorm(v,mean=0,sd=1)	calculate the probability that a normally distributed random number will be less than v(cummulative probability).
3. qnorm(v,mean=0,sd=1) calculate the number whose cummulative probability is v.
4. rnorm(n,mean=0,sd=1)	gives n random numbers following the normal distribution.
Two generic functions: qqnorm(y) and qqline(y) provide a normal QQ plot of the values in 'y'.qqline is a line connecting the first and the third quartiles.
**** sample from vector with replacement
sample(vec,10,replace=TRUE)     sample from a vector
**** uniform distribution
sample
> set.seed(1)
> sample(1:10,4)
[1] 3 4 5 7
> sample(1:10,4)
[1] 3 9 8 5
> sample(letters, 5)
[1] "q" "b" "e" "x" "p"
> sample(1:10) # permutation
 [1]  4  7 10  6  9  2  8  3  1  5
> sample(1:10)
 [1]  2  3  4  1  9  5 10  8  6  7
> sample(1:10, replace=TRUE) # sample with replacement (same number may appear multiple times)
 [1] 2 9 7 8 2 8 5 9 7 8

**** normal distribution
rnorm(n, mu, sigma)
r: random number generation
sample n random variable from normal distribution
rnorm(n, mean = 0, sd = 1, log = FALSE) 
x <- rnorm(10) # generate 10 random variables from a normal distribution with mean 0 sd 1.
x <- rnorm(10,20,2) # generate 10 random variables from a normal distribution with mean 20 and sd 2.
> summary(x)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  17.39   19.22   19.97   20.05   20.70   23.11 


dnorm
d: density
evaluate density
dnorm (x, mean = 0, sd = 1, log = FALSE)

pnorm
evaluate cumulative distribution function
given value of x compute cumulative probability
pnorm (q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) 


qnorm
q: quantile function, given cumulative probability, compute value of x, the inverse function of pnorm.
qnorm (p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)

**** Poisson distribution
rpois
generate random Poisson variate with a given rate
rpois (n, lambda)
> rpois(10,1)
 [1] 0 0 1 1 2 1 1 4 1 2
> rpois(10,2)
 [1] 4 1 2 0 1 1 0 1 4 1
> rpois(10,20)
 [1] 19 19 24 23 22 24 23 20 11 22
model log.mu = 0.5 + 0.3 * x where x is standard normal distribution
> set.seed(1)
> x<-rnorm(100)
> log.mu<-0.5+0.3*x
> y<-rpois(100,exp(log.mu))
> summary(y)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
   0.00    1.00    1.00    1.55    2.00    6.00 

ppois
cumulative distribution Poisson distribution
> ppois(2,2)
[1] 0.6766764
> ppois(4,2)
[1] 0.947347
> ppois(6,2)
[1] 0.9954662

**** binomial distribution
rbinom(100, 1, 0.5)

*** other descriptive statistics

frequency count of discrete values
Tabulate frequency counts
table(d$numcc)
*** linear regression:

Before doing linear regression, one have to check if the factor attribute of the argument is removed or not first. In other words, one need to make sure x is not a factor. Otherwise, it would become an ANOVA on with categories corresponding to different levels of x.
fm<-lm(y~x,data=dummy)
fit<-lm(rate~year)
To get the interest rate in year 2015:	Note that one should use two square braces to get the number.
#+BEGIN_SRC R
fit$coefficients[[2]]*2015 + fit$coefficients[[1]]
#+END_SRC

residuals(fit)	the residuals of the fit. By definition is the difference between the each data point with the predicted value.

abline(fit)	add fitted line onto the data plot.
summary(fit)	can get the result of F-test.
fm$call
fm$coefficient: Intercept & x
*** formula and design matrix
a formula is something like: 
#+BEGIN_SRC R
log(Volume) ~ log(Height) + log(Girth)
#+END_SRC
when one supply a formula into a regression model, a function called "data.matrix" is automatically called to produce a design matrix.

*** model.matrix (build design matrix)
when one supply a formula, R automatically computes the design matrix. When doing so, R appends the intercept term, which is a column of 1's. One could suppress the intercept term by "y~0+x" or "y~x-1". All the character columns are converted to factors. Factors are then 'dummy-indexed', meaning they are splitted into levels(f)-1 variables and each variable has 0 or 1 binary values corresponding to whether the sample falls into the category or not.

#+BEGIN_SRC R

df
# A B C
# 1 2 3
# ...

model.matrix(C~log(B),df)
# (intercept) log(B)
# 1 0.693
# 1 ...

#+END_SRC

or if a column is a factor

#+BEGIN_SRC R
> df <- data.frame(A=c('a','b','c'),B=c(1,2,3))
> df
  A B
1 a 1
2 b 2
3 c 3

> model.matrix(B~A,df)
  (Intercept) Ab Ac
1           1  0  0
2           1  1  0
3           1  0  1
attr(,"assign")
[1] 0 1 1
attr(,"contrasts")
attr(,"contrasts")$A
[1] "contr.treatment"

# the first column is intercept. Only two of the three levels of A is used. Aa is used as reference group. degree of freedom = levels(A)-1

#+END_SRC

*** model.frame

extract columns and apply function
#+BEGIN_SRC R
df
# A B C
# 1 2 3
# ...

model.frame(log(B)~C,df)
# log(B) C
# 0.693 3
# ...

#+END_SRC

** Plotting
graphics package
plot, hist, boxplot

lattice package
xyplot, bwplot, levelplot

grid package
the package lattrice package depend on

grDevices package
code for implementing graphics devices such as X11, PDF, PostScript, PNG, etc.

*** histogram with y log
barplot(hist(x, plot=F)$counts,log='y')
*** quickly show a color vector
pie(rep(1,n), col=jet.colors(n))
barplot(seq(1:n), col=topo.colors(12))

*** never use palette
all it does is to let one specify color by integer index
e.g.,
palette(rainbow(6))
then one can do
col=1:30

*** get color gradient
ColorRampPalette(c("red","green"))(5)
colorRampPalette(c("#3794bf", "#FFFFFF", "#df8640"))(5)
*** basic graphics
**** common arguments
***** cex
****** cex.lab
       change label size relative to the plot
       magnification of x and y labels relative to cex
       cex.lab = 1.5
****** cex.axis
       magnification of axis annotation relative to cex
***** xlim, ylim - x, y plotting range
      plot(x, y, log='y', ylim=c(10, 20))
***** pch - plotting character
      plot character/marker/symbol
      +: 3
***** col - border color of the symbol
***** bg - fill color of the symbol
***** xlab, ylab - put labels
**** smoothScatter
smoothScatter(d$oldfitness, d$newfitness, xlab='iJR904 fitness', ylab='iAF1260 fitness', cex.lab=1.5)
***** making a log scale smoothScatter
smoothScatter(x, log(y))
instead of using
smoothScatter(x, y, log='y')
see
https://stat.ethz.ch/pipermail/r-help/2009-July/205922.html
**** plot
     plot(x,y) scatter plot
     plot(... type="h") histogram
     plot(... type="p") point graph
     plot(... type="l") line graph
     plot(... type="b") both line and points
     plot(density(x)) density plot
**** lines
     add line using lines
     x<-c(1,2,5,10,20,50,100)
     y<-sapply(x, log2)
     plot(d$numdiffcogs, d$entropy, log='x', xlab='alphabet size', ylab='entropy')
     lines(x, y, lty='dashed')
***** lty - line type
      dashed: 2
***** lwd - line width
      relative to the default (default=1). 2 is twice as wide.
**** abline
     abline(0, 1, lty='dashed')
**** title
     title(main = NULL, sub = NULL, xlab = NULL, ylab = NULL)
**** hist
hist(w1$vals)
hist(w1$vals,breaks=2)	breaks defines how many hists there are.
hist(w1$vals,breaks=4)
hist(w1$vals,breaks=12)
hist(w1$vals,breaks=12,xlim=c(0,10))	xlim defines the ranges of the x axis for display
hist(w1$vals,breaks=12,xlim=c(-1,2))
**** stripchart
stripchart(w1$vals)
stripchart(w1$vals,method="stack")
stripchart(w1$vals,method="jitter")
stripchart(w1$vals,vertical=TRUE)
stripchart(w1$vals,vertical=TRUE,method="jitter")
stripchart(w1$vals,method="stack",main='Leaf BioMass in High CO2 Environment',xlab="BioMass of Leaves"
add title and label: title('Leaf BioMass in High CO2 Environment',xlab='BioMass of Leave')
**** matplot
x<-seq(-4,4,len=101)
y<-cbind(sin(x),cos(x))
matplot(x,y,type="l",xaxt="n",main=expression(paste(plain(sin)*phi," and ",plain(cos)*phi)),ylab=expression("sin"*phi,"cos"*phi),#only 1st is taken
xlab=expression(past("Phase Angle ",phi)),
col.main="blue")
axis(1,at=c(-pi,-pi/2,0,pi/2,pi),labels=expression(-pi,-pi/2,0,pi/2,pi)) add an axis to a plot.
**** Venn's diagram
===================
library(limma)
hsb2<-read.table("http://www.ats.ucla.edu/stat/R/notes/hsb2.csv",sep=',',header=T)
attach(hsb2)
hw<-(write>=60) # the result is an array of TRUE or FALSE.
hm<-(math>=60)  # the result is an array of TRUE or FALSE.
hr<-(read>=60)  # the result is an array of TRUE or FALSE.
c3<-cbind(hw,hm,hr)
a<-vennCounts(c3)
vennDiagram(a)
===================
vennDiagram(a,include="both", names=c("High Writing","High Math","High Reading"), cex=1, counts.sol="red")
g1<-c(rep(0,6),rep(1,3))
g2<-c(rep(1,6),rep(0,3))
g<-cbind(g1,g2)
c<-vennCounts(g)
vennDiagram(c)
**** combined plots

1. Combine hist and stripchart
hist(w1$vals,main='Leaf BioMass in High CO2 Environment',xlab='BioMass of Leaves',ylim=c(0,16))
stripchart(w1$vals,add=TRUE,at=15.5)

2. Combine hist with boxplot
hist(w1$vals,main='Leaf BioMass in High CO2 Environment',xlab='BioMass of Leaves',ylim=c(0,16))
boxplot(w1$vals,horizontal=TRUE,at=15.5,add=TRUE,axes=FALSE)
boxplot(w1$vals,main='Leaf BioMass in High CO2 Environment',ylab="BioMass of Leaves",horizontal=TRUE)
boxplot(tree$STBM~tree$C)	tree$C is a factor vector.
/*FMARK_END*/

layout.show()  显示layout分布和编号
**** legend
legend(-3,.9,ex.cs1,lty=1:2,col=2:3,adj=c(0,.6))	##the components of each vector have to match. 
##col: the color vector of each component.
##lty: the line type 0=blank 1=solid 2=dashed 3=dotted 4=dotdash,5,longdash,6,twodash.
op<-par(bg="white")	get an opaque box for the legend

legend("topright",expression(paste("sin",phi),paste("cos",phi)),lty=1:1,col=c(1,2),title="right")
Or we can designate the character on the line with exact coordinate of the legend by specifying pch:
legend(-3,.9,expression(paste("sin",phi),paste("cos",phi)),lty=1:1,col=c(1,2),title="right",pch=2)

legend("topright","(x,y)",pch=1,title="right")
legend(-1,1.9,c("sin","cos","tan"),col=c(3,4,6),text.col="green4",lty=c(2,-1,1),pch=c(-1,3,4),merge=TRUE,bg='gray90')

**** mtext - margin text
mtext() add text into the margin of a plot.
mtext(c("Low","High"),side=1,line=2,at=c(5,7))
mtext(text,side=,line=,outer=FALSE,at=NA,adj=NA,padj=NA,cex=NA,col=NA,font=NA,...)
side=1(bottom),2(left),3(top),4(right)
line: Margin line starting at 0 counting outwards.
outer=TRUE,use outer margins if available.
adj,padj: used to adjust the location of the text.
col: the color to use,e.g. col=100 or col="darkred"
font: the font for text

**** text
plot(1:10,1:10)
text(4,9,expression(hat(beta)==(X^t *X)^{-1}*X^t*y))	add text to a plot.
text(4,8.4,"expression(hat(beta)==(X^t*X)^{-1}*X^t*y)",cex=.8)
text(4,7,expression(bar(x)==sum(frac(x[i],n),i==1,n))
text(4,6.4,"expression(bar(x)==sum(frac(x[i],n),i==1,n))",cex=.8)
text(8,5,expression(paste(frac(1,sigma*sqrt(2*pi))," ",plain(e)^{frac(-(x-mu)^2,2*sigma^2)})),cex=1.2)

text(x,y,labels,adj=,pos=,offset=,vfont=,cex=,col=,font=)
	adj: adjustment of relative position of the text
	pos=1,2,3,4,correspond to below, left, above, right. This overrides adj
	cex= a value with 1 for normal size. for minization and enlargement

**** the special math symbols
demo(Hershey)   see how to draw special font in detail
**** combine math and numeric variable
plot(1:10,type="n",xlab="",ylab="",main="plot math & numbers")
theta<-1.23;
mtext(bquote(hat(theta)==.(theta)))
for(i in 2:9)
text(i,i+1,substitute(list(xi,eta)==group("(",list(x,y),")"),
list(x=i,y=i+1)))
**** colors
colors(): list all the colors available and their index

**** save figure as pdf
pdf(file="my2.pdf")
<do my plot,which won't display>
dev.off()
**** add line using lines
x<-c(1,2,5,10,20,50,100)
y<-sapply(x, log2)
plot(d$numdiffcogs, d$entropy, log='x', xlab='alphabet size', ylab='entropy')
lines(x, y, lty='dashed')

change label size relative to the plot
use argument
cex.lab = 1.5
**** smoothScatter
**** par
change parameters of the plotting
multiple subplots
par(mfrow=c(2,1))

margin
par(mar=c(2,2,1,1))
bottom, left, top, right

xtick positions
xaxp=c(0,136,8)
start, end, number of ticks
***** mfrow and mfcol
multiple subplots
par(mfrow=c(1,1))
**** make x-axis label vertical
par(las=2)

**** boxplot
boxplot() 带error bar的盒装图

boxplot(list(d$numhsp[d$density < 0.5], d$numhsp[d$density >= 0.5]), log='y', names=c("density < 0.5", "density >= 0.5"), ylab="connected component size", cex.lab=1.4, cex.axis=1.4)
cex.axis change the size of label on x axis
names
labels on x-axis

another way to boxplot against some factor
boxplot(envcnt ~ gltrunc12, data=d, xlab='number of gains and losses')

*** lattice plotting
library(lattice)
package ? Lattice
library(help=lattice)
data(environmental)
?environmental
head(environmental)
xyplot(ozone ~ radiation, data=environmental)
xyplot(ozone ~ radiation, data=environmental, main=”Ozone vs. Radiation”)

#+BEGIN_SRC R
xyplot(-log10(adj.P.Val)~CHROMOSOME, tt, ylab='-log10(Adjusted P-value)', main='P-values for probes\ndistinguishing males from females')
# tt is the data.frame containing columns adj.P.Val and CHROMOSOME
#+END_SRC

*** convert R table to LaTeX table
library(xtable)
xt <- xtable(x, label="tab:chromosomepvals", caption="The median adjusted p-value ...")
digits(xt) <- 6
print(xt, include.rownames=FALSE, align='cr')
** data IO
*** read.table
**** commonly used arguments
***** file
***** header
***** sep
       default to space
***** colClasses=c("character", "integer", "numeric")
       this may speed the data import
       e.g., colClasses=”numeric”
***** nrows
***** comment.char
       default to “#”, set comment.char=”” if there are no commented lines in your file.
***** skip
       the number of lines to skip from the beginning
***** stringsAsFactors
       hould character variables be coded as factors?
**** read.table("clipboard")
      Read table from clipboard
      read.table(“clipboard”)
      One can copy a region in Excel then use this command to import to R.
**** handle large data
      d.full=read.table('/home/wanding/sync/project_HGT/data2/2013_02_04_eval2alen.tab', header=T, comment.char="", colClasses=c("character", "integer", "numeric"), stringsAsFactors=F)
** scripting
*** Old style: R CMD BATCH --args my_arg
*** New style: ./script.R my_arg
*** get command line argument
args<-(commandArgs(TRUE))
args
"lalala"
** other
*** time your code
system.time(yourcodehere)
*** how to create package
http://cran.r-project.org/doc/contrib/Leisch-CreatingPackages.pdf

to build the package tarball
R CMD build

to check the package tarball
R CMD check

 Edit the help file skeletons in 'man', possibly combining help files for multiple functions.
 Edit the exports in 'NAMESPACE', and add necessary imports.
 Put any C/C++/Fortran code in 'src'.
 If you have compiled code, add a useDynLib() directive to 'NAMESPACE'.

what "build and reload" in Rstudio do
1. Unloads any existing version of the package (including shared libraries if necessary).
2. Builds and installs the package using R CMD INSTALL.
3. Restarts the underlying R session to ensure a clean environment for re-loading the package.
4. Reloads the package in the new R session by executing the library function.

*** set default packages that are loaded on start
options(defaultPackages=c(getOption("defaultPackages"),
       "mypackage1","mypackage2", ... [etc.]))
*** type <- in Rstudio, emacs ESS
RStudio ALT -
emacs ESS SHIFT -
*** convert to LaTeX table
#+BEGIN_SRC R
library(xtable)
print(xtable(res), file="~/tab.tex", digits=2, floating=FALSE)
#+END_SRC

*** repress printing to console when a variable is not assigned
*** Get working directory
> getwd()

*** list file in the working directory
> dir()

*** list object in the work space
> ls()

*** look for help
?chisq.test

*** display the internal structure of an R object using str
diagnostic function and alternative to “summary”
> str(str)
function (object, ...)
> str(lm)
function (formula, data, subset, weights, na.action, method = "qr", 
    model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, 
    contrasts = NULL, offset, ...)  
> str(ls)
function (name, pos = -1, envir = as.environment(pos), all.names = FALSE, 
    pattern)  
> x<-rnorm(100,2,4)
> summary(x)
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
-6.0210 -0.6476  2.0160  2.1340  4.5160 14.4300 
> str(x)
 num [1:100] -0.61 3.586 2.446 -0.265 2.366 …

*** Scoping rules
to see the search paths (and their priorities). The global environment (or the user's workspace) is always the first on the search list and base package is always the last.
> search()
 [1] ".GlobalEnv"        "tools:rstudio"     "package:stats"    
 [4] "package:graphics"  "package:grDevices" "package:utils"    
 [7] "package:datasets"  "package:methods"   "Autoloads"        
[10] "package:base"   
library() put the package to the second position of the search list
> library(lattice)
> search()
 [1] ".GlobalEnv"        "package:lattice"   "tools:rstudio"    
 [4] "package:stats"     "package:graphics"  "package:grDevices"
 [7] "package:utils"     "package:datasets"  "package:methods"  
[10] "Autoloads"         "package:base"    
function and non-function are separated. One can have a function c and a object called c simultaneously.

*** Lexical scoping
the values of free variables are searched for in the environment in which the function was defined.
A function + an environment = a closure or function closure.
Keep looking back in the search list.
> f<-function(x) {x*x}
> environment(f)
<environment: R_GlobalEnv>
> parent.env(environment(f))
<environment: package:lattice>
attr(,"name")
[1] "package:lattice"
attr(,"path")
[1] "/usr/lib/R/library/lattice"

*** free variables are searched in the parent closure
> make.power <- function(n) {pow <-function(x) {x^n}; pow}
> cube <- make.power(3)
> square <- make.power(2)
> cube(3)
[1] 27
> square(3)
[1] 9
> ls(environment(cube))
[1] "n"   "pow"
> get('n',environment(cube))
[1] 3
> ls(environment(square))
[1] "n"   "pow"
> get('n',environment(square))
[1] 2

*** debugging tools
Message, warning, error, condition
condition: a generic concept for indicating that something unexpected can occur; programmers can create their own conditions.
Warning
>log(-1)
[1] NaN
Warning message: 
In log(-1) : NaNs produced 

*** traceback
prints out the function call stack, does nothing if there's no error
> mean(x)
Error in mean(x) : object 'x' not found
> traceback()
1: mean(x)
or
> lm(y~x)
Error in eval(expr, envir, enclos) : object 'y' not found
> traceback()
7: eval(expr, envir, enclos)
6: eval(predvars, data, env)
5: model.frame.default(formula = y ~ x, drop.unused.levels = TRUE)
4: model.frame(formula = y ~ x, drop.unused.levels = TRUE)
3: eval(expr, envir, enclos)
2: eval(mf, parent.frame())
1: lm(y ~ x)

*** debug
flag a function for “debug” mode which allows you to step through execution of a function one line a time
n: next line
Q: quit
c: continue without single stepping
where: show call stack
> debug(lm)
> lm(y~x)
debugging in: lm(y ~ x)
debug: {
    ret.x <- x
    ret.y <- y
…
    if (!qr) 
        z$qr <- NULL
    z
}
Browse[2]> n
debug: ret.x <- x
Browse[2]> n
debug: ret.y <- y

*** browser
suspend the execution of a function wherever it is called and puts the function in debug mode
the browser is just like your R work space (interactive shell)

*** recover
allows you to modify the error behavior so that you can browse the function call stack
> options(error=recover)
> read.csv("nosuchfile")
Error in file(file, "rt") : cannot open the connection
In addition: Warning message:
In file(file, "rt") :
  cannot open file 'nosuchfile': No such file or directory

Enter a frame number, or 0 to exit   

1: read.csv("nosuchfile")
2: read.table(file = file, header = header, sep = sep, quote = quote, dec = dec, fill 
3: file(file, "rt")

Selection: 1
select a level in the call stack to see around, in this case, read.csv, for example. The control is handed over to browse.
Called from: top level 
Browse[1]> Q

*** edit spreadsheet manually
x <- edit(data.frame()) # Starts empty GUI spreadsheet editor for manual data entry
x <- edit(x) # Opens existing data frame (table) 'x' in GUI spreadsheet editor.
x <- scan(w="c") # Lets you enter values from the keyboard or by copy&paste and assigns them to vector 'x'.
*** get help
example(topic)

?help.search("chi-square")	fuzzy-match search for chi-square
help.search(igraph)	to search for topic in documents
help("[[")
?"[["
vignette("sandwich")	## a brief introduction to the package sandwich
*** attach data
attach(data1)	使data1的列自动储入，无须data1$...
*** clear workspace
rm(list=ls())
*** change directory
getwd()
setwd(dir)

*** set library path
To change libPath temporarily:
.libPaths(c("/usr/lib/R/library","/usr/share/R/library"))

permanently add the library path:
in ~/.Renviron add R_LIBS="home/wan/lib/R"

*** use html help
options(htmlhelp=FALSE)
*** list files in current dir
list.files(pattern=".pairs")	list all the files satisfying some pattern in the current directory.
file.show(filepath)	display the file indicated in the filepath
** parallel
#+BEGIN_SRC R
mclapply(x, func)

#+END_SRC

* Python

** bokeh

*** heatmap example

from bokeh.charts import HeatMap, output_file, show
from bokeh.sampledata.unemployment1948 import data

# pandas magic
# df = data[data.columns[:-2]]
# df2 = df.set_index(df[df.columns[0]].astype(str))
# df2.drop(df.columns[0], axis=1, inplace=True)
# df3 = df2.transpose()

output_file("2015_05_05_450_orphan_CpG_pmd_byseg_heatmap.html")

hm = HeatMap(tumor_betas_pmd.iloc[1:100,1:100], title="categorical heatmap", width=800)
show(hm)

** pip

install to local directory
pip install --user toolshed

upgrade
pip install --upgrade cutadapt

** pandas

*** rpy2 interface
http://rpy.sourceforge.net/rpy2/doc-2.5/html/overview.html

*** convert pandas data frame to R
import pandas,rpy,common as com
r_dataframe = com.convert_to_r_dataframe(df)

see
http://pandas.pydata.org/pandas-docs/dev/r_interface.html

*** map in data frame
applymap

*** plot legend outside

df.myCol.plot().legend(loc='center left', bbox_to_anchor=(1, 0.5))

*** how to do string operation to index
df[df.index.to_series().str.contains('LLChit')]
** difference between numpy.array and numpy.matrix
numpy.matrix is strictly 2-d
please use numpy.array most of the time
** to set python default install location
edit
~/.pydistutils.cfg
====
[easy_install]

# set the default location to install packages
install_dir = /scratch/bcb/wzhou1/tools/pythonlib
====
** python oneline if
if condition:
    do_something()


if condition: do_something()

if condition:
    do_something()
    do_something_else()

if condition: do_something(); do_something_else()
** numpy.array how to index elements individually
https://scipy-lectures.github.io/intro/numpy/array_object.html

b = np.ones(6)
b[ [1,2]] += 3

# note that it is a [1,2] not a (1,2) or 1,2
b
array([ 1.,  4.,  4.,  1.,  1.,  1.])

# one can also reuse index
b[ [,2,2,5]]
array([ 1.,  3.,  3.,  1.])
** install python package to local location
python setup.py install --user

** wait for enqueued tasks to be completed

https://docs.python.org/2/library/queue.html#Queue.Queue
#+BEGIN_SRC python
def worker():
    while True:
        item = q.get()
        do_work(item)
        q.task_done()

q = Queue() # equivalent to multiprocessing.JoinableQueue
for i in range(num_worker_threads):
     t = Thread(target=worker)
     t.daemon = True
     t.start()

for item in source():
    q.put(item)

q.join()       # block until all tasks are done
#+END_SRC

** information entropy

#+BEGIN_SRC python
  def entropy(labels):
      """ Computes entropy of label distribution. """
      n_labels = float(len(labels))
  
      if n_labels <= 1:
          return 0
  
      counter = collections.Counter(labels)
      probs = [ _ / n_labels for _ in counter.values()]
      n_classes = np.count_nonzero(probs)
  
      if n_classes <= 1:
          return 0
  
      ent = 0.
  
      # Compute standard entropy.
      for i in probs:
          ent -= i * math.log(i, n_classes)
  
      return ent
  
#+END_SRC

** how to time a one-liner code snippet?

#+BEGIN_SRC python
python -m timeit "i=5" "i *= i"
#+END_SRC

** how to get logic AND for all elements in a list
all(list)
any(list) for OR logic
** matplotlib ggplot style
#+BEGIN_SRC 

from mpltools import style
from mpltools import layout

style.use('ggplot')

figsize = layout.figaspect(scale=1.2)
fig, axes = plt.subplots(ncols=2, nrows=2, figsize=figsize)
ax1, ax2, ax3, ax4 = axes.ravel()

# scatter plot (Note: `plt.scatter` doesn't use default colors)
x, y = np.random.normal(size=(2, 200))
ax1.plot(x, y, 'o')

# sinusoidal lines with colors from default color cycle
L = 2*np.pi
x = np.linspace(0, L)
ncolors = len(plt.rcParams['axes.color_cycle'])
shift = np.linspace(0, L, ncolors, endpoint=False)
for s in shift:
    ax2.plot(x, np.sin(x + s), '-')
ax2.margins(0)

# bar graphs
x = np.arange(5)
y1, y2 = np.random.randint(1, 25, size=(2, 5))
width = 0.25
ax3.bar(x, y1, width)
ax3.bar(x+width, y2, width, color=plt.rcParams['axes.color_cycle'][2])
ax3.set_xticks(x+width)
ax3.set_xticklabels(['a', 'b', 'c', 'd', 'e'])

# circles with colors from default color cycle
for i, color in enumerate(plt.rcParams['axes.color_cycle']):
    xy = np.random.normal(size=2)
    ax4.add_patch(plt.Circle(xy, radius=0.3, color=color))
ax4.axis('equal')
ax4.margins(0)

# Remove ticks on top and right sides of plot
for ax in axes.ravel():
    layout.cross_spines(ax=ax)

plt.show()
#+END_SRC

** how to get a variable / object by its name as a string?
there are two ways.
1. globals()['var']
2. eval('var') # this is not very safe.
*** spearman r
      from scipy.stats import spearmanr
      rho, p = spearmanr(x, y)
      print rho, p

** customize matplotlibrc file
cp /etc/matplotlibrc ~/.matplotlib/
then edit .matplotlib/matplotlibrc

** semantics
*** how to write python main?
#+begin_src python
  def main():
      pass
  # main code goes here
  if __name__ == '__main__':
      main()
#+end_src

**** 更加fancy的写法

#+begin_src python
  import sys
  
  def main(*args):
      try:
        # some code there
        pass
    except:
        # handle some exceptions
        pass
    else:
        return 0 # exit errorlessly
  
  if __name__ == '__main__':
      sys.exit(main(*sys.argv))
#+end_src

*** how to avoid cyclic import
Imports are pretty straightforward really. Just remember the following:

'import' and 'from xxx import yyy' are executable statements. They execute when the running program reaches that line.

If a module is not in sys.modules, then an import creates the new module entry in sys.modules and then executes the code in the module. It does not return control to the calling module until the execution has completed.

If a module does exist in sys.modules then an import simply returns that module whether or not it has completed executing. That is the reason why cyclic imports may return modules which appear to be partly empty.

Finally, the executing script runs in a module named __main__, importing the script under its own name will create a new module unrelated to __main__.

Take that lot together and you shouldn't get any surprises when importing modules.
**** difference between 'import' and 'from xxx import yyy'
      from xxx import yyy need the entire script of xxx be compiled(or executed) before import yyy.
*** how to writing an extension
     The following discuss the reference counting:
     http://bytes.com/topic/python/answers/158440-py_decref-question

*** how to sort a list based on the order of another list?
I have a list of strings like this:

X = ["a", "b", "c", "d", "e", "f", "g", "h", "i"]
Y = [ 0,   1,   1,    0,   1,   2,   2,   0,   1]

What is the shortest way of sorting X using values from Y to get the following output?

    ["a", "d", "h", "b", "c", "e", "i", "f", "g"]

The order for the elements having the same "key" does not matter. I can resort to using for constructs but I am curious if there is a shorter way. Any suggestions?

[x for (y,x) in sorted(zip(y,x))]

This is due to python's behavior of sorting list of tuples.
It automatically sorts a list of tuples by the first elements in the tuples, then by the second elements and so on tuple([1,2,3]) will go before tuple([1,2,4]). If you want to override this behaviour pass a callable as the second argument to the sort method. This callable should return 1, -1, 0.

*** elementwise operation of two lists
[x+y for x,y in zip(list1, list2)]
*** how to count occurrence (estimate frequency) of each item in the list?
**** method 1 (best)
use collections.Counter
import collections
a = [1,1,1,1,2,2,2,2,3,3,4,5,5]
counter=collections.Counter(a)
print(counter)
# Counter({1: 4, 2: 4, 3: 2, 5: 2, 4: 1})
print(counter.values())
# [4, 4, 2, 1, 2]
print(counter.keys())
# [1, 2, 3, 4, 5]
print(counter.most_common(3))
# [(1, 4), (2, 4), (3, 2)]
**** method 2
use itertools.groupby
a = [1,1,1,1,2,2,2,2,3,3,4,5,5]
from itertools import groupby
[len(list(group)) for key, group in groupby(a)]

Output:

[4, 4, 2, 1, 2]
**** method 3
use defaultdict

from collections import defaultdict

words = "apple banana apple strawberry banana lemon"

d = defaultdict(int)
for word in words.split():
    d[word] += 1

This runs in O(n).

*** dot product
use operator.mul
sum(map( operator.mul, vector1, vector2))
there should be better methods
*** how to sort dictionary according to value
**** solution 1
import operator
x = {1:2, 3:4, 4:3, 2:1, 0:0}
sorted_x = sorted(x.items(), key=operator.itemgetter(1))
**** solution 2
from operator import itemgetter
sorted(d.items(), key=itemgetter(1))
**** solution 3
sorted([(value,key) for (key, value) in mydict.items()])
**** solution 4
sorted(dictionaryObject.items(), key=lambda x:x[1])** 字符编码声明
  # -*- coding: utf-8 -*- 

*** get docstring of an object via __doc__ 
>>>print object.__doc__
*** environment variables related to the python interpreter
#!/usr/bin/python
Or
#!/usr/bin/env python
Or
$ set PYTHONINSPECT = 1
$ export PYTHONINSPECT
$ python sample.py
Or
env PYTHONINSPECT = 1 sample.py
*** command line options of python interpreter
D       PYTHONDEBUG     generate debug information
-I      PYTHONINSPECT   Cause the interpreter to go into interactive mode after executing the script
-O      PYTHONOPTIMIZE  optimize the bytecode
-OO     optimize and also remove the document string
-S      give warning when using tab as indentation
-u      PYTHONUNBEFFERED        Force the standard output and error filehandle to operate unbuffered.
-v      PYTHONVERBOSE   generate information about modules imported.
-x      skip the first line
-X      disable class-based exceptions
-c cmd  cmd is used as the script source instead of a source file
-       read the source file from standard input.
*** how to do type coerce
>>>print 5/12
0
>>>print 5.0/12
0.41666666667
int()
float()
coerce(a,b)     coerce the type of a into the type of b
*** numbers:
integer = -1234         32 bits long
decimal = 255
hexadeimal = 0xff       0x is prefix
octal = 0377            0(zero) is prefix
long = 1234L            L is suffix, arbitrary long
float = 1234.5678
scientific = -12.34E-56
complex = 1.2+3.4j      j or J is suffix

** when to use, "open" vs "file"?
always use "open"
http://stackoverflow.com/questions/112970/python-when-to-use-file-vs-open

** how to zip?
zip([1,2,3],[4,5,6],[7,8,9]) = [1,4,7], [2,5,8], [3,6,9]
** string
s.split()  with no argument, it splits on whitespace.
s.ljust(n)  pad the string on the right until n characters if string longer than n, leave the string untruncated.
** numpy.take
x = [1,2,3,4]
numpy.take(x, [1,3])		# return [2,4]
** scipy
*** cluster
**** hierarchy
***** pdist
       takes m by n ndarray
       regard the array as m data points each of dimension n
       compute m by m distance matrix
       the matrix is in a condensed form.
       has shape: (m(m-1)/2, )
       not (m,m)
       but can be converted to (m,m) by using squareform() function
***** squareform
***** linkage
       compute the dendrogram connectivity from the distance matrix
***** dendrogram
*** k-mean clustering
#+BEGIN_SRC python
from pylab import plot,show
from numpy import vstack,array
from numpy.random import rand
from scipy.cluster.vq import kmeans,vq

# data generation
data = vstack((rand(150,2) + array([.5,.5]),rand(150,2)))

# computing K-Means with K = 2 (2 clusters)
centroids,_ = kmeans(data,2)
# assign each sample to a cluster
idx,_ = vq(data,centroids)

# some plotting using numpy's logical indexing
plot(data[idx==0,0],data[idx==0,1],'ob',
     data[idx==1,0],data[idx==1,1],'or')
plot(centroids[:,0],centroids[:,1],'sg',markersize=8)
show()
#+END_SRC

split into 3 clusters
#+BEGIN_SRC python
# now with K = 3 (3 clusters)
centroids,_ = kmeans(data,3)
idx,_ = vq(data,centroids)

plot(data[idx==0,0],data[idx==0,1],'ob',
     data[idx==1,0],data[idx==1,1],'or',
     data[idx==2,0],data[idx==2,1],'og') # third cluster points
plot(centroids[:,0],centroids[:,1],'sm',markersize=8)
show()
#+END_SRC

** shlex
*** split

** random
*** sample
*** shuffle
** os
*** path
     tutorial <http://www.doughellmann.com/PyMOTW/ospath/>
     getcwd
     expanduser('~/Dropbox/')
     join
     commprefix
     dirname
     split
     splitext
     basename
     expandvars('/path/to/$MYVAR')
     normpath
     abspath
*** listdir
*** sep
*** extsep
*** pardir
*** curdir
*** mkdir
** sys
*** how to pass command line argument
import sys      # this is required.
filename        sys.argv[0]
the 1st argument   sys.argv[1]
** itertools
*** itertools.groupby
     http://stackoverflow.com/questions/893417/item-frequency-count-in-python
*** combinations
** datetime
*** to time python code
from datetime import datetime
time1 = datetime.now()
time2 = datetime.now()
dt = time2 - time1		# a timedelta object
str(dt)			# the human readable format
   
** rpy
    from rpy import r
    print r.summary(some_vector)
** matplotlib
*** bar
     plt.bar(left_ends_of_each_bar, heights_of_each_bar)
     plt.xticks([_+bar_width for _ in left_ends_of_each_bar], bar_labels)
*** Set xticks and xticklabels
ax=plt.gca()
ax.set_xticks([<a list of numbers>])
ax.set_xticklabels([<a list of labels>])
*** set the color of boxplot
#+begin_src python
    def setcolor(ob, col):
        for item in ['medians', 'fliers', 'whiskers', 'boxes', 'caps']:
            plt.setp(ob[item], color=col)

        return

    b1=plt.boxplot([[vaf[0] for vaf in vaf_seq] for vaf_seq in vaf_seqs], 
                   positions=[_-0.25 for _ in xrange(1, len(seq_depth_seq)+1)], widths=0.2)
    setcolor(b1, "magenta")
#+end_src

*** imshow and pcolor
** statistics
*** linear regression
      from scipy import polyfit, polyval
      (ar, br) = polyfit(x, y, 1)
      xr=polyval([ar,br], x)
      print ar, br

** ete2
*** Tree rendering
t=Tree(..)
ts=TreeStyle()
ts.layout_fn=mylayout
ts.show_leaf_name=False
t.render(“outputfile.png”, tree_style=ts)

*** Layout function
def mylayout(node):
	faces.add_face_to_node(faces.AttrFace(“lcamapped”, fsize=7, fgcolor=”red”), node, column=0, position=”branch-top”)

*** NodeStyle
nstyle=NodeStyle()
nstyle['shape']='sphere'
for n in t.traverse():
n.set_style(nstyle)
** thread and threading
[2013-07-21 Sun]
Python 标准库提供了 thread 和 threading 两个模块来对多线程进行支持。其中， thread 模块以低级、原始的方式来处理和控制线程，而 threading 模块通过对 thread 进行二次封装，提供了更方便的 api 来处理线程。 虽然使用 thread 没有 threading 来的方便，但它更灵活。今天先介绍 thread 模块的基本使用，
*** thread
#+begin_src python
  #coding=gbk  
  import thread, time, random  
  count = 0  
  def threadTest():  
      global count  
      for i in xrange(10000):  
          count += 1  
  for i in range(10):  
      thread.start_new_thread(threadTest, ()) #如果对start_new_thread函数不是很了解，不要着急，马上就会讲解  
  time.sleep(3)  
  print count #count是多少呢？是10000 * 10 吗？ 
#+end_src
*** threading
Thread 是threading模块中最重要的类之一，可以使用它来创建线程。有两种方式来创建线程：一种是通过继承Thread类，重写它的run方法；另一种是创建一个threading.Thread对象，在它的初始化函数（__init__）中将可调用对象作为参数传入。下面分别举例说明。先来看看通过继承threading.Thread类来创建线程的例子：
#+begin_src python
  #coding=gbk  
  import threading, time, random  
  count = 0  
  class Counter(threading.Thread):  
      def __init__(self, lock, threadName):  
          '''''@summary: 初始化对象。 
           
          @param lock: 琐对象。 
          @param threadName: 线程名称。 
          '''  
          super(Counter, self).__init__(name = threadName)  #注意：一定要显式的调用父类的初始  
  化函数。  
          self.lock = lock  
        
      def run(self):  
          '''''@summary: 重写父类run方法，在线程启动后执行该方法内的代码。 
          '''  
          global count  
          self.lock.acquire()  
          for i in xrange(10000):  
              count = count + 1  
          self.lock.release()
  
  lock = threading.Lock()  
  for i in range(5):   
      Counter(lock, "thread-" + str(i)).start()  
  time.sleep(2)   #确保线程都执行完毕  
  print count
#+end_src
在代码中，我们创建了一个Counter类，它继承了threading.Thread。初始化函数接收两个参数，一个是琐对象，另一个是线程的名称。在Counter中，重写了从父类继承的run方法，run方法将一个全局变量逐一的增加10000。在接下来的代码中，创建了五个Counter对象，分别调用其start方法。最后打印结果。这里要说明一下run方法 和start方法: 它们都是从Thread继承而来的，run()方法将在线程开启后执行，可以把相关的逻辑写到run方法中（通常把run方法称为活动[Activity]。）；start()方法用于启动线程。

*** what's 线程安全?
如果你的代码所在的进程中有多个线程在同时运行，而这些线程可能会同时运行这段代码。如果每次运行结果和单线程运行的结果是一样的，而且其他的变量的值也和预期的是一样的，就是线程安全的。

或者说:一个类或者程序所提供的接口对于线程来说是原子操作或者多个线程之间的切换不会导致该接口的执行结果存在二义性,也就是说我们不用考虑同步的问题。

比如一个 ArrayList 类，在添加一个元素的时候，它可能会有两步来完成：1. 在 Items[Size] 的位置存放此元素；2. 增大 Size 的值。
在单线程运行的情况下，如果 Size = 0，添加一个元素后，此元素在位置 0，而且 Size=1；
而如果是在多线程情况下，比如有两个线程，线程 A 先将元素存放在位置 0。但是此时 CPU 调度线程A暂停，线程 B 得到运行的机会。线程B也向此 ArrayList 添加元素，因为此时 Size 仍然等于 0 （注意哦，我们假设的是添加一个元素是要两个步骤哦，而线程A仅仅完成了步骤1），所以线程B也将元素存放在位置0。然后线程A和线程B都继续运行，都增加 Size 的值。
那好，我们来看看 ArrayList 的情况，元素实际上只有一个，存放在位置 0，而 Size 却等于 2。这就是“线程不安全”了。
*** what cause 线程安全问题
线程安全问题都是由全局变量及静态变量引起的。
  
若每个线程中对全局变量、静态变量只有读操作，而无写操作，一般来说，这个全局变量是线程安全的；若有多个线程同时执行写操作，一般都需要考虑线程同步，否则的话就可能影响线程安全。
*** Python：使用threading模块实现多线程编程一[综述]
Python这门解释性语言也有专门的线程模型，Python虚拟机使用GIL（Global Interpreter Lock，全局解释器锁）来互斥线程对共享资源的访问，但暂时无法利用多处理器的优势。

在Python中我们主要是通过thread和threading这两个模块来实现的，其中Python的threading模块是对thread做了一些包装的，可以更加方便的被使用，所以我们使用 threading模块实现多线程编程。这篇文章我们主要来看看Python对多线程编程的支持。

在语言层面，Python对多线程提供了很好的支持，可以方便地支持创建线程、互斥锁、信号量、同步等特性。下面就是官网上介绍threading模块的基本资料及功能：

*实现模块*
thread：多线程的底层支持模块，一般不建议使用；
threading：对thread进行了封装，将一些线程的操作对象化。

*threading模块*
Thread 线程类，这是我们用的最多的一个类，你可以指定线程函数执行或者继承自它都可以实现子线程功能；
Timer与Thread类似，但要等待一段时间后才开始运行；
Lock 锁原语，这个我们可以对全局变量互斥时使用；
RLock 可重入锁，使单线程可以再次获得已经获得的锁；
Condition 条件变量，能让一个线程停下来，等待其他线程满足某个“条件”；
Event 通用的条件变量。多个线程可以等待某个事件发生，在事件发生后，所有的线程都被激活；
Semaphore为等待锁的线程提供一个类似“等候室”的结构；
BoundedSemaphore 与semaphore类似，但不允许超过初始值；
Queue：实现了多生产者（Producer）、多消费者（Consumer）的队列，支持锁原语，能够在多个线程之间提供很好的同步支持。

*其中Thread类*
是你主要的线程类，可以创建进程实例。该类提供的函数包括：
getName(self) 返回线程的名字
isAlive(self) 布尔标志，表示这个线程是否还在运行中
isDaemon(self) 返回线程的daemon标志
join(self, timeout=None) 程序挂起，直到线程结束，如果给出timeout，则最多阻塞timeout秒
run(self) 定义线程的功能函数
setDaemon(self, daemonic) 把线程的daemon标志设为daemonic
setName(self, name) 设置线程的名字
start(self) 开始线程执行

*其中Queue提供的类*
Queue队列
LifoQueue后入先出（LIFO）队列
PriorityQueue 优先队列

接下来的一系列文章，我们将会用一个一个示例来展示threading的各个功能，包括但不限于：两种方式起线程、threading.Thread类的重要函数、使用Lock互斥及RLock实现重入锁、使用Condition实现生产者和消费者模型、使用Event和Semaphore多线程通信。

*** Python：使用threading模块实现多线程编程二[两种方式起线程]
在Python中我们主要是通过thread和threading这两个模块来实现的，其中Python的threading模块是对thread做了一些包装的，可以更加方便的被使用，所以我们使用threading模块实现多线程编程。一般来说，使用线程有两种模式，一种是创建线程要执行的函数，把这个函数传递进Thread对象里，让它来执行；另一种是直接从Thread继承，创建一个新的class，把线程执行的代码放到这个新的 class里。
**** 将函数传递进Thread对象
#+begin_src python
  '''''  
  Created on 2012-9-5  
     
  @author:  walfred 
  @module: thread.ThreadTest1  
  @description: 
  '''    
  import threading  
    
  def thread_fun(num):  
      for n in range(0, int(num)):  
          print " I come from %s, num: %s" %( threading.currentThread().getName(), n)  
    
  def main(thread_num):  
      thread_list = list();  
      # 先创建线程对象  
      for i in range(0, thread_num):  
          thread_name = "thread_%s" %i  
          thread_list.append(threading.Thread(target = thread_fun, name = thread_name, args = (20,)))  
        
      # 启动所有线程     
      for thread in thread_list:  
          thread.start()  
        
      # 主线程中等待所有子线程退出  
      for thread in thread_list:  
          thread.join()  
    
  if __name__ == "__main__":  
      main(3)  
#+end_src
程序启动了3个线程，并且打印了每一个线程的线程名字，这个比较简单吧，处理重复任务就派出用场了，下面介绍使用继承threading的方式；

**** 继承自threading.Thread类
#+begin_src python
    ''''' 
    Created on 2012-9-6 
      
    @author: walfred 
    @module: thread.ThreadTest2 
    '''  
      
    import threading  
      
    class MyThread(threading.Thread):  
        def __init__(self):  
            threading.Thread.__init__(self);  
              
        def run(self):  
            print "I am %s" %self.name  
              
    if __name__ == "__main__":  
        for thread in range(0, 5):  
            t = MyThread()  
            t.start()  
#+end_src
接下来的文章，将会介绍如何控制这些线程，包括子线程的退出，子线程是否存活及将子线程设置为守护线程(Daemon)。
*** Python：使用threading模块实现多线程编程三[threading.Thread类的重要函数]
这篇文章主要介绍threading模块中的主类Thread的一些主要方法，实例代码如下：

#+begin_src python
'''''  
Created on 2012-9-7  
   
@author:  walfred 
@module: thread.ThreadTest3  
@description: 
'''    
import threading  
  
class MyThread(threading.Thread):  
    def __init__(self):  
        threading.Thread.__init__(self)  
      
    def run(self):  
        print "I am %s" % (self.name)  
      
if __name__ == "__main__":  
    for i in range(0, 5):  
        my_thread = MyThread()  
        my_thread.start()
#+end_src

1 name相关
你可以为每一个thread指定name，默认的是Thread-No形式的，如上述实例代码打印出的一样：
---
I am Thread-1
I am Thread-2
I am Thread-3
I am Thread-4
I am Thread-5
---
当然你可以指定每一个thread的name，这个通过setName方法，代码：

#+begin_src python
  def __init__(self):  
      threading.Thread.__init__(self)  
      self.setName("new" + self.name)  
#+end_src

2 join方法
join方法原型如下，这个方法是用来阻塞当前上下文，直至该线程运行结束：

#+begin_src python
  def join(self, timeout=None):  
#+end_src

3 setDaemon方法
当我们在程序运行中，执行一个主线程，如果主线程又创建一个子线程，主线程和子线程就分兵两路，当主线程完成想退出时，会检验子线程是否完成。如果子线程未完成，则主线程会等待子线程完成后再退出。但是有时候我们需要的是，只要主线程完成了，不管子线程是否完成，都要和主线程一起退出，这时就可以用setDaemon方法，并设置其参数为True。

*** Python：使用threading模块实现多线程编程四[使用Lock互斥锁]
前面已经演示了Python：使用threading模块实现多线程编程二两种方式起线程和Python：使用threading模块实现多线程编程三threading.Thread类的重要函数，这两篇文章的示例都是演示了互不相干的独立线程，现在我们考虑这样一个问题：假设各个线程需要访问同一公共资源，我们的代码该怎么写？

#+begin_src python
''''' 
Created on 2012-9-8 
  
@author: walfred 
@module: thread.ThreadTest3 
'''  
import threading  
import time  
   
counter = 0  
   
class MyThread(threading.Thread):  
    def __init__(self):  
        threading.Thread.__init__(self)  
      
    def run(self):  
        global counter  
        time.sleep(1);  
        counter += 1  
        print "I am %s, set counter:%s" % (self.name, counter)  
      
if __name__ == "__main__":  
    for i in range(0, 200):  
        my_thread = MyThread()  
        my_thread.start()
#+end_src
解决上面的问题，我们兴许会写出这样的代码，我们假设跑200个线程，但是这200个线程都会去访问counter这个公共资源，并对该资源进行处理(counter += 1)，代码看起来就是这个样了，但是我们看下运行结果：
---
I am Thread-69, set counter:64
I am Thread-73, set counter:66I am Thread-74, set counter:67I am Thread-75, set counter:68I am Thread-76, set counter:69I am Thread-78, set counter:70I am Thread-77, set counter:71I am Thread-58, set counter:72I am Thread-60, set counter:73I am Thread-62, set counter:74I am Thread-66, set counter:75I am Thread-70, set counter:76I am Thread-72, set counter:77I am Thread-79, set counter:78I am Thread-71, set counter:78
---
打印结果我只贴了一部分，从中我们已经看出了这个全局资源(counter)被抢占的情况，问题产生的原因就是没有控制多个线程对同一资源的访问，对数据造成破坏，使得线程运行的结果不可预期。这种现象称为“线程不安全”。在开发过程中我们必须要避免这种情况，那怎么避免？这就用到了我们在综述中提到的互斥锁了。

*互斥锁概念*
Python编程中，引入了对象互斥锁的概念，来保证共享数据操作的完整性。每个对象都对应于一个可称为" 互斥锁" 的标记，这个标记用来保证在任一时刻，只能有一个线程访问该对象。在Python中我们使用threading模块提供的Lock类。

我们对上面的程序进行整改，为此我们需要添加一个互斥锁变量mutex = threading.Lock()，然后在争夺资源的时候之前我们会先抢占这把锁mutex.acquire()，对资源使用完成之后我们在释放这把锁mutex.release()。代码如下：

#+begin_src python
  ''''' 
  Created on 2012-9-8 
    
  @author: walfred 
  @module: thread.ThreadTest4 
  '''  
    
  import threading  
  import time  
     
  counter = 0  
  mutex = threading.Lock()  
     
  class MyThread(threading.Thread):  
      def __init__(self):  
          threading.Thread.__init__(self)  
        
      def run(self):  
          global counter, mutex  
          time.sleep(1);  
          if mutex.acquire():  
              counter += 1  
              print "I am %s, set counter:%s" % (self.name, counter)  
              mutex.release()  
        
  if __name__ == "__main__":  
      for i in range(0, 100):  
          my_thread = MyThread()  
          my_thread.start()
#+end_src

*同步阻塞*
当一个线程调用Lock对象的acquire()方法获得锁时，这把锁就进入“locked”状态。因为每次只有一个线程1可以获得锁，所以如果此时另一个线程2试图获得这个锁，该线程2就会变为“blo同步阻塞状态。直到拥有锁的线程1调用锁的release()方法释放锁之后，该锁进入“unlocked”状态。线程调度程序从处于同步阻塞状态的线程中选择一个来获得锁，并使得该线程进入运行（running）状态。

*进一步考虑*
通过对公共资源使用互斥锁，这样就简单的到达了我们的目的，但是如果我们又遇到下面的情况：
1、遇到锁嵌套的情况该怎么办，这个嵌套是指当我一个线程在获取临界资源时，又需要再次获取；
2、如果有多个公共资源，在线程间共享多个资源的时候，如果两个线程分别占有一部分资源并且同时等待对方的资源；

上述这两种情况会直接造成程序挂起，即死锁，下面我们会谈死锁及可重入锁RLock。

*** Python：使用threading模块实现多线程编程五[死锁的形成]
前一篇文章Python：使用threading模块实现多线程编程四[使用Lock互斥锁]我们已经开始涉及到如何使用互斥锁来保护我们的公共资源了，现在考虑下面的情况
如果有多个公共资源，在线程间共享多个资源的时候，如果两个线程分别占有一部分资源并且同时等待对方的资源，这会引起什么问题？
*死锁概念*
所谓死锁： 是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程。 由于资源占用是互斥的，当某个进程提出申请资源后，使得有关进程在无外力协助下，永远分配不到必需的资源而无法继续运行，这就产生了一种特殊现象死锁。
#+begin_src python
  ''''' 
  Created on 2012-9-8 
    
  @author: walfred 
  @module: thread.TreadTest5 
  '''   
  import threading  
     
  counterA = 0  
  counterB = 0  
     
  mutexA = threading.Lock()  
  mutexB = threading.Lock()  
     
  class MyThread(threading.Thread):  
      def __init__(self):  
          threading.Thread.__init__(self)  
        
      def run(self):  
          self.fun1()  
          self.fun2()  
            
      def fun1(self):  
          global mutexA, mutexB  
          if mutexA.acquire():  
              print "I am %s , get res: %s" %(self.name, "ResA")  
                
              if mutexB.acquire():  
                  print "I am %s , get res: %s" %(self.name, "ResB")  
                  mutexB.release()  
               
          mutexA.release()   
            
      def fun2(self):  
          global mutexA, mutexB  
          if mutexB.acquire():  
              print "I am %s , get res: %s" %(self.name, "ResB")  
                
              if mutexA.acquire():  
                  print "I am %s , get res: %s" %(self.name, "ResA")  
                  mutexA.release()  
               
          mutexB.release()   
        
  if __name__ == "__main__":  
      for i in range(0, 100):  
          my_thread = MyThread()  
          my_thread.start()
#+end_src

代码中展示了一个线程的两个功能函数分别在获取了一个竞争资源之后再次获取另外的竞争资源，我们看运行结果：
---
I am Thread-1 , get res: ResA
I am Thread-1 , get res: ResB
I am Thread-2 , get res: ResAI am Thread-1 , get res: ResB
---
可以看到，程序已经挂起在那儿了，这种现象我们就称之为”死锁“。

*避免死锁*
避免死锁主要方法就是：正确有序的分配资源，避免死锁算法中最有代表性的算法是Dijkstra E.W 于1968年提出的银行家算法。
*** Python：使用threading模块实现多线程编程六[可重入锁RLock]
考虑这种情况：如果一个线程遇到锁嵌套的情况该怎么办，这个嵌套是指当我一个线程在获取临界资源时，又需要再次获取。
根据这种情况，代码如下：

#+begin_src python
  ''''' 
  Created on 2012-9-8 
    
  @author: walfred 
  @module: thread.ThreadTest6 
  '''  
    
  import threading  
  import time  
     
  counter = 0  
  mutex = threading.Lock()  
     
  class MyThread(threading.Thread):  
      def __init__(self):  
          threading.Thread.__init__(self)  
        
      def run(self):  
          global counter, mutex  
          time.sleep(1);  
          if mutex.acquire():  
              counter += 1  
              print "I am %s, set counter:%s" % (self.name, counter)  
              if mutex.acquire():  
                  counter += 1  
                  print "I am %s, set counter:%s" % (self.name, counter)  
                  mutex.release()  
              mutex.release()  
        
  if __name__ == "__main__":  
      for i in range(0, 200):  
          my_thread = MyThread()  
          my_thread.start() 
#+end_src

这种情况的代码运行情况如下：
之后就直接挂起了，这种情况形成了最简单的死锁。

那有没有一种情况可以在某一个线程使用互斥锁访问某一个竞争资源时，可以再次获取呢？在Python中为了支持在同一线程中多次请求同一资源，python提供了“可重入锁”：threading.RLock。这个RLock内部维护着一个Lock和一个counter变量，counter记录了acquire的次数，从而使得资源可以被多次require。直到一个线程所有的acquire都被release，其他的线程才能获得资源。上面的例子如果使用RLock代替Lock，则不会发生死锁：

代码只需将上述的：
=mutex = threading.Lock()=
替换成：
=mutex = threading.RLock()=

*** Python：使用threading模块实现多线程编程七[使用Condition实现复杂同步]
目前我们已经会使用Lock去对公共资源进行互斥访问了，也探讨了同一线程可以使用RLock去重入锁，但是尽管如此我们只不过才处理了一些程序中简单的同步现象，我们甚至还不能很合理的去解决使用Lock锁带来的死锁问题。所以我们得学会使用更深层的解决同步问题。

Python提供的Condition对象提供了对复杂线程同步问题的支持。Condition被称为条件变量，除了提供与Lock类似的acquire和release方法外，还提供了wait和notify方法。

使用Condition的主要方式为：线程首先acquire一个条件变量，然后判断一些条件。如果条件不满足则wait；如果条件满足，进行一些处理改变条件后，通过notify方法通知其他线程，其他处于wait状态的线程接到通知后会重新判断条件。不断的重复这一过程，从而解决复杂的同步问题。

下面我们通过很著名的“生产者-消费者”模型来来演示下，在Python中使用Condition实现复杂同步。

#+begin_src python
  ''''' 
  Created on 2012-9-8 
    
  @author: walfred 
  @module: thread.TreadTest7 
  '''  
    
  import threading  
  import time  
     
  condition = threading.Condition()  
  products = 0  
     
  class Producer(threading.Thread):  
      def __init__(self):  
          threading.Thread.__init__(self)  
            
      def run(self):  
          global condition, products  
          while True:  
              if condition.acquire():  
                  if products < 10:  
                      products += 1;  
                      print "Producer(%s):deliver one, now products:%s" %(self.name, products)  
                      condition.notify()
                  else:
                      print "Producer(%s):already 10, stop deliver, now products:%s" %(self.name, products)
                      condition.wait();
                  condition.release()  
                  time.sleep(2)  
            
  class Consumer(threading.Thread):  
      def __init__(self):  
          threading.Thread.__init__(self)  
            
      def run(self):  
          global condition, products  
          while True:  
              if condition.acquire():  
                  if products > 1:  
                      products -= 1  
                      print "Consumer(%s):consume one, now products:%s" %(self.name, products)  
                      condition.notify()  
                  else:  
                      print "Consumer(%s):only 1, stop consume, products:%s" %(self.name, products)  
                      condition.wait();  
                  condition.release()  
                  time.sleep(2)  
                    
  if __name__ == "__main__":  
      for p in range(0, 2):  
          p = Producer()  
          p.start()  
            
      for c in range(0, 10):  
          c = Consumer()  
          c.start()
#+end_src
代码中主要实现了生产者和消费者线程，双方将会围绕products来产生同步问题，首先是2个生成者生产products ，而接下来的10个消费者将会消耗products，代码运行如下：
---
Producer(Thread-1):deliver one, now products:1
Producer(Thread-2):deliver one, now products:2
Consumer(Thread-3):consume one, now products:1
Consumer(Thread-4):only 1, stop consume, products:1
Consumer(Thread-5):only 1, stop consume, products:1
Consumer(Thread-6):only 1, stop consume, products:1
Consumer(Thread-7):only 1, stop consume, products:1
Consumer(Thread-8):only 1, stop consume, products:1
Consumer(Thread-10):only 1, stop consume, products:1
Consumer(Thread-9):only 1, stop consume, products:1
Consumer(Thread-12):only 1, stop consume, products:1
Consumer(Thread-11):only 1, stop consume, products:1
---
另外：Condition对象的构造函数可以接受一个Lock/RLock对象作为参数，如果没有指定，则Condition对象会在内部自行创建一个RLock；除了notify方法外，Condition对象还提供了notifyAll方法，可以通知waiting池中的所有线程尝试acquire内部锁。由于上述机制，处于waiting状态的线程只能通过notify方法唤醒，所以notifyAll的作用在于防止有线程永远处于沉默状态。

*** Python：使用threading模块实现多线程编程八[使用Event实现线程间通信]
使用threading.Event可以实现线程间相互通信，之前的Python：使用threading模块实现多线程编程七[使用Condition实现复杂同步]我们已经初步实现了线程间通信的基本功能，但是更为通用的一种做法是使用threading.Event对象。
使用threading.Event可以使一个线程等待其他线程的通知，我们把这个Event传递到线程对象中，Event默认内置了一个标志，初始值为False。一旦该线程通过wait()方法进入等待状态，直到另一个线程调用该Event的set()方法将内置标志设置为True时，该Event会通知所有等待状态的线程恢复运行。
#+begin_src python
  ''''' 
  Created on 2012-9-9 
    
  @author: walfred 
  @module: thread.TreadTest8 
  '''  
    
  import threading  
  import time  
    
  class MyThread(threading.Thread):  
      def __init__(self, signal):
          threading.Thread.__init__(self)  
          self.singal = signal  
            
      def run(self):  
          print "I am %s,I will sleep ..."%self.name  
          self.singal.wait()  
          print "I am %s, I awake..." %self.name  
            
  if __name__ == "__main__":  
      singal = threading.Event()  
      for t in range(0, 3):  
          thread = MyThread(singal)  
          thread.start()  
        
      print "main thread sleep 3 seconds... "  
      time.sleep(3)  
        
      singal.set()
#+end_src
运行效果如下：
---
I am Thread-1,I will sleep ...
I am Thread-2,I will sleep ...
I am Thread-3,I will sleep ...
main thread sleep 3 seconds...
I am Thread-1, I awake...
I am Thread-2, I awake...
I am Thread-3, I awake...
---
** cProfile 
需要安装python-profiler
*** 命令行profile
     python -m cProfile -o ProfileOutput -s 'cumulative' myscript.py
     -m tells Python to run the library module profile as a script
     optional:
     -o tells cProfile to write the profile data to a specific file
     -s what to order by
*** Interactive Shell Profile
     import cProfile
     cProfile.run('foo()','ProfileOutput')
     import pstats
     pstats.Stats('ProfileOutput')
*** pstats (用来查看cProfile生成的文件)
     import pstats
     p = pstats.Stats('profile_file')
     p.sort_stats('cumulative').print_stats(10)  # sort by cumulative time
** Cython
*** Build manually using gcc
     1. This will create a .c file
     cython mymodule.pyx
     
     2. Compile .c into .o
     gcc -O2 -c -fPIC -I/usr/include/python2.6/ mymodule.c
     -c
     produces a .o file instead of an executable.
     -fPIC
     Produces position independent code, so we can dynamically link against it
     -I/usr/include/python2.6/
     the location of the Python 2.6 include file
     
     3. Link the .o into a .so
     gcc -shared mymodule.o -o mymodule.so
     -shared
     produces a shared-object file, instead of an executable.
     mymodule.o
     is the name of the module you wish to compile.
     -lxosd (optional)
     links against the C-library xosd.
     xosd(X On Screen Display) library displays text on your screen.
     -o mymodule.so
     causes gcc to put the output into a file called mymodule.so
*** compile using distutils
**** save the following as setup.py
      from disutils.core import setup
      from disutils.extension import Extension
      from Pyrex.Distutils import build_ext
      setup(
      name = "PyrexGuide",
      ext_modules=[ 
      Extension("mymodule", ["mymodule.pyx"], libraries = ["xosd"])
      ],
      cmdclass = {'build_ext': build_ext}
      )
**** python setup.py build_ext --inplace
*** Use your module
     import mymodule
     print dir(mymodule)
** vim_python mode:
    ]t      - jump to beginning of block
    ]e      -- Jump to end of block
    ]v      -- Select (Visual Line Mode) block
    ]<      -- Shift block to left
    ]>      -- Shift block to right
    ]#      -- Comment selection
    ]u      -- Uncomment selection
    ]c      -- Select current/previous class
    ]d      -- Select current/previous function
    ]<up>   -- Jump to previous line with the same/lower indentation
    ]<down> -- Jump to next line with the same/lower indentation
** xml
*** sax
**** good xml references:
      www.boddie.org.uk/python/XML_intro.html
**** xml.sax explained
      http://mail.python.org/pipermail/python-dev/2000-October/009946.html
      and see Troy's pyGS2 script in Dropbox/scripts/python/analysis
      a general pattern is 
      --
      from xml.sax import make_parser, ContentHandler
      parser = make_parser()
      # build a handler
      parser.setContentHandler(handler)
      parser.parse(fstream)
***** how to define a handler
       class YourHandler(ContentHandler):
       you need to define:
       __init__, 
       startElement(self,name,attrs), endElement(self,name), and characters(self,ch)
** optparse

** matplotlib
[plt.ylabel]    plt.ylabel('some numbers')
[plt.show]      plt.show()
[plt.axis]      plt.axis([0,6,0,20])
imshow		takes a matrix and plot it as image
contour		plot the contour
*** configure backend
     edit .matplotlib/matplotlibrc file
     To locate .matplotlib directory
     do
     >>> import matplotlib as mpl
     >>> mpl.get_configdir()
     '/home/wanding/.matplotlib'
     and write in matplotlibrc file
     backend : WXAgg
     Or in most cased
     backend : TkAgg
     
     import matplotlib
     matplotlib.use('PS')
*** change a [(x1, y1), (x2, y2)] into [x1,x2], [y1, y2]
**** zip() and unzip
**** numpy.asarray method
      xy = numpy.asarray([(x1, y1), (x2, y2)]
      xy[:,0], xy[:,1]
*** the structure of matplotlib
**** matplotlib.artist.Artist
      base class for adding something onto the canvas
      the most useful plotting functions are defined there.
***** attribute
****** canvas
       	matplotlib.backends.backend_tkagg.FigureCanvasTkAgg
****** figure
       	matplotlib.figure.Figure
       	set in set_figure() function
****** axes
       	matplotlib.axes.Axes, where the artist "resides".
       	initialized to None
****** _picker
       	can be None, a boolean, a float, or a function
       	see set_picker() docstring for detail
****** zorder
       	any number, artists with lower zorder values are drawn first
****** _alpha
       	the alpha value used for blending
       	float (0.0 transparent through 1.0 opaque)
***** function
****** set_picker()
       	set self._picker which is often overrides by the same name
       	in the derived classes. 
****** update()
       	set the parameters from the **kwargs
       	if we specify "picker = 5" in the argument
       	the update() would call self.set_picker(5)
****** pick()
       	call matplotlib.backend_bases.FigureCanvasBase.pick_event()
****** set_figure()
****** set_zorder()
       	take any number, artists with lower zorder values are drawn first
****** set_alpha()
       	set the alpha value used for blending
       	take a floating number (0.0 transparent through 1.0 opaque)
***** matplotlib.axes.Axes
****** attributes
******* name
       	 rectilinear
******* _get_lines
       	 matplotlib.axes._process._plot_var_args
       	 initialized to matplotlib.axes._process_plot_var_args(self)
       	 when it's called, the matplotlib.axes._process_plot_var_args.__call__()
       	 would be called.
******* lines
       	 a list of matplotlib.lines.line2D
******* patches
******* texts
******* tables
******* artists
******* images
******* collections
****** functions
******* cla()
       	 clear current axes
******* set_title()
       	 return matplotlib.text.Text object       
******* add_artist()
******* add_collection()
******* add_line()
******* add_patch()
******* add_table()
******* pie()
       	 call add_patch(), make a piechart
****** matplotlib.projections.polar.PolarAxes
****** matplotlib.projections.geo.GeoAxes
******* matplotlib.projections.geo.AitoffAxes
******* matplotlib.projections.geo.HammerAxes
******* matplotlib.projections.geo.MollweideAxes
******* matplotlib.projections.geo.LamberAxes
***** matplotlib.figure.Figure
****** attributes
******* canvas
       	 this is originally "None"
       	 and is usually set in matplotlib.pyplot.figure()
       	 into matplotlib.backends.backend_tkagg.FigureCanvasTkAgg
******* axes
       	 a list of matplotlib.axes.Axes
******* _axstack
       	 matplotlib.cbook.Stack
       	 for maintaining the current axes
****** functions
******* add_subplot()
       	 return matplotlib.axes.subplot_class_factory() class object,
       	 Notice that this object is obscure since the class's created on the fly
       	 in most cases it's a joint child class of matplotlib.axes.SubplotBase
       	 and the projection class object
******* sca()
       	 take a matplotlib.axes.Axes as its argument
       	 bubble the current axes in the self._axstack
******* gca()
       	 get current axes
       	 this returns a projection_class directly or calls add_subplot()
       	 and return a joint object of projection_class and SubplotBase
       	 class, if multiple axes exists, the gca() returns the top in
       	 the self._axstack()
******* clf()
       	 delete all the axes and other stuff drawn
       	 now self.axes become []
***** matplotlib.patches.Patch
       geometric shapes of various kinds
****** matplotlib.patches.Shadow
****** matplotlib.patches.Rectangle
****** matplotlib.patches.RegularPolygon
******* matplotlib.patches.CirclePolygon
****** matplotlib.patches.PathPatch
****** matplotlib.patches.Polygon
******* matplotlib.patches.FancyArrow
****** matplotlib.patches.Wedge
****** matplotlib.patches.Arrow
****** matplotlib.patches.YAArrow
****** matplotlib.patches.Ellipse
******* matplotlib.patches.Circle
******* matplotlib.patches.Arc
***** matplotlib.collections.Collection
       jointly inherited from matplotlib.artist.Artist
       and matplotlib.cm.ScalarMappable
****** matplotlib.collections.QuadMesh
****** matplotlib.collections.PollyCollection
******* matplotlib.collections.BrokenBarHCollection
****** matplotlib.collections.RegularPolyCollection
******* matplotlib.collections.StarPolygonCollection
******* matplotlib.collections.AsteriskPolygonCollection
****** matplotlib.collections.LineCollection
****** matplotlib.collections.EllipseCollection
****** matplotlib.collections.PatchCollection
***** matplotlib.lines.line2D
****** funcs
******* set_picker()
       	 set self._picker which is a matplotlib.artist.Artist member
**** matplotlib.artist.ArtistInspector
**** matplotlib.backend_bases.FigureCanvasBase
***** attributes
****** figure
       	matplotlib.figure.Figure
****** callbacks
       	matplotlib.cbooks.CallbackRegistry
       	it is initialized to have the record of all the events
****** events
       	all the possible responsive events
       	this includes, resize_event, draw_event, key_press_event,
       	key_release_event, button_press_event, button_release_event,
       	scroll_event, motion_notify_event,
       	pick_event, idle_event, figure_enter_event, figure_leave_event,
       	anxes_enter_event, axes_leave_event.
***** function
       the xxx_event function would be triggered by the human
       interaction with canvas
****** mpl_connect()
       	connect a string to a function(event) where event is
       	matplotlib.backend_bases.Event
****** pick_event()
       	callbacks.process(s, PickEvent)
****** scroll_event()
       	callbacks.process(s, MouseEvent)
****** button_press_event()
       	callbacks.process(s, MouseEvent)
****** button_release_event()
       	callbacks.process(s, MouseEvent)
****** motion_notify_event()
       	callbacks.process(s, MouseEvent)
***** matplotlib.backends.backend_tkagg.FigureCanvasTkAgg
       needs matplotlib.figure.Figure to initialize
**** matplotlib.backend_bases.FigureManagerBase
***** attributes
****** canvas
       	a matplotlib.backends.backend_tkagg.FigureCanvasTkAgg
***** matplotlib.backends.backend_tkagg.FigureManagerTkAgg
       needs matplotlib.backends.backend_tkagg.FigureCanvasTkAgg to
       initialize
**** matplotlib.backend_bases.Event
      needs a matplotlib.backend_bases.FigureCanvas to initialize
***** attributes
****** canvas
       	FigureCanvas
****** guiEvent
       	the GUI event that triggered the event
***** matplotlib.backend_bases.IdleEvent
***** matplotlib.backend_bases.DrawEvent
***** matplotlib.backend_bases.LocationEvent
****** matplotlib.backend_bases.MouseEvent
****** matplotlib.backend_bases.KeyEvent
***** matplotlib.backend_bases.PickEvent
       needs a MouseEvent and an Artist to initialize
       the pick_event would check how close the coordinates of this
       MouseEvent and this Artist is.
****** mouseevent
       	matplotlib.backend_bases.MouseEvent
****** artist
       	matplotlib.artist.Artist
**** matplotlib.axes._pylab_helpers.Gcf
      Manage a set of integer-numbered figures.
***** attributes
****** _aciveQue
       	a list of active matplotlib.figure.Figure()
****** figs
       	dictionary
***** funcs
****** get_fig_manager()
       	take an interger number *num* as argument
       	return figs[num]
****** get_active
       	return Gcf._activeQue[-1]
**** matplotlib.axes._process_plot_var_args
      needs matplotlib.axes.Axes and command (default to "plot") to
      initiaize
***** attribute
****** axes
****** command
***** funcs
****** __call__
       	this call mainly does set_units
**** matplotlib.patches._Style
**** matplotlib.pyplot (module)
***** funcs
****** figure()
       	return a matplotlib.figure.Figure using either
       	_pylab_helpers.Gcf.get_fig_manager(num) to get a current figure
       	or new_figure_manager (returned by pylab_setup from
       	matplotlib.backends.__init__) to create a new figure.
****** pie()
       	call matplotlib.axes.Axes.pie()
**** matplotlib.backends.backend_tkagg (module)
***** attributes
****** rcParams
       	matplotlib.rcParams
***** functions
****** draw_if_interactive()
****** show()
****** new_figure_manager()
**** matplotlib.backends (package)
***** function
****** pylab_setup()
       	import backends according to the string "backend" which is
       	obtained by matplotlib.get_backend()
       	mainly import
       	1. matplotlib.backends.backend_XXX.new_figure_manager() 
       	2. matplotlib.backends.backend_XXX.draw_if_interactive()
       	3. matplotlib.backends.backend_XXX.show()
***** possible backends can be:
       1. backend_agg
       2. backend_cairo
       3. backend_cocoaagg
       4. backedn_emf
       5. backend_fltkagg
       6. backend_gtk
       7. backend_gtkagg
       8. backend_gtkcairo
       9. backend_macosx
       10. backend_mixed
       11. backend_pdf
       12. backend_qt
       13. backend_ps
       14. backend_qt4
       15. backend_qt4agg
       16. backend_svg
       17. backend_template
       18. backend_tkagg (default)
       19. backend_wx
       20. backend_wxagg
       21. tkagg (not a backend)
**** matplotlib.backend_bases.NavigationToolbar2
**** matplotlib.cbook.CallbackRegistry
***** attributs
****** signals
       	a set of strings (signals)
****** callbacks
       	a dictionary mapping signals to a dictionary mapping calback id
       	to the callback function
***** functions
****** process()
       	call all the functions registered to the signal string.
**** matplotlib.cbook.Stack
**** the relationship between figure.Figure and backends.backend_tkagg.FigureCanvasTkAgg
      they are 1 on 1. figure.Figure has canvas attribute which is
      FigureCanvasTkAgg while FigureCanvasTkAgg has figure attribute
      which is figure.Figure.
      
      When they are initialized in pyplot.figure(), figure.Figure is
      first initialized with canvas set to None. then through
      backend_tkagg.new_figure_manager() function, FigureCanvasTkAgg is
      initialized with the figure.Figure() object. 

      In the initialization of FigureCanvasBase which is the base class
      of FigureCanvasTkAgg, figure.set_canvas is called.
**** Tkinter.Frame
***** matplotlib.backends.backend_tkagg.NavigationToolbar
***** matplotlib.backends.backend_tkagg.NavigationToolbar2TkAgg

*** how to plot hexbin
     plt.hexbin(entropies, error_seqs, bins='log', gridsize=20, cmap=cm.Greys)
*** remove edgecolor in scatterplot
     plt.scatter(entropies, error_seqs, edgecolor='none')
*** adjust margin
plt.subplots_adjust(bottom=0.15)
plt.subplots_adjust(bottom=0.08, top=0.99, left=0.08,right=0.99)
plt.savefig(myfig,bbox_inches='tight',pad_inches=0.05)
*** remove margin
plt.savefig(myfig,bbox_inches='tight',pad_inches=0.05)
*** the callback mechanism used in matplotlib
**** register a function func to a string s
      CallbackRegistry.connect(s, func)
      or in matplotlib: mpl_connect(s, func) (actually this subroutine call the
      CallbackRegistry.connects)
      Note: multiple functions can be connected to s. When s is called, all these functions are called.
**** call all the function registered to s
      CallbackRegistry.process(s, *args, **kwargs)
      call all the functions registered to s with  *args and **kwargs
**** s is so-called signals
** ipython
*** how to paste code to ipython?
%paste
*** reload problem: e.g. we have module.py then we need to first import module
     import module
     reload(module)
     then use module.class
*** useful magic command
     who, whos 
     psearch msg*
     psearch msg* int
     store msgflag
     store msgflag > /tmp/a.txt
     reset
     store -r        # recover the stored setting
     store -z        # clean the storage
     logstate
     logstart
     logon
     logoff
     lsmagic
     lsmagic??
     lsmagic?
     msgflag?
     p short for "print"
     p sys.path
     page sys.path
     help(sys)
     help('for')
     \function
     help()
     topics
     functions
     pdef re.match		# show function definition
     pdoc re.match		# show the docstring
     pinfo re.match		# show all the information (including
     # definition, docstring, stringform and file location) If you want
     # to check the file location. This is the command to issue. This is
     the same as ?? and ? suffix.
     psource re.match	# print part of the source file that contains the function
     magic
     pfile re.match 		# print out the entire file
     edit -x re.match
     pwd
     cd /tmp
     pushd /usr/lib/
     popd
     dhist
     dirs
     bookmark btree
     bookmark site /usr/lib/python2
     bookmark -l     # list bookmarks
     cd -<n>
     cd?
     ![BashCommand]
     patt = '*.py'
     !ls -l $patt
     !ls -l ${patt+'c'}
     
     alias
     alias pr echo you said %s
     pr Hello
     unalias pr

     def go(patt=patt)
     !ls -l $patt
     go()
     go('*.so')

     len(list)
     list.s  # smart list
     list.x  # newline list
     list.n  # newline string
     !ls *.py | sort

     run calc
     less
     pycat calc
     run -p calc profile
     run -d calc debugger
     q

     _i _ii _iii
     _i1
     In[3]
     hist 1 10
     hist -r # filter all the magic command for cut and paste use
     hist 10
     exec _i3
     exec In[1:4]

     macro
     macro begin 1-4 5
     print macro
     store begin

     edit
     save apple 1-3 5        # save line 1 to 3 and 5 to file apple
     cat apple.py
     pycat apple.py          # with syntax coloring
     edit _i163

     cpaste # end with "--"
     Out[118]
     Out[120]
     less ipython_log.py
     runlog


     Debug:
     xmode Plain
     xmode Verbose
     xmode Context
     pdb
     run -d calc2
     c

     To embed into code:
     from IPython.Shell import IPShellEmbed
     ipshell = IPShellEmbed()
     ipshell()

     time sum(range(100000))
     timeit sum(range(1000))
     time zopedocs();        # ; inhibit output
     prun    # profile run expression on file
     run -p  # file on expression
     less gofetch.py
     ed gofetch.py   # ed short for 'edit'
     bg gofetch()
     jobs[0].status
     jobs[0].result

     ipython -p physics      # with unit
     e.convertToUnit('J')
     ipython -pylab
** how to look at modules that has been loaded
    import sys
    sys.modules
** how to know the type of a python object
    type(object)
    e.g. a = [1,2,3]
    type(a) 			# return <type 'list'>
    isinstance(a, type(a))	# return True
** getattr and callable
    func = getattr(self, 'set_'+k, None) # same as calling self.set_*k*
    if func is None or not callable(func): raise AttributeError("Unknown
    Prop")
    func(v)
** the difference between __call__ and __init__
    class A:
    def __init__(self):
    print "init"
    def __call__(self):
    print "call"
    # then,
    a = A() 			# print init
    a()				# print call
* Perl
** load perl modules
A little builtin syntactic sugar means you can also say -mmodule=foo,bar or -Mmodule=foo,bar as a shortcut for -Mmodule qw(foo bar). This avoids the need to use quotes when importing symbols. The actual code generated by -Mmodule=foo,bar is use module split(/,/,q{foo,bar}). Note that the = form removes the distinction between -m and -M.

** print with newline
use feature qw(say);
say "hello";
** grep - the perl built-in
remember to put // around the matching pattern
grep (/$p/, <REF>)
** find help
perldoc -f eval
sudo apt-get install perl-doc
** build a perl module
ExtUtils::MakeMaker
** one command perl
cat lpm.spi | perl -p -e 's/(XU\d+) /\1 VVD /;' >lpm_new.spi
** to read command line argument
argument = shift;
** 命令行perl
perl -ne 'print if /^(Subject|From):/' Mailbox ## Mailbox is a file.
perl -pe 's/1999/2000/g' newyear.txt	## substitute and print out again
perl -ne 'next if /^s/;print' strictvar.pl	## print only those lines that don't start with a hash sign(#).
*** -e  
specified the one-line to be executed.
*** -n 
causes Perl to assume the following loop around your script, which makes it iterate over filename arguments somewhat like sed -n or awk:

while (<>) { ... # your script goes here }

The lines are not printed by default. You need to use -p to have lines printed. If a file named by an argument cannot be opened for some reason, Perl warns you about it, and moves on to the next file. 
*** -p 
causes Perl to assume the following loop around your script, which makes it iterate over filename arguments somewhat like sed:

while (<>) { ... # your script } continue { print or die "-p destination: $!\n";}

To suppress printing use the -n switch. A -p overrides a -n switch. If a file named by an argument cannot be opened for some reason, Perl warns you about it, and moves on to the next file. Note that the lines are printed automatically. An error occurring during printing is treated as fatal. 
** one-liner
Compiled by Peteris Krumins (peter@catonmat.net, @pkrumins on Twitter)
http://www.catonmat.net -- good coders code, great reuse
Latest version of this file is always at:
http://www.catonmat.net/download/perl1line.txt
This file is also available in other languages:
(None at the moment.)
Please email me peter@catonmat.net if you wish to translate it.
Perl One-Liners on Github:
*** FILE SPACING
**** Double line space a file
perl -pe '$\="\n"'
The "$\" variable is similar to ORS in Awk. It gets appended after every "print" operation. Without any arguments "print" prints the contents of "$_" (the good stuff).

perl -pe 'BEGIN { $\="\n" }'
This one sets the "$\" to newline just once before Perl does anything (BEGIN block gets executed before everything else).
perl -pe '$_ .= "\n"'
It appends another new-line at the end of each line, then prints it out.

perl -pe 's/$/\n/'
It replaces the regular expression "$" that matches at the end of line with a newline, effectively adding a newline at the end.

**** Double space a file, except the blank lines
perl -pe '$_ .= "\n" unless /^$/'
perl -pe '$_ .= "\n" if /\S/'

**** Triple space a file
perl -pe '$\="\n\n"'
perl -pe '$_.="\n\n"'

**** N-space a file
perl -pe '$_.="\n"x7'
The "x" operator repeats the thing on the left N times.e.g., perl -e 'print "foo"x5'

**** Add a blank line before every line
perl -pe 's//\n/'
This one-liner uses the "s/pattern/replacement/" operator. It substitutes the first pattern (regular expression) in the "$_" variable with the replacement. In this one-liner the pattern is empty, meaning it matches any position between chars (and in this case it's the position before first char) and replaces it with "\n". The effect is that a newline char gets inserted before the line.

**** Remove all blank lines
perl -ne 'print unless /^$/'
perl -lne 'print if length'
perl -ne 'print if /\S/'

**** Remove all consecutive blank lines, leaving just one
perl -00 -pe ''
perl -00pe0

**** Compress/expand all blank lines into N consecutive ones
perl -00 -pe '$_.="\n"x4'

**** Fold a file so that every set of 10 lines becomes one tab-separated line
perl -lpe '$\ = $. % 10 ? "\t" : "\n"'

*** LINE NUMBERING
**** Number all lines in a file
perl -pe '$_ = "$. $_"'

**** Number only non-empty lines in a file
perl -pe '$_ = ++$a." $_" if /./'

**** Number and print only non-empty lines in a file (drop empty lines)
perl -ne 'print ++$a." $_" if /./'

**** Number all lines but print line numbers only non-empty lines
perl -pe '$_ = "$. $_" if /./'

**** Number only lines that match a pattern, print others unmodified
perl -pe '$_ = ++$a." $_" if /regex/'

**** Number and print only lines that match a pattern
perl -ne 'print ++$a." $_" if /regex/'

**** Number all lines, but print line numbers only for lines that match a pattern
perl -pe '$_ = "$. $_" if /regex/'

**** Number all lines in a file using a custom format (emulate cat -n)
perl -ne 'printf "%-5d %s", $., $_'

**** Print the total number of lines in a file (emulate wc -l)
perl -lne 'END { print $. }'
perl -le 'print $n=()=<>'
perl -le 'print scalar(()=<>)'
perl -le 'print scalar(@foo=<>)'
perl -ne '}{print $.'
perl -nE '}{say $.'

**** Print the number of non-empty lines in a file
perl -le 'print scalar(grep{/./}<>)'
perl -le 'print ~~grep{/./}<>'
perl -le 'print~~grep/./,<>'
perl -E 'say~~grep/./,<>'

**** Print the number of empty lines in a file
perl -lne '$a++ if /^$/; END {print $a+0}'
perl -le 'print scalar(grep{/^$/}<>)'
perl -le 'print ~~grep{/^$/}<>'
perl -E 'say~~grep{/^$/}<>'

**** Print the number of lines in a file that match a pattern (emulate grep -c)
perl -lne '$a++ if /regex/; END {print $a+0}'
perl -nE '$a++ if /regex/; END {say $a+0}'

*** CALCULATIONS
**** Check if a number is a prime
perl -lne '(1x$_) !~ /^1?$|^(11+?)\1+$/ && print "$_ is prime"'
**** Print the sum of all the fields on a line
perl -MList::Util=sum -alne 'print sum @F'
**** Print the sum of all the fields on all lines
perl -MList::Util=sum -alne 'push @S,@F; END { print sum @S }'
perl -MList::Util=sum -alne '$s += sum @F; END { print $s }'
**** Shuffle all fields on a line
perl -MList::Util=shuffle -alne 'print "@{[shuffle @F]}"'
perl -MList::Util=shuffle -alne 'print join " ", shuffle @F'
**** Find the minimum element on a line
perl -MList::Util=min -alne 'print min @F'
**** Find the minimum element over all the lines
perl -MList::Util=min -alne '@M = (@M, @F); END { print min @M }'
perl -MList::Util=min -alne '$min = min @F; $rmin = $min unless defined $rmin && $min > $rmin; END { print $rmin }'
**** Find the maximum element on a line
perl -MList::Util=max -alne 'print max @F'
**** Find the maximum element over all the lines
perl -MList::Util=max -alne '@M = (@M, @F); END { print max @M }'
**** Replace each field with its absolute value
perl -alne 'print "@{[map { abs } @F]}"'
**** Find the total number of fields (words) on each line
perl -alne 'print scalar @F'
**** Print the total number of fields (words) on each line followed by the line
perl -alne 'print scalar @F, " $_"'
**** Find the total number of fields (words) on all lines
perl -alne '$t += @F; END { print $t}'
**** Print the total number of fields that match a pattern
perl -alne 'map { /regex/ && $t++ } @F; END { print $t }'
perl -alne '$t += /regex/ for @F; END { print $t }'
perl -alne '$t += grep /regex/, @F; END { print $t }'
**** Print the total number of lines that match a pattern
perl -lne '/regex/ && $t++; END { print $t }'
**** Print the number PI to n decimal places
perl -Mbignum=bpi -le 'print bpi(n)'
**** Print the number PI to 39 decimal places
perl -Mbignum=PI -le 'print PI'
**** Print the number E to n decimal places
perl -Mbignum=bexp -le 'print bexp(1,n+1)'
**** Print the number E to 39 decimal places
perl -Mbignum=e -le 'print e'
**** Print UNIX time (seconds since Jan 1, 1970, 00:00:00 UTC)
perl -le 'print time'
**** Print GMT (Greenwich Mean Time) and local computer time
perl -le 'print scalar gmtime'
perl -le 'print scalar localtime'
**** Print local computer time in H:M:S format
perl -le 'print join ":", (localtime)[2,1,0]'
**** Print yesterday's date
perl -MPOSIX -le '@now = localtime; $now[3] -= 1; print scalar localtime mktime @now'
**** Print date 14 months, 9 days and 7 seconds ago
perl -MPOSIX -le '@now = localtime; $now[0] -= 7; $now[4] -= 14; $now[7] -= 9; print scalar localtime mktime @now'
**** Prepend timestamps to stdout (GMT, localtime)
tail -f logfile | perl -ne 'print scalar gmtime," ",$_'
tail -f logfile | perl -ne 'print scalar localtime," ",$_'
**** Calculate factorial of 5
perl -MMath::BigInt -le 'print Math::BigInt->new(5)->bfac()'
perl -le '$f = 1; $f *= $_ for 1..5; print $f'
**** Calculate greatest common divisor (GCM)
perl -MMath::BigInt=bgcd -le 'print bgcd(@list_of_numbers)'
**** Calculate GCM of numbers 20 and 35 using Euclid's algorithm
perl -le '$n = 20; $m = 35; ($m,$n) = ($n,$m%$n) while $n; print $m'
**** Calculate least common multiple (LCM) of numbers 35, 20 and 8
perl -MMath::BigInt=blcm -le 'print blcm(35,20,8)'
**** Calculate LCM of 20 and 35 using Euclid's formula: n*m/gcd(n,m)
perl -le '$a = $n = 20; $b = $m = 35; ($m,$n) = ($n,$m%$n) while $n; print $a*$b/$m'
**** Generate 10 random numbers between 5 and 15 (excluding 15)
perl -le '$n=10; $min=5; $max=15; $, = " "; print map { int(rand($max-$min))+$min } 1..$n'
**** Find and print all permutations of a list
perl -MAlgorithm::Permute -le '$l = [1,2,3,4,5]; $p = Algorithm::Permute->new($l); print @r while @r = $p->next'
**** Generate the power set
perl -MList::PowerSet=powerset -le '@l = (1,2,3,4,5); for (@{powerset(@l)}) { print "@$_" }'
**** Convert an IP address to unsigned integer
perl -le '$i=3; $u += ($_<<8*$i--) for "127.0.0.1" =~ /(\d+)/g; print $u'
perl -le '$ip="127.0.0.1"; $ip =~ s/(\d+)\.?/sprintf("%02x", $1)/ge; print hex($ip)'
perl -le 'print unpack("N", 127.0.0.1)'
perl -MSocket -le 'print unpack("N", inet_aton("127.0.0.1"))'
**** Convert an unsigned integer to an IP address
perl -MSocket -le 'print inet_ntoa(pack("N", 2130706433))'
perl -le '$ip = 2130706433; print join ".", map { (($ip>>8*($_))&0xFF) } reverse 0..3'
perl -le '$ip = 2130706433; $, = "."; print map { (($ip>>8*($_))&0xFF) } reverse 0..3'

*** STRING CREATION AND ARRAY CREATION
**** Generate and print the alphabet
perl -le 'print a..z'
perl -le 'print ("a".."z")'
perl -le '$, = ","; print ("a".."z")'
perl -le 'print join ",", ("a".."z")'
**** Generate and print all the strings from "a" to "zz"
perl -le 'print ("a".."zz")'
perl -le 'print "aa".."zz"'
**** Create a hex lookup table
@hex = (0..9, "a".."f")
**** Convert a decimal number to hex using @hex lookup table
perl -le '$num = 255; @hex = (0..9, "a".."f"); while ($num) { $s = $hex[($num%16)&15].$s; $num = int $num/16 } print $s'
perl -le '$hex = sprintf("%x", 255); print $hex'
perl -le '$num = "ff"; print hex $num'
**** Generate a random 8 character password
perl -le 'print map { ("a".."z")[rand 26] } 1..8'
perl -le 'print map { ("a".."z", 0..9)[rand 36] } 1..8'
**** Create a string of specific length
perl -le 'print "a"x50'
**** Create a repeated list of elements
perl -le '@list = (1,2)x20; print "@list"'
**** Create an array from a string
@months = split ' ', "Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec"
@months = qw/Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec/
**** Create a string from an array
@stuff = ("hello", 0..9, "world"); $string = join '-', @stuff
**** Find the numeric values for characters in the string
perl -le 'print join ", ", map { ord } split //, "hello world"'
**** Convert a list of numeric ASCII values into a string
perl -le '@ascii = (99, 111, 100, 105, 110, 103); print pack("C*", @ascii)'
perl -le '@ascii = (99, 111, 100, 105, 110, 103); print map { chr } @ascii'
**** Generate an array with odd numbers from 1 to 100
perl -le '@odd = grep {$_ % 2 == 1} 1..100; print "@odd"'
perl -le '@odd = grep { $_ & 1 } 1..100; print "@odd"'
**** Generate an array with even numbers from 1 to 100
perl -le '@even = grep {$_ % 2 == 0} 1..100; print "@even"'
**** Find the length of the string
perl -le 'print length "one-liners are great"'
**** Find the number of elements in an array
perl -le '@array = ("a".."z"); print scalar @array'
perl -le '@array = ("a".."z"); print $#array + 1'

*** TEXT CONVERSION AND SUBSTITUTION
**** ROT13 a string
'y/A-Za-z/N-ZA-Mn-za-m/'
**** ROT 13 a file
perl -lpe 'y/A-Za-z/N-ZA-Mn-za-m/' file
**** Base64 encode a string
perl -MMIME::Base64 -e 'print encode_base64("string")'
perl -MMIME::Base64 -0777 -ne 'print encode_base64($_)' file
**** Base64 decode a string
perl -MMIME::Base64 -le 'print decode_base64("base64string")'
perl -MMIME::Base64 -ne 'print decode_base64($_)' file
**** URL-escape a string
perl -MURI::Escape -le 'print uri_escape($string)'
**** URL-unescape a string
perl -MURI::Escape -le 'print uri_unescape($string)'
**** HTML-encode a string
perl -MHTML::Entities -le 'print encode_entities($string)'
**** HTML-decode a string
perl -MHTML::Entities -le 'print decode_entities($string)'
**** Convert all text to uppercase
perl -nle 'print uc'
perl -ple '$_=uc'
perl -nle 'print "\U$_"'
**** Convert all text to lowercase
perl -nle 'print lc'
perl -ple '$_=lc'
perl -nle 'print "\L$_"'
**** Uppercase only the first word of each line
perl -nle 'print ucfirst lc'
perl -nle 'print "\u\L$_"'
**** Invert the letter case
perl -ple 'y/A-Za-z/a-zA-Z/'
**** Camel case each line
perl -ple 's/(\w+)/\u$1/g'
perl -ple 's/(?<!['])(\w+)/\u\1/g'
**** Strip leading whitespace (spaces, tabs) from the beginning of each line
perl -ple 's/^[ \t]+//'
perl -ple 's/^\s+//'
**** Strip trailing whitespace (space, tabs) from the end of each line
perl -ple 's/[ \t]+$//'
**** Strip whitespace from the beginning and end of each line
perl -ple 's/^[ \t]+|[ \t]+$//g'
**** Convert UNIX newlines to DOS/Windows newlines
perl -pe 's|\n|\r\n|'
**** Convert DOS/Windows newlines to UNIX newlines
perl -pe 's|\r\n|\n|'
**** Convert UNIX newlines to Mac newlines
perl -pe 's|\n|\r|'
**** Substitute (find and replace) "foo" with "bar" on each line
perl -pe 's/foo/bar/'
**** Substitute (find and replace) all "foo"s with "bar" on each line
perl -pe 's/foo/bar/g'
**** Substitute (find and replace) "foo" with "bar" on lines that match "baz"
perl -pe '/baz/ && s/foo/bar/'
**** Binary patch a file (find and replace a given array of bytes as hex numbers)
perl -pi -e 's/\x89\xD8\x48\x8B/\x90\x90\x48\x8B/g' file

*** SELECTIVE PRINTING AND DELETING OF CERTAIN LINES
**** Print the first line of a file (emulate head -1)
perl -ne 'print; exit'
**** Print the first 10 lines of a file (emulate head -10)
perl -ne 'print if $. <= 10'
perl -ne '$. <= 10 && print'
perl -ne 'print if 1..10'
**** Print the last line of a file (emulate tail -1)
perl -ne '$last = $_; END { print $last }'
perl -ne 'print if eof'
**** Print the last 10 lines of a file (emulate tail -10)
perl -ne 'push @a, $_; @a = @a[@a-10..$#a]; END { print @a }'
**** Print only lines that match a regular expression
perl -ne '/regex/ && print'
**** Print only lines that do not match a regular expression
perl -ne '!/regex/ && print'
**** Print the line before a line that matches a regular expression
perl -ne '/regex/ && $last && print $last; $last = $_'
**** Print the line after a line that matches a regular expression
perl -ne 'if ($p) { print; $p = 0 } $p++ if /regex/'
**** Print lines that match regex AAA and regex BBB in any order
perl -ne '/AAA/ && /BBB/ && print'
**** Print lines that don't match match regexes AAA and BBB
perl -ne '!/AAA/ && !/BBB/ && print'
**** Print lines that match regex AAA followed by regex BBB followed by CCC
perl -ne '/AAA.*BBB.*CCC/ && print'
**** Print lines that are 80 chars or longer
perl -ne 'print if length >= 80'
**** Print lines that are less than 80 chars in length
perl -ne 'print if length < 80'
**** Print only line 13
perl -ne '$. == 13 && print && exit'
**** Print all lines except line 27
perl -ne '$. != 27 && print'
perl -ne 'print if $. != 27'
**** Print only lines 13, 19 and 67
perl -ne 'print if $. == 13 || $. == 19 || $. == 67'
perl -ne 'print if int($.) ~~ (13, 19, 67)'
**** Print all lines between two regexes (including lines that match regex)
perl -ne 'print if /regex1/../regex2/'
**** Print all lines from line 17 to line 30
perl -ne 'print if $. >= 17 && $. <= 30'
perl -ne 'print if int($.) ~~ (17..30)'
perl -ne 'print if grep { $_ == $. } 17..30'
**** Print the longest line
perl -ne '$l = $_ if length($_) > length($l); END { print $l }'
**** Print the shortest line
perl -ne '$s = $_ if $. == 1; $s = $_ if length($_) < length($s); END { print $s }'
**** Print all lines that contain a number
perl -ne 'print if /\d/'
**** Find all lines that contain only a number
perl -ne 'print if /^\d+$/'
**** Print all lines that contain only characters
perl -ne 'print if /^[[:alpha:]]+$/
**** Print every second line
perl -ne 'print if $. % 2'
**** Print every second line, starting the second line
perl -ne 'print if $. % 2 == 0'
**** Print all lines that repeat
perl -ne 'print if ++$a{$_} == 2'
**** Print all unique lines
perl -ne 'print unless $a{$_}++'
**** Print the first field (word) of every line (emulate cut -f 1 -d ' ')
perl -alne 'print $F[0]'

*** HANDY REGULAR EXPRESSIONS
**** Match something that looks like an IP address
/^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$/
/^(\d{1,3}\.){3}\d{1,3}$/
**** Test if a number is in range 0-255
/^([0-9]|[0-9][0-9]|1[0-9][0-9]|2[0-4][0-9]|25[0-5])$/
**** Match an IP address
my $ip_part = qr|([0-9]|[0-9][0-9]|1[0-9][0-9]|2[0-4][0-9]|25[0-5])|;
if ($ip =~ /^($ip_part\.){3}$ip_part$/) {
say "valid ip";
}
**** Check if the string looks like an email address
/\S+@\S+\.\S+/
**** Check if the string is a decimal number
/^\d+$/
/^[+-]?\d+$/
/^[+-]?\d+\.?\d*$/
**** Check if the string is a hexadecimal number
/^0x[0-9a-f]+$/i
**** Check if the string is an octal number
/^0[0-7]+$/
**** Check if the string is binary
/^[01]+$/
**** Check if a word appears twice in the string
/(word).*\1/
**** Increase all numbers by one in the string
$str =~ s/(\d+)/$1+1/ge
**** Extract HTTP User-Agent string from the HTTP headers
/^User-Agent: (.+)$/
**** Match printable ASCII characters
/[ -~]/
**** Match unprintable ASCII characters
/[^ -~]/
**** Match text between two HTML tags
m|<strong>([^<]*)</strong>|
m|<strong>(.*?)</strong>|
**** Replace all <b> tags with <strong>
$html =~ s|<(/)?b>|<$1strong>|g
**** Extract all matches from a regular expression
my @matches = $text =~ /regex/g;

*** PERL TRICKS
**** Print the version of a Perl module
perl -MModule -le 'print $Module::VERSION'
perl -MLWP::UserAgent -le 'print $LWP::UserAgent::VERSION'

*** PERL ONE-LINERS EXPLAINED E-BOOK
I have written an ebook based on the one-liners in this file. If you wish to
support my work and learn more about these one-liners, you can get a copy
of my ebook at:
http://www.catonmat.net/blog/perl-book/
The ebook is based on the 7-part article series that I wrote on my blog.
In the ebook I reviewed all the one-liners, improved explanations, added
new ones, and added two new chapters - introduction to Perl one-liners
and summary of commonly used special variables.
You can read the original article series here:
http://www.catonmat.net/blog/perl-one-liners-explained-part-one/
http://www.catonmat.net/blog/perl-one-liners-explained-part-two/
http://www.catonmat.net/blog/perl-one-liners-explained-part-three/
http://www.catonmat.net/blog/perl-one-liners-explained-part-four/
http://www.catonmat.net/blog/perl-one-liners-explained-part-five/
http://www.catonmat.net/blog/perl-one-liners-explained-part-six/
http://www.catonmat.net/blog/perl-one-liners-explained-part-seven/
* Javascript
** basic syntax
*** switch
#+BEGIN_SRC javascript
switch (o) {
   case 'option1':
      console.log('seen option1');
   case 'option2':
      console.log('seen option2');
   default:
      console.log('non of above');
}
#+END_SRC

*** function
#+BEGIN_SRC javascript
var multiply = function (x, y) {
   return x * y;
};
#+END_SRC
Note the ";" at the end. compare this with "def multiply(x,y)" python way and "int multiply(int x, int y)" the C way.
*** object definition

#+BEGIN_SRC javascript
var myobj = {
   age: 30,
   country: "China"
}

or equivalently, 

var myobj = new Object();
myobj.name = 'Bob Smith';
myobj.age = 30;
#+END_SRC

*** class definition
class is defined as a "function" in JavaScript
#+BEGIN_SRC javascript
function Person(name, age) {
    this.name = name;
    this.age = age;
}

var bob = new Person('Bob Smith', 30);
#+END_SRC

class can have method defined to constructor
#+BEGIN_SRC javascript
function Rectange(height, width) {
    this.height = height;
    this.width = width;
    this.calcArea = function() {
        return this.height * this.width;
    };
};
#+END_SRC

*** object method definition
#+BEGIN_SRC javascript

var bob = new Object();
bob.name = 'Bob Smith';
bob.age = 30;
bob.setAge = function(newAge) {
    this.age = newAge;
}

bob.setAge(40);

bob.getYearOfBirth = function () {
   return 2014 - this.age;
};

#+END_SRC

another example
#+BEGIN_SRC javascript
var square = new Object();
square.sideLength = 6;
square.calcPerimeter = function() {
    return this.sideLength * 4;
};
// help us define an area method here
square.calcArea = function() {
    return this.sideLength * this.sideLength;
};
#+END_SRC

*** accessing object properties
There are 2 ways of accessing properties in Javascript.

1. the dot notation
var name = bob.name;

2. the bracket notation
var name = bob["name"];

** string concatenation

#+BEGIN_SRC javascript
var name='Sally Bowles';
console.log("sally's name " + name);
#+END_SRC

string concatenation works for non-string variables, e.g.,
#+BEGIN_SRC javascript
var age = 16;
console.log("sally's age " + age);
#+END_SRC

** confirm
confirm('This is an example of using JS to create some interaction on a website. Click OK to continue!');
** prompt
var p=prompt("where are you from");
** console.log
console.log(2*5)
console.log("hello")
** if else
if ("Wanding".length < 7) {
    console.log("Let's go down the first road!");
} else {
    console.log("Let's go to the second road!");
}
** substring
s.substring(x,y)
x: 0-based first index
y: 0-based first index unwanted
** var
var myAge=19;
** function
var divideByThree = function (number) {
    var val = number / 3;
    console.log(val);
    return false;
};
** for loop
for (var i = 1; i < 11; i = i + 1){
console.log(i);
}
** array
var mixed = [34, "candy", "blue", 11];
*** create an array of people
#+BEGIN_SRC javascript

function Person (name, age) {
    this.name = name;
    this.age = age;
}

var family = new Array();
family[0] = new Person('alice', 40);
family[1] = new Person('bob', 42);
family[2] = new Person('michelle', 8);
#+END_SRC

*** loop over array
for (var i = 0; i < cities.length; i++) {
    console.log("I would like to visit " + cities[i]);
}
** do while
do {
console.log("something");
} while (cond);
** not a number
var isEven = function(number) {
  // Your code goes here!
  if (number % 2 == 0) {
      return true;
  } else if (isNaN(number)) {
      return "not a number";
  } else {
      return false;
  }
};
** JavaScript object is like a dictionary (Python) or hash (Perl)
var me = {
    name: "dfdsf",
    age: 30
};
** create object
var me = new Object();
me.name = "waa";
me["age"] = 30;
var newArray = [[1,2,3],[1,2],[{a:4}]];
* jquery
fadeIn, fadeOut, fadeTo, mouseenter,
** $(this)
$('div').click(function() {
  $(this).fadeOut('slow');
});
** slideToggle
$(document).ready(function(){
    
  $('.pull-me').click(function(){
      $('.panel').slideToggle('slow');
      });
});
** after
$('#one').after('<p>Somethinga</p>');   
$('#one').after($('p'));
$('#two').after($('p'));
$('p').remove();
** add, remove and toggle classes
    $(this).toggleClass('highlighted');
** box shadow
.highlighted {
    -webkit-box-shadow: 0 0 8px #FFD700;
    -moz-box-shadow: 0 0 8px #FFD700;
    box-shadow: 0 0 8px #FFD700;
    cursor:pointer;
}
** customize css in jquery
$('div').height("200px");
$('div').width("200px");
$('div').css('border-radius', '10px');
** get value(s) of a form
$('input:checkbox:checked').val();
var input = $('input[name=checkListItem]').val();
** get/set html of a element
$('div').html("I love jQuery!");
** select input form field
    var toAdd=$('input[name=checkListItem]').val();
    selects
    <input type="text" name="checkListItem"/>
** append to list
    $('.list').append('<div class="item">'+toAdd+'</div>');
** remove on click
    $(document).on('click', '.item', function(){
        $(this).remove();
    });
** turn red
    $('div').hover(function(){
        $(this).addClass('red');
    });
** double click
$(document).ready(function(){
    $('div').dblclick(function(){
        $(this).fadeOut('fast');
    })
})
** round-cornered rectangle
div {
    height: 100px;
    width: 100px;
    border-radius: 5px;
    background-color: #ABCDEF;
}
** hover effect
div {
    border-radius: 5px;
    background-color: #ABCDEF;
    transition: background-color 0.5s ease;
    display:inline;
    font-size:25px;
    padding:20px;
    border:1px solid #ccc;
    margin-top:10px;
}

.active {
    background-color:#556677;
}

$(document).ready(function(){

  $('div').hover(
    function(){
        $(this).addClass('active');
    },
    function(){
        $(this).removeClass('active');
    }
  );

});
** focus on textarea/input
$(document).ready(function(){
    $('input').focus(function(){
        $(this).css('outline-style','solid');
        $(this).css('outline-color','#FF0000');
    });
});
** animate on keydown
$(document).ready(function(){
    $(document).keydown(function(){
        $('div').animate({left:'+=10px'}, 500);
    });
});
** animate with a keydown
    $(document).ready(function() {
    $(document).keydown(function(key) {
    switch(parseInt(key.which,10)) {
    // Left arrow key pressed
    case 37:
    $('img').animate({left: "-=10px"}, 'fast');
    break;
    // Up Arrow Pressed
    case 38:
    // Put our code here
    $('img').animate({top: "-=10px"}, 'fast');
    break;
    // Right Arrow Pressed
    case 39:
    // Put our code here
    $('img').animate({left: "+=10px"}, 'fast');
    break;
    // Down Arrow Pressed
    case 40:
    // Put our code here
    $('img').animate({top: "+=10px"}, 'fast');
    break;
    }
    });
});
** jquery UI
    <script src="//ajax.googleapis.com/ajax/libs/jqueryui/1.9.1/jquery-ui.min.js"></script>
** effect('explode')
$(document).ready(function(){
    $('div').effect('explode');
})
** effect('bounce')
$('div').click(function(){
$(this).effect('bounce', {times:3}, 500);
})
** effect('slide')
    $(this).effect('slide');
** accordion
<!DOCTYPE html>
<html>
    <head>
    	<title>Behold!</title>
        <link rel='stylesheet' type='text/css' href='http://code.jquery.com/ui/1.9.1/themes/base/jquery-ui.css'/>
        <script type='text/javascript' src='script.js'></script>
        <script src="//ajax.googleapis.com/ajax/libs/jqueryui/1.9.1/jquery-ui.min.js"></script>
	</head>
	<body>
        <div id="menu">
            <h3>jQuery</h3>
            <div>
                <p>jQuery is a JavaScript library that makes your websites look absolutely stunning.</p>
            </div>
            <h3>jQuery UI</h3>
            <div>
                <p>jQuery UI includes even more jQuery goodness!</p>
            </div>
            <h3>JavaScript</h3>
            <div>
                <p>JavaScript is a programming language used in web browsers, and it's what powers jQuery and jQuery UI. You can learn about JavaScript in the <a href="http://www.codecademy.com/tracks/javascript" target="blank" style="text-decoration:none; color:#F39814">JavaScript track</a> here on Codecademy.</p>
            </div>
        </div>
	</body>
</html>
*** javascript
$(document).ready(function() {
    $("#menu").accordion({collapsible: true, active: false});
});
** draggable
$(document).ready(function(){
    $('#car').draggable();
})
** resizable
    $('div').resizable();
** make list selectable
    $('ol').selectable()
*** notice their css
ol {
    list-style-type: none;
	position: relative;
	left: -20px;
}

ol li {
	background: #eeeeee;
	border-radius: 5px;
	border: 1px solid black;
	margin: 3px;
	padding: 0.4em;
	font-size: 1em;
	height: 16px;
	font-family: Verdana, Arial, Sans-Serif;
}

ol .ui-selected {
	background: #F39814; color: white;
}
** list sortable
    $('ol').sortable()
** hide/show accordion
var main=function() {
    $('.article').click(function(){
        $('.article').removeClass('current');
        $('.description').hide();
        $(this).addClass('current');
        $(this).children('.description').show();
    });  
};

$(document).ready(main);
$(document).keypress(function(event){
    if(event.which === 111){
        $('.current').children('.description').toggle()
    }else if(event.which === 110){
        var currentArticle = $('.current');
        var nextArticle = currentArticle.next();
        currentArticle.removeClass('current');
        nextArticle.addClass('current');
        
    }
})
*** html
<!doctype html>
<html>
  <head>
    <link href="http://s3.amazonaws.com/codecademy-content/courses/ltp2/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="style.css">
  </head>
  <body>
    <div class="articles container">
      
      <div class="article current">
        <div class="item row">
          <div class="col-xs-3">
            <p class="source">FLIGHT</p>
          </div>
          <div class="col-xs-6">
            <p class="title">Embraer adds third Legacy 500 prototype to flight test campaign</p>
          </div>
          <div class="col-xs-3">
            <p class="pubdate">Mar 23</p>
          </div>
        </div>
        <div class="description row">
          <div class="col-xs-3">&nbsp;</div>
          <div class="col-xs-6">
            <h1>Embraer adds third Legacy 500 prototype to flight test campaign</h1>
            <p>The third Legacy 500 has joined Embraer's flight test programme aimed at delivering the midsize business jet in 2014. The airtcraft, serial number 003...</p>
          </div>
          <div class="col-xs-3">&nbsp;</div>
        </div>
      </div>

      <div class="article">
        <div class="item row">
          <div class="col-xs-3">
            <p class="source">AW Commercial Aviation</p>
          </div>
          <div class="col-xs-6">
            <p class="title">CSeries Supplier Scramble</p>
          </div>
          <div class="col-xs-3">
            <p class="pubdate">Mar 22</p>
          </div>
        </div>
        <div class="description row">
          <div class="col-xs-3">&nbsp;</div>
          <div class="col-xs-6">
            <h1>CSeries Supplier Scramble</h1>
            <p>Three months before the planned first flight of its CSeries, Bombardier is grappling with supplier issues crucial to meeting its production cost...</p>
          </div>
          <div class="col-xs-3">&nbsp;</div>
        </div>
      </div>

      <div class="article">
        <div class="item row">
          <div class="col-xs-3">
            <p class="source">AW business aviation</p>
          </div>
          <div class="col-xs-6">
            <p class="title">Flying the Gulfstream G650</p>
          </div>
          <div class="col-xs-3">
            <p class="pubdate">Mar 22</p>
          </div>
        </div>
        <div class="description row">
          <div class="col-xs-3">&nbsp;</div>
          <div class="col-xs-6">
            <h1>Flying the Gulfstream G650</h1>
            <p>Gulfstream is turning up the heat in the large-cabin business aircraft competition with its new G650 flagship, the largest, fastest, farthest-ranging...</p>
          </div>
          <div class="col-xs-3">&nbsp;</div>
        </div>
      </div>

      <div class="article">
        <div class="item row">
          <div class="col-xs-3">
            <p class="source">FLIGHT</p>
          </div>
          <div class="col-xs-6">
            <p class="title">New retirements cut RAF VC10 fleet to four</p>
          </div>
          <div class="col-xs-3">
            <p class="pubdate">Mar 22</p>
          </div>
        </div>
        <div class="description row">
          <div class="col-xs-3">&nbsp;</div>
          <div class="col-xs-6">
            <h1>New retirements cut RAF VC10 fleet to four</h1>
            <p>The UK Royal Air Force has retired another two of its Vickers VC10 tankers, with the pair's departure reducing its inventory of the Rolls-Royce...</p>
          </div>
          <div class="col-xs-3">&nbsp;</div>
        </div>
      </div>

      <div class="article">
        <div class="item row">
          <div class="col-xs-3">
            <p class="source">FLIGHT</p>
          </div>
          <div class="col-xs-6">
            <p class="title">Virgin can deliver more value for Delta than for SIA: Bastian</p>
          </div>
          <div class="col-xs-3">
            <p class="pubdate">Jul 17</p>
          </div>
        </div>
        <div class="description row">
          <div class="col-xs-3">&nbsp;</div>
          <div class="col-xs-6">
            <h1>Virgin can deliver more value for Delta than for SIA: Bastian</h1>
            <p>Delta Air Lines president Ed Bastian is confident that the carrier can extract far more value from its shareholding in Virgin Atlantic compared with...</p>
          </div>
          <div class="col-xs-3">&nbsp;</div>
        </div>
      </div>

      <div class="article">
        <div class="item row">
          <div class="col-xs-3">
            <p class="source">AW Defense</p>
          </div>
          <div class="col-xs-6">
            <p class="title">Freedom Experiences Two More Power Outages</p>
          </div>
          <div class="col-xs-3">
            <p class="pubdate">Mar 22</p>
          </div>
        </div>
        <div class="description row">
          <div class="col-xs-3">&nbsp;</div>
          <div class="col-xs-6">
            <h1>Freedom Experiences Two More Power Outages</h1>
            <p>The Littoral Combat Ship (LCS-1) USS Freedom’s first overseas deployment to Southeast Asia has been marred by two more power outages...</p>
          </div>
          <div class="col-xs-3">&nbsp;</div>
        </div>
      </div>

      <div class="article">
        <div class="item row">
          <div class="col-xs-3">
            <p class="source">FLIGHT</p>
          </div>
          <div class="col-xs-6">
            <p class="title">FedEx to acquire up to 30 United 757s</p>
          </div>
          <div class="col-xs-3">
            <p class="pubdate">Mar 22</p>
          </div>
        </div>
        <div class="description row">
          <div class="col-xs-3">&nbsp;</div>
          <div class="col-xs-6">
            <h1>FedEx to acquire up to 30 United 757s</h1>
            <p>United Airlines is to sell up to 30 Boeing 757s to freight operator FedEx Express, with deliveries of the twinjets set to start this year.</p>
          </div>
          <div class="col-xs-3">&nbsp;</div>
        </div>
      </div>

    </div>
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
    <script src="app.js"></script>
  </body>
</html>
*** css
body {
  background-image: url('http://s3.amazonaws.com/codecademy-content/courses/ltp2/img/reader/bg.png');
  -webkit-background-size: cover;
     -moz-background-size: cover;
       -o-background-size: cover;
          background-size: cover;
}

p {
  margin: 0;
}

.row {
  margin: 0;
}

.articles {
  margin-top: 30px;
  margin-bottom: 30px;
}

.article {
  color: #222;
  background: rgba(255,255,255,.9);
  border-spacing: 2px;
  border-color: gray;
  font-family: arial,sans-serif;
  border-bottom: 1px #e5e5e5 solid;
}

.current .item {
  background: rgba(206,220,206,.9);
}

.item {
  cursor: pointer;
  padding-top: 7px;
  padding-bottom: 7px;
  
}

.item .source {
  margin-left: 20px;
}

.item .title {
  font-weight: bold;
}

.item .pubdate {
  margin-right: 20px;
}

.item .pubdate {
  text-align: right;  
}

.description {
  display: none;
  padding-top: 10px;
  padding-bottom: 10px;
}

.description h1 {
  margin-top: 0px;
  font-size: 23px;
}
** modify DOM
*** add tag
$('tag').text('SomeText') results in <tag>SomeText</tag>
** append to items
$('.btn').click(function() {
  $('<li>').text('New item').appendTo('.items');
});
** prepend to items
$('.btn').click(function() {
  $('<li>').text('New item').prependTo('.items');
});
** $('li') vs $('<li>')
first selects a <li>, second creates an <li>.
** remove an element
  $('.selected').remove();
** hide/show/toggle/addClass/removeClass/toggleClass
var main = function () {
  $(".btn").click(function () {
    $(".read").hide();
  });
};

  $('li').click(function() {
  $(this).addClass('read');
})
** traverse DOM
*** .next next sibling
*** .prev previous sibling
*** .children() get all children
to select a particular child, one can use
$('ul').children('.pubdate')
* Java
** Basic concepts
*** Java version history
http://en.wikipedia.org/wiki/Java_version_history
JDK 1.0 → JDK 1.1 → J2SE 1.2 → … → J2SE 1.4 → J2SE 5.0 → Java SE 6 → Java SE 7

There are three implementations of Java on Ubuntu, openjdk, sun java and ibm java. Most likely you're gonna need openjdk and sun java. I like openjdk because it comes with nice font and look and feel.
openjdk package: openjdk-6-jre + icetea6-plugin (the browser plugin) + openjdk-6-jdk
sun java package: sun-java6-jre + sun-java6-plugin (the browser plugin) + sun-java6-jdk

*** to change java running environment:
sudo update-java-alternatives --set java-6-sun

on Redhat system
alternatives --config java

*** to list all java implementation on your computer:
update-java-alternatives --list

*** to show the java you are currently running:
java -version

Source file name has to be spelled the same (case sensitive) as the name of the public class contained.
e.g., ImageViewer.java has public class ImageViewer
*** to compile and run java
javac Welcome.java
java Welcome # NO ".class"
*** to compile all the .java files
java src/**/*.java -d <target dir>
otherwise, java put .class file in the same directory as .java file
*** peek into .jar file
jar tf [.jar]
*** .java .class and .jar
.java : source file to be compiled to .class by javac command
.class : byte code compiled and run by JVM
.jar : a collection of .class and resource files (images, binaries etc.), .jar file is zip compressed. Therefore one could use "jar" on normal zip file. e.g., "jar xvf src.zip"

**** convert .class to .jar
NOTE: the base class is super important! If you find something like NullPointerError, you are not setting the base directory right.

jar cf myFile.jar *.class
f is for output .jar file name
all the class file names are appended to the end

jar cmf Manifest.txt myFile.jar *.class
m is for Manifest file

**** write manifest file
Main-Class: oata.HelloWorld
**** convert .jar to .class
jar xf myFile.jar
**** compile .java to .class
javac myCode.java
*** specify maximum java memory size
# 80 Mb memory
java -Xmx80m
# 2 Gb memory
java -Xmx4096m
# 4 Gb memory
java -Xmx4g
*** specify class path and class name in execution
java -cp [CLASSPATH]
or
java -classpath [CLASSPATH]
java -cp /home/uec-00/shared/production/software/ECWorkflow/ECWorkFlow.jar:/home/uec-00/shared/production/software/ECWorkflow/pegasus.jar edu.usc.epigenome.workflow.generator.ReportFromPileup -pbs workFlowParams.txt
*** what's main method signature
public static void main(String args[])
public means it's accessible from outside the class.
static means it's not invoked upon an object.

// for comments

; for ending a statement

the following are equivalent
1. String[] args
2. String []args
3. String args[]

** Applet
*** how to compile and use applet?
You need 2 files, WelcomeApplet.java is the source code. And WelcomeApplet.html is the html
javac WelcomeApplet.java
appletviewer WelcomeApplet.html

inside WelcomeApplet.html you have something like
#+BEGIN_SRC html
<applet code="WelcomeApplet.class">
<param name="greeting" value="something">
</applet>
#+END_SRC

** Ant
*** to write a build.xml file
https://ant.apache.org/manual/tutorial-HelloWorldWithAnt.html

**** from command line
to achieve from command line
#+BEGIN_SRC 
mkddir build/classes
javac -sourcepath src -d build\classes src\oata\HelloWorld.java
echo Main-Class: oata.HelloWorld>mf
mkdir build/jar
jar cfm build/jar/HelloWorld.jar mf -C build\classes .
java -jar build\jar\HelloWorld.jar
#+END_SRC
**** basic
#+BEGIN_SRC
<mkdir dir="build/classes"/>
<javac
    srcdir="src"
    destdir="build/classes"/>
<!-- automatically detected -->
<!-- obsolete; done via manifest tag -->
<mkdir dir="build/jar"/>
<jar
    destfile="build/jar/HelloWorld.jar"

    basedir="build/classes">
    <manifest>
        <attribute name="Main-Class" value="oata.HelloWorld"/>
    </manifest>
</jar>
<java jar="build/jar/HelloWorld.jar" fork="true"/>
#+END_SRC

**** with variable substitution, dependency and default target
SPECIAL: ant.project.name
The "default" is <project> is the default target to build

#+BEGIN_SRC 
<project name="HelloWorld" basedir="." default="main"> <!--default is the default target-->

  <property name="src.dir" value="src" />
  <property name="build.dir" value="build" />
  <property name="classes.dir" value="${build.dir}/classes" />
  <property name="jar.dir" value="${build.dir}/jar" />

  <property name="main-class" value="oata.HelloWorld" />
  
  <target name="clean">
    <delete dir="${build.dir}" />
  </target>

  <target name="compile">
    <mkdir dir="${classes.dir}" />
    <javac srcdir="src" destdir="build/classes" />
  </target>

  <target name="jar" depends="compile">
    <mkdir dir="${jar.dir}" />
    <jar destfile="${jar.dir}/${ant.project.name}.jar" basedir="${classes.dir}">
      <manifest>
	<attribute name="Main-Class" value="${main-class}" />
      </manifest>
    </jar>
  </target>

  <target name="run" depends="jar">
    <java jar="${jar.dir}/${ant.project.name}.jar" fork="true"/>
  </target>

  <target name="clean-build" depends="clean,jar"/>
  <target name="main" depends="clean,run"/>
</project>

#+END_SRC

**** with path and external library

"classpathref" attribute in javac is the path to classes

Note: class dependencies are NOT needed when making the .jar file. They are only needed when running and compiling.

#+BEGIN_SRC 
<project name="HelloWorld" basedir="." default="main"> <!--default is the default target-->

...
  <property name="lib.dir" value="lib" />
  <path id="classpath">
    <fileset dir="${lib.dir}" includes="**/*.jar"/>
  </path>

...
  <target name="compile">
    <mkdir dir="${classes.dir}" />
    <javac srcdir="src" destdir="build/classes" classpathref="classpath"/>
  </target>

...

  <target name="run" depends="jar">
    <java fork="true" classname="${main-class}">
      <classpath>
	<path refid="classpath"/>
	<path location="${jar.dir}/${ant.project.name}.jar"/>
      </classpath>
    </java>
  </target>

  <target name="clean-build" depends="clean,jar"/>
  <target name="main" depends="clean,run"/>
</project>
    
#+END_SRC

*** build javadoc.xml
need javadoc.xml
ant -buildfile javadoc.xml html

*** build java library
need build.xml
ant # this build the default target 'dist-all' in the default file 'build.xml'

** maven
MAKE SURE YOU USE THE ORACLE JAVA!
"java -version" may show openjdk still
but you shoudl set JAVA_HOME by
export $JAVA_HOME=/path/to/oracle/java/ # without /bin, which will be appended afterwards
mvn is smart enough to follow the JAVA_HOME
*** build project
mvn package
* CSS
** create font-face in CSS
@font-face {
  font-family: 'Shift';
  font-style: normal;
  font-weight: normal;
  src: url("http://s3.amazonaws.com/codecademy-content/courses/ltp/fonts/shift.woff") format("woff");
}
@font-face {
  font-family: 'Shift';
  font-style: normal;
  font-weight: bold;
  src: url("http://s3.amazonaws.com/codecademy-content/courses/ltp/fonts/shift-bold.woff") format("woff");
}
** pull-left and pull-right
require "bootstrap"

<ul class="pull-left">
<li><a href="#">Name</a></li>
<li><a href="#">Browse</a></li>
</ul>
<ul class="pull-right">
<li><a href="#">Sign Up</a></li>
<li><a href="#">Log In</a></li>
<li><a href="#">Help</a></li>
</ul>
** grid layout
<div class="container">
  <div class="row">
    <div class="col-md-4">
      ...
    </div>
    <div class="col-md-4">
      ...
    </div>
    <div class="col-md-4">
      ...
    </div>
  </div>
</div>
** pseudo-class selector
a:link
a:visited
a:hover
** remove text-decoration
text-decoration: none
** first-child
p:first-child {
  color: red;
}
** circles
div {
display: inline-block;
margin-left: 5px;
height: 100px;
width: 100px;
border-radius: 100%;
border: 2px solid black;
}
** rounded triangle
div {
height: 50px;
width: 100px;
border: 2px solid black;
border-radius: 5px;
display: inline-block
}
** margin, border and padding
margin: auto
put equal left and right margins on the element
margin: 1px 2px 3px 4px
# the order is: top, right, bottom, left
** use float to divide the screen into two
float:left 
float:right
** use clear to set footer
clear: both
you can also
clear: left
clear: right
** position
static (default)
absolute
relative
fixed
** rounded corner look for div
border-radius: 5px
* git
** to change previous git commit message (not necessarily the most recent one) :unknown:
If the commit you want to fix isn’t the most recent one:

git rebase --interactive $parent_of_flawed_commit

If you want to fix several flawed commits, pass the parent of the oldest one of them.

An editor will come up, with a list of all commits since the one you gave.
Change pick to reword (or on old versions of Git, to edit) in front of any commits you want to fix.
Once you save, Git will replay the listed commits.

For each commit you want to reword, Git will drop you back into your editor. For each commit you want to edit, Git drops you into the shell. If you’re in the shell:
Change the commit in any way you like.
git commit --amend
git rebase --continue

Most of this sequence will be explained to you by the output of the various commands as you go. It’s very easy, you don’t need to memorise it – just remember that git rebase --interactive lets you correct commits no matter how long ago they were.

** no check file mode while git pull                               :unknown:
git config core.filemode false
** ignore file mode change in Git
git config core.fileMode false

** color ui in git                                                :memorize:
git config --global color.ui true
** to exclude/hide/ignore file from git status
put this to ~/.gitconfig
=======
[core]
	excludesfile = ~/.gitignore
=======
and this to ~/.gitignore
=======
*~
*.pyc
=======
** to reduce repo size
git gc --aggressive --prune=now
** view the change between two arbitrary commits
git diff <commit1>..<commit2>
git diff 8751261..9d3a192 | git apply -
** to view operation log (reflog)
git reflog -2
view last two operations
** to automatically stage files that are modified or deleted while committing
git commit -a (for --all)
** what is tilde (~ and ^)?
^1 means first parent
^2 means second parent where a commit has more than one parent
~1 means first ancestor (by first parent) equivalent to ^1 or simply ^
~2 means up two levels in the hierarchy, via the first parent if a commit has more than one parent, equivalent to ^1^1 or ^^
e.g.,
HEAD^^^ == HEAD~3
see
http://schacon.github.io/git/git-rev-parse

** how to reuse username and message while committing
# reuse username and message
git commit -C
# just reuse username
git commit -c 
** what's ORIG_HEAD?
$ git commit ...
$ git reset --soft HEAD^      <1>
$ edit                        <2>
$ git commit -a -c ORIG_HEAD  <3>
"reset" copies the old head to .git/ORIG_HEAD
** what does reset (--soft, --mixed, --hard) do?
git reset --soft <target-commit>
change the branch pointer that HEAD points to to <target-commit>
index and working directory still look like the current commit

git reset --mixed <target-commit>
this is the default
make Index look like <target-commit>

git reset --hard <target-commit>
update the working directory by applying the index. (this is dangerous because it overwrite all the uncommited changes)

In other words,
http://git-scm.com/blog
#1) Move whatever branch HEAD points to (stop if --soft)
#2) THEN, make the Index look like that (stop here unless --hard)
#3) THEN, make the Working Directory look like tha
** what is reset with a filename?
there is no "reset --soft" or "reset --hard" with a filename.
There is only reset (--mix) with a filename
** to revert only a file (or a path) to previous commit
git reset <target-commit> <path>
This is slightly different from "git reset" without the <path>
git reset file.txt
is a shorthand for
git reset --mixed HEAD file.txt
This is equivalently, unstaging that file.

if <target-commit> is not HEAD, then
git reset eb43bf file.txt
revert the file.txt to eb43bf in the index (not working directory)
run
git add file.txt revert the index to the working version again.
** to stage partially a file
git add --patch
** to unstage partially a file
git reset -p (-p is short for --patch)
** to squash commits                                              :memorize:
git reset --soft HEAD~2
git commit
The other intermediate commits are now detached. They are still in reflog and may be cleaned at some point.
** git checkout vs git reset
with filename:
git checkout -- filename
is equivalen to
git checkout filename
The "--" is for preventing a branch with the same name as filename
** to show branches
git branch
or to show branch with its last commit
git branch -v
** to rebase
git rebase [basebranch]
it will use the branch you are currently on as the topic branch and rebase onto the base branch. Or you can specify the topic branch (you don't have to checkout then) by
git rebase [basebranch] [topicbranch]

$ git checkout experiment
$ git rebase master (on experiment)
This takes the patch of experiment and reapply at master. In other words, you take all the changes that were committed on one branch (experiment) and replay them on another one (master).
It works by going to the common ancestor of the two branches (the one you’re on and the one you’re rebasing onto), getting the diff introduced by each commit of the branch you’re on, saving those diffs to temporary files, resetting the current branch to the same commit as the branch you are rebasing onto, and finally applying each change in turn.
Then you can do a fastforward merge:
$ git checkout master
$ git merge experiment (on master)
** to show branches that have (not) been merged into the current branch
git branch --merged
you can delete branches that have been merged by "git branch -d"
git branch --no-merged
you cannot delete these branches unless by "git branch -D" to force it.
** why you shouldn't rebase commits that you've pushed to public repository
When you rebase stuff, you’re abandoning existing commits and creating new ones that are similar but different. If you push commits somewhere and others pull them down and base work on them, and then you rewrite those commits with git rebase and push them up again, your collaborators will have to re-merge their work and things will get messy when you try to pull their work back into yours.
** why rebase
There is no difference between merge and rebase in the end product of the integration, but rebasing makes for a cleaner history. If you examine the log of a rebased branch, it looks like a linear history: it appears that all the work happened in series, even when it originally happened in parallel.

Often, you’ll do this to make sure your commits apply cleanly on a remote branch — perhaps in a project to which you’re trying to contribute but that you don’t maintain. In this case, you’d do your work in a branch and then rebase your work onto origin/master when you were ready to submit your patches to the main project. That way, the maintainer doesn’t have to do any integration work — just a fast-forward or a clean apply.

Note that the snapshot pointed to by the final commit you end up with, whether it’s the last of the rebased commits for a rebase or the final merge commit after a merge, is the same snapshot — it’s only the history that is different. Rebasing replays changes from one line of work onto another in the order they were introduced, whereas merging takes the endpoints and merges them together.
** how to delete a branch
git branch -d hotfix
** what's fast-forward merging?
Because the commit pointed to by the branch you merged in was directly upstream of the commit you’re on, Git moves the pointer forward. To phrase that another way, when you try to merge one commit with a commit that can be reached by following the first commit’s history, Git simplifies things by moving the pointer forward because there is no divergent work to merge together — this is called a "fast forward".
** to resolve merge file conflict
In short: git merge, edit, git add, git commit
also see
https://help.github.com/articles/resolving-a-merge-conflict-from-the-command-line
# initial merge
$ git merge iss53
Auto-merging index.html
CONFLICT (content): Merge conflict in index.html
Automatic merge failed; fix conflicts and then commit the result.
# see files that are unmerged after a merge conflict
$ git status
# now change manually change the unmerged file
emacs index.html
The conflicting portion are in blocks like
>>>>> branch1
content1
========
content2
<<<<<< branch2
# or use a visualization tool
git mergetool
# mark the file as resolved by "git add"
git add index.html
# verify all the conflicts are resolved
$ git status
# finalize the "merge commit"
$ git commit

** to resolve removed file conflict
# resolve by keeping the file
git status
git add <conflict-file>
git commit
git show | head
# resolve by removing the file
git status
git rm <conflict-file>
git commit
git show | head
** revert a commit by introducing a new commit
git revert 
"git revert" means creating a commit with the reverse patch to cancel it out

# revert 3 separate commits
git revert 0766c053 25eee4ca a867b4af

# It also takes ranges. This will revert the last two commits:
git revert HEAD~2..HEAD

** extract file as if they were in other commit (i.e., revert one file to older version)
git checkout <commit> -- <filename>
Caution! this discards uncommitted changes in your working directory
** how to branch
git checkout -b <new-branch-name>
this is equivalent to
git branch <new-branch-name>
git checkout <new-branch-name>
** what is branch and what is HEAD
Each branch is a file containing the pointer to a commit.
HEAD is also a file which has a pointer to the current branch.
When one does "git checkout <abranch-name>", HEAD is updated and the pointer it has now points to <branch-name>.
The branch that HEAD points to moves forward with each commit.
** how to view HEAD, its checksum, the file tree and the log file?
$ cat .git/HEAD 
ref: refs/heads/master

$ cat .git/refs/heads/master 
e9a570524b63d2a2b3a7c3325acf5b89bbeb131e

$ git cat-file -p e9a570524b63d2a2b3a7c3325acf5b89bbeb131e
tree cfda3bf379e4f8dba8717dee55aab78aef7f4daf
author Scott Chacon  1301511835 -0700
committer Scott Chacon  1301511835 -0700

initial commit

$ git ls-tree -r cfda3bf379e4f8dba8717dee55aab78aef7f4daf
100644 blob a906cb2a4a904a152...   README
100644 blob 8f94139338f9404f2...   Rakefile
040000 tree 99f1a6d12cb4b6f19...   lib
** how to stash a state
*** to list all stashes
git stash list
stash@{0}: WIP on master: 049d078 added the index file
stash@{1}: WIP on master: c264051 Revert "added file_size"
stash@{2}: WIP on master: 21d80a5 added number to log
*** to apply stash
$ git stash apply
# On branch master
# Changes not staged for commit:
#   (use "git add <file>..." to update what will be committed)
#
#      modified:   index.html
#      modified:   lib/simplegit.rb

You can also have modified and uncommitted files in your working directory when you apply a stash — Git gives you merge conflicts if anything no longer applies cleanly.
*** to apply older stash
git stash apply stash@{2}
*** to reapply staging while applying stash
git stash apply --index
*** to drop stash
git stash drop stash@{0}
*** to apply stash and drop
git stash pop
*** to unapply a stash
simply retrieving the patch associated with a stash and applying it in reverse
git stash show -p stash@{0} | git apply -R

Again, if you don’t specify a stash, Git assumes the most recent stash:
git stash show -p | git apply -R

or setup an alias
git config --global alias.stash-unapply '!git stash show -p | git apply -R'
*** to create a new branch from stash
git stash branch <branchname>

E.g.,
$ git stash branch testchanges
Switched to a new branch "testchanges"
# On branch testchanges
# Changes to be committed:
#   (use "git reset HEAD <file>..." to unstage)
#
#      modified:   index.html
#
# Changes not staged for commit:
#   (use "git add <file>..." to update what will be committed)
#
#      modified:   lib/simplegit.rb
#
Dropped refs/stash@{0} (f0dfc4d5dc332d1cee34a634182e168c4efc3359)

** what's a git "index"
It's the proposed next commit.
The staging area is a simple file, generally contained in your Git directory, that stores information about what will go into your next commit. It’s sometimes referred to as the index, but it’s becoming standard to refer to it as the staging area.
git ls-files -s
100644 a906cb2a4a904a152e80877d4088654daad0c859 0	README
100644 8f94139338f9404f26296befa88755fc2598c289 0	Rakefile
100644 47c6340d6459e05787f644c2447d2595f5d3a54b 0	lib/simplegit.rb
** setup autocompletion in git
download git-completion.bash from
https://github.com/git/git/blob/master/contrib/completion
source ~/git-completion.bash in your .bashrc or 
/opt/local/etc/bash_completion.d (mac) /etc/bash_completion.d/(linux)
** setup username and email
git config --global user.name "John Doe"
git config --global user.email johndoe@example.com
** setup git alias
git config --global alias.co heckout
git config --global alias.br branch
git config --global alias.ci commit
git config --global alias.st status
git config --global alias.unstage 'reset HEAD --'
git config --global alias.last 'log -1 HEAD'

# alias external command (no git preceeding that command)
git config --global alias.visual '!gitk'
** how to tag release points (e.g., v1.0 and so on)
*** list available tags
git tag

*** search tags
$ git tag -l 'v1.4.2.*'
v1.4.2.1
v1.4.2.2
v1.4.2.3
v1.4.2.4

*** add an annotated tag
git tag -a v1.4 -m 'my version 1.4'
git tag
v0.1
v1.3
v1.4

*** show tag annotation
$ git show v1.4
tag v1.4
Tagger: Scott Chacon <schacon@gee-mail.com>
Date:   Mon Feb 9 14:45:11 2009 -0800

my version 1.4

commit 15027957951b64cf874c3557a0f3547bd83b3ff6
Merge: 4a447f7... a6b4c97...
Author: Scott Chacon <schacon@gee-mail.com>
Date:   Sun Feb 8 19:02:46 2009 -0800

    Merge branch 'experiment'

*** create a signed tag (need private key)
$ git tag -s v1.5 -m 'my signed 1.5 tag'
You need a passphrase to unlock the secret key for
user: "Scott Chacon <schacon@gee-mail.com>"
1024-bit DSA key, ID F721C45A, created 2009-02-09

$ git show v1.5
tag v1.5
Tagger: Scott Chacon <schacon@gee-mail.com>
Date:   Mon Feb 9 15:22:20 2009 -0800

my signed 1.5 tag
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.8 (Darwin)

iEYEABECAAYFAkmQurIACgkQON3DxfchxFr5cACeIMN+ZxLKggJQf0QYiQBwgySN
Ki0An2JeAVUCAiJ7Ox6ZEtK+NvZAj82/
=WryJ
-----END PGP SIGNATURE-----
commit 15027957951b64cf874c3557a0f3547bd83b3ff6
Merge: 4a447f7... a6b4c97...
Author: Scott Chacon <schacon@gee-mail.com>
Date:   Sun Feb 8 19:02:46 2009 -0800

    Merge branch 'experiment'

*** create a lightweight tag
$ git tag v1.4-lw
$ git tag
v0.1
v1.3
v1.4
v1.4-lw
v1.5

$ git show v1.4-lw
commit 15027957951b64cf874c3557a0f3547bd83b3ff6
Merge: 4a447f7... a6b4c97...
Author: Scott Chacon <schacon@gee-mail.com>
Date:   Sun Feb 8 19:02:46 2009 -0800

    Merge branch 'experiment'

*** verify tags
$ git tag -v v1.4.2.1
object 883653babd8ee7ea23e6a5c392bb739348b1eb61
type commit
tag v1.4.2.1
tagger Junio C Hamano <junkio@cox.net> 1158138501 -0700

GIT 1.4.2.1

Minor fixes since 1.4.2, including git-mv and git-http with alternates.
gpg: Signature made Wed Sep 13 02:08:25 2006 PDT using DSA key ID F3119B9A
gpg: Good signature from "Junio C Hamano <junkio@cox.net>"
gpg:                 aka "[jpeg image of size 1513]"
Primary key fingerprint: 3565 2A26 2040 E066 C9A7  4A7D C0C6 D9A4 F311 9B9A

*** tag previous commit
git tag -a v1.2 -m 'version 1.2' 9fceb02

*** push tag(s)
# push one tag
git push origin v1.5
# push all tags
git push origin --tags

** to inspect a remote
git remote show [remote-name]
git remote show origin
** to see the difference between local and remote
git diff <local branch> <remote-tracking branch>
** to push to remote
git push origin master
** to unstage a file
git reset HEAD <file>
e.g.,
git reset HEAD benchmarks.rb
There's hint in "git status"
** to unmodify a modified file
git checkout -- <file>
e.g.,
git checkout -- benchmarks.rb
There's hint in "git status".

This is a dangerous command: any changes you made to that file are gone — you just copied another file over it. Don’t ever use this command unless you absolutely know that you don’t want the file.
Otherwise, use "git stash"
** to change last commit
git commit --amend

Example,
git commit -m "initial commit"
git add forgotten_file
git commit --amend
The previous, erroneous commit will be edited to reflect the new index state - in other words, it'll be like you never made the mistake in the first place :)

Note that you should only do this if you haven't pushed yet. If you have pushed, then you'll just have to commit a fix normally.

One can imagine "commit --amend" is short for 
git reset --soft HEAD^
git commit
Only that the above commit with a new username, email and message

** how to view remote-tracking branches
git branch -a
** how to show diff with color
git diff --color
** how to show only summary in diff
git diff --stat
** how to compare one branch with another (maybe remote)
git diff <local branch> <remote-name>/<remote-tracking branch>
git diff master origin/master

An interesting experiment,
========
/home/meder/foo #in the "local" directory
git init #initialize a repo and add stuff
git add .
git commit

/home/meder/bar #in the "remote" directory. can be any remote. 
git init #initialize a repo and add stuff. U
git add .
git commit

/home/meder/foo #back in the "local" directory
git remote add bar ../bar #add (register) "bar" as a remote
git fetch bar #fetch "bar"
git branch -a #lists all the branches. may be ommited
git diff master bar/master #diff
========

** how to remove file from staging area but keep on hard disk
This is particularly useful if you forgot to add something to your .gitignore file and accidentally staged it, like a large log file or a bunch of .a compiled files. To do this, use the --cached option:
git rm --cached readme.txt
** how to remove a modified file without commiting change?
git rm -f 
** to limit git log by 2 entries
git log -2
** to print diff in git log
git log -p
** to show word difference in git log
git log -U1 --word-diff
** to show summary of changes in git log
git log --stat
** oneline output of git log
git log --oneline
** how to put detailed diff into commit message
git commit -v
** how to set default editor for git
git config --global core.editor emacs
** setup diff tool
git config --global merge.tool vimdiff
** check your setting
# list all settings
git config --list
# view just one setting
git config user.name
** how to stage a modified file for commit
use "git add"
if you modified the file after you stage, the subsequent modification will NOT be commited. Only the modification at the moment when you stage (run "git add") will be commited. That's why it's called "stage"

# before
$ git status
On branch master
Changes to be committed:
  (use "git reset HEAD <file>..." to unstage)

        new file:   README

Changes not staged for commit:
  (use "git add <file>..." to update what will be committed)
  (use "git checkout -- <file>..." to discard changes in working directory)

        modified:   benchmarks.rb

$ git add benchmarks.rb
# after
$ git status
On branch master
Changes to be committed:
  (use "git reset HEAD <file>..." to unstage)

        new file:   README
        modified:   benchmarks.rb
** how to glob file names in git
git rm log/\*.log
git rm \*~
Note the backslash (\) in front of the \*. This is necessary because Git does its own filename expansion in addition to your shell’s filename expansion

** to see what you've changed but not staged
git diff
It’s important to note that git diff by itself doesn’t show all changes made since your last commit — only changes that are still unstaged. This can be confusing, because if you’ve staged all of your changes, git diff will give you no output.
** to see what you've staged but not commited
git diff --cached
or equivalently (after git 1.6.1)
git diff --staged
** how to resolve "non-fast-forward errors" while pushing
If another person (or yourself) has pushed to the same branch as you, Git won't be able to push you commits.

For example,
git push origin master
# To https://github.com/user/repo.git
#  ! [rejected]        master -> master (non-fast-forward)
# error: failed to push some refs to 'https://github.com/user/repo.git'
# To prevent you from losing history, non-fast-forward updates were rejected
# Merge the remote changes (e.g. 'git pull') before pushing again.  See the
# 'Note about fast-forwards' section of 'git push --help' for details.

git fetch origin
# Fetches updates made to an online repository
git merge origin branch
# Merges updates made online with your local work
Note that this is a merge with a "remote branch"

or equivalently,
git pull origin branch
# Grabs online updates and merges them with your local work
** how to abort a merge
If you run into a merge conflict you cannot resolve, or if you decide to quit the merge, you can use git merge --abort to take the branch back to where it was in before you pulled.
** what is equivalent to git pull
git pull is a shortcut for git fetch and git merge
e.g., 

** to show remote
git remote
origin
** to find out remote url
$ git remote -v
origin  git@bitbucket.org:wanding/ioan.git (fetch)
origin  git@bitbucket.org:wanding/ioan.git (push)
** to fetch all remote branches without merging
git fetch remotename
** to merge local with remote
git merge remotename/branchname
this is equivalent to
git merge remotename branchname
that's why sometimes we see
git merge origin master

Same to
git pull remotename/branchname

** what's "origin" in git
It's the usual name for the default remote
If you clone a repository, the command automatically adds that remote repository under the name origin.
** what happened in a git clone

    A new folder called repo is made
    It is initialized as a Git repository
    A remote named origin is created, pointing to the URL you cloned from
    All of the repository's files and commits are downloaded there
    The default branch (usually called master) is checked out

** to set the new url for a remote in git
$ git remote set-url origin git@github.com:someuser/newprojectname.git
This associates the name origin with the url git@github.com:someuser/newprojectname.git
"git remote set-url" is used to match a remote URL with a name.

in the old version
$ git remote rm origin
$ git remote add origin git@github.com:someuser/newprojectname.git
** what's the format for git remotes
ssh: git@github.com:user/repo.git
HTTTPS: https://github.com/user/repo.git
** to add a (new) remote
git remote add origin https://github.com/user/repo.git
then verify
git remote -v
# Verify new remote
origin  https://github.com/user/repo.git (fetch)
origin  https://github.com/user/repo.git (push)

** to switch remote from SSH to HTTPS
simply use "remote set-url"

# Before
git remote -v
# origin  git@github.com:USERNAME/REPOSITORY.git (fetch)
# origin  git@github.com:USERNAME/REPOSITORY.git (push)

git remote set-url origin https://github.com/USERNAME/REPOSITORY2.git

# After
git remote -v
# Verify new remote URL
# origin  https://github.com/USERNAME/REPOSITORY2.git (fetch)
# origin  https://github.com/USERNAME/REPOSITORY2.git (push)

** to rename a remote
# Before
git remote -v
# View existing remotes
# origin  git@github.com:user/repo.git (fetch)
# origin  git@github.com:user/repo.git (push)

git remote rename origin destination
# Change remote name from 'origin' to 'destination'

# After
git remote -v
# Verify remote's new name
# destination  git@github.com:user/repo.git (fetch)
# destination  git@github.com:user/repo.git (push)
** to remove a remote
git remote rm does not delete the remote repository from the server. It simply removes the remote and its references from your local repository.
# Before
git remote -v
# View current remotes
# origin  git@github.com:user/repo.git (fetch)
# origin  git@github.com:user/repo.git (push)
# destination  git@github.com:forker/repo.git (fetch)
# destination  git@github.com:forker/repo.git (push)

git remote rm destination

# After
git remote -v
# Verify it's gone
# origin  git@github.com:user/repo.git (fetch)
# origin  git@github.com:user/repo.git (push)
* Mercurial
** list all files
    hg st --all
** forget - forget the operation recorded on certain file
    hg forget Today.org
** clone - similar to checkout
    hg clone https://wanding@bitbucket.org/wanding/reaa
** hg add
    add all the rst file from hg st
    hg st | grep '.*.rst' | cut -d " " -f 2 | xargs hg add
** hg add (without anything else)
    add all files not tracked.
** bitbucket.org
*** start from scratch
     mkdir /path/to/your/project
     cd /path/to/your/project
     hg clone https://wanding@bitbucket.org/wanding/personal
*** wiki instruction
   Welcome
   Welcome to your wiki! This is the default page we've installed for your convenience. Go ahead and edit it.
   ## Wiki features
   This wiki uses the [Markdown](http://daringfireball.net/projects/markdown/) syntax.
   The wiki itself is actually a mercurial repository, which means you can clone it, edit it locally/offline, add images or any other file type, and push it back to us. It will be live immediately.
   Go ahead and try:
   ```
   $ hg clone https://bitbucket.org/wanding/mergeresplit/wiki
   ```
   Wiki pages are normal files, with the .md extension. You can edit them locally, as well as creating new ones.
   ## Syntax highlighting
   You can also highlight snippets of text (we use the excellent [Pygments][] library).
   [Pygments]: http://www.pygments.org/
   Here's an example of some Python code:
   ```
   #!python
   def wiki_rocks(text):
   formatter = lambda t: "funky"+t
   return formatter(text)
   ```
   You can check out the source of this page to see how that's done, and make sure to bookmark [the vast library of Pygment lexers][lexers], we accept the 'short name' or the 'mimetype' of anything in there.
   [lexers]: http://pygments.org/docs/lexers/
Have fun!
** push
    hg push https://wanding@bitbucket.org/wanding/reaa
    hg push https://wanding:password@bitbucket.org/wanding/reaa 
    hg push https://wanding@bitbucket.org/wanding/fasimup
** .hg/hgrc file
    [paths]
    default = https://wanding@bitbucket.org/wanding/fasimup
    [auth]
    wanding.prefix = https://bitbucket.org
    wanding.username = wanding
    wanding.password = wanding-pass
    [ui]
    ignore = ~/Dropbox/config/hgignore
** hgignore file - ignore emacs backup file (those with ~)
** init
    hg init
** update
    hg update [-r version]
    go back to history
** st
    hg st | awk '/\?/{print $2;}' | hg add
* bash
** variable substitution
if aaa is set
$ aaa='eee'
$ echo ${aaa+x}
x
$ unset aaa
$ echo ${aaa+x}
[nothing here]

There is also ${var:-x}, ${var:=x} etc.
see http://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_06_02

** join array
*** method1
function join { local IFS="$1"; shift; echo "$*"; }
*** method2
#!/bin/bash
foo=('foo bar' 'foo baz' 'bar baz')
bar=$(printf ",%s" "${foo[@]}")
bar=${bar:1}

echo $bar
** reverse a file
tail -r file
** how to loop over range of numbers                              :memorize:
three expression for loop
for (( EXP1; EXP2; EXP3 )); do command; done

Numerical range (explicit)
for i in 1 2 3 4 5; do command; done

numerical range (implicit) bash 3.0+
for i in {1..5}; do echo "Welcome $i times" done

numerical range (implicit) {START..END..INCREMENT} bash 4.0+
for i in {0..10..2}; do echo "Welcome $i times" done

NOTE: the seq command is outdated. use new version (below)
generate a sequence of numbers
for i in `seq 1 $END`; do echo $i; done
equivalent to
for ((i=1;i<=END;++i)); do echo $i; done
** convert rows to columns

A1
B1
C1

A2
B2
C2

=>
A1 B1 C1
A2 B2 C2

awk 'ORS=(NR%8==0)?"\n":"\t"{print $1}'
wzmanip transpose

** to use bash getopt                                              :unknown:
example 1
#+BEGIN_SRC sh

usage() { echo "Usage: $0 [-s <45|90>] [-p <string>]" 1>&2; exit 1; }

while getopts ":s:p:" o; do
    case "${o}" in
        s)
            s=${OPTARG}
            ((s == 45 || s == 90)) || usage
            ;;
        p)
            p=${OPTARG}
            ;;
        *)
            usage
            ;;
    esac
done
shift $((OPTIND-1))

if [ -z "${s}" ] || [ -z "${p}" ]; then
    usage
fi

echo "s = ${s}"
echo "p = ${p}"

Example runs:

$ ./myscript.sh
Usage: ./myscript.sh [-s <45|90>] [-p <string>]

$ ./myscript.sh -h
Usage: ./myscript.sh [-s <45|90>] [-p <string>]

$ ./myscript.sh -s "" -p ""
Usage: ./myscript.sh [-s <45|90>] [-p <string>]

$ ./myscript.sh -s 10 -p foo
Usage: ./myscript.sh [-s <45|90>] [-p <string>]

$ ./myscript.sh -s 45 -p foo
s = 45
p = foo

$ ./myscript.sh -s 90 -p bar
s = 90
p = bar
#+END_SRC

example 2
#+BEGIN_SRC sh
OPTIND=1         # Reset in case getopts has been used previously in the shell.

# Initialize our own variables:
output_file=""
verbose=0

while getopts "h?vf:" opt; do
    case "$opt" in
    h|\?)
        show_help
        exit 0
        ;;
    v)  verbose=1
        ;;
    f)  output_file=$OPTARG
        ;;
    esac
done

shift $((OPTIND-1))

[ "$1" = "--" ] && shift

echo "verbose=$verbose, output_file='$output_file', Leftovers: $@"

#+END_SRC

example 3 space separated
#+BEGIN_SRC sh
while [[ $# > 1 ]]
do
key="$1"
shift

case $key in
    -e|--extension)
    EXTENSION="$1"
    shift
    ;;
    -s|--searchpath)
    SEARCHPATH="$1"
    shift
    ;;
    -l|--lib)
    LIBPATH="$1"
    shift
    ;;
    --default)
    DEFAULT=YES
    shift
    ;;
    *)
            # unknown option
    ;;
esac
done
echo FILE EXTENSION  = "${EXTENSION}"
echo SEARCH PATH     = "${SEARCHPATH}"
echo LIBRARY PATH    = "${LIBPATH}"
echo "Number files in SEARCH PATH with EXTENSION:" $(ls -1 "${SEARCHPATH}"/*."${EXTENSION}" | wc -l)
if [[ -n $1 ]]; then
    echo "Last line of file specified as non-opt/last argument:"
    tail -1 $1
fi
#+END_SRC

example 4 equal separated
#+BEGIN_SRC sh
#!/bin/bash

for i in "$@"
do
case $i in
    -e=*|--extension=*)
    EXTENSION="${i#*=}"
    shift
    ;;
    -s=*|--searchpath=*)
    SEARCHPATH="${i#*=}"
    shift
    ;;
    -l=*|--lib=*)
    LIBPATH="${i#*=}"
    shift
    ;;
    --default)
    DEFAULT=YES
    shift
    ;;
    *)
            # unknown option
    ;;
esac
done
echo "FILE EXTENSION  = ${EXTENSION}"
echo "SEARCH PATH     = ${SEARCHPATH}"
echo "LIBRARY PATH    = ${LIBPATH}"
echo "Number files in SEARCH PATH with EXTENSION:" $(ls -1 "${SEARCHPATH}"/*."${EXTENSION}" | wc -l)
if [[ -n $1 ]]; then
    echo "Last line of file specified as non-opt/last argument:"
    tail -1 $1
fi
#+END_SRC

** to see/print the definition of a bash function
type <your function>
** to redirect stderr to file and screen                           :unknown:
command 2>(tee <log file> >&2)
** to create a file and input content                              :unknown:
cat >newfile
then type stuff
<Ctrl-D> to end
** change space to tab
handles multiple space
#+BEGIN_SRC sh
unexpand -a
#+END_SRC

or
#+BEGIN_SRC sh
tr -s [:blank:] \\t
#+END_SRC

** to change login to bash                                        :memorize:
if ( -x /bin/bash ) then
exec /bin/bash -l 
endif
** to replace a pattern of a string                               :memorize:

${string/substring/replacement} replace first match
${string//substring/replacement} replace all matches
E.g.,
stringZ=abcABC123ABCabc
echo ${stringZ/abc/xyz}       # xyzABC123ABCabc
echo ${stringZ//abc/xyz}      # xyzABC123ABCxyz
# parameterize
match=abc
repl=000
echo ${stringZ/$match/$repl}  # 000ABC123ABCabc
echo ${stringZ//$match/$repl} # 000ABC123ABC000
# What happens if no $replacement string is supplied? A simple deletion occurs
echo ${stringZ/abc}           # ABC123ABCabc
echo ${stringZ//abc}          # ABC123ABC
** to delete a pattern of a string                                :memorize:
stringZ=IPCT-CH-5031-Tumor-1161
echo ${stringZ//Tu*r} # IPCT-CH-5031--1161
echo ${stringZ/1} # delete first 1, IPCT-CH-503-Tumor-1161
echo ${stringZ//1} # delete all 1, IPCT-CH-503-Tumor-6
** how to extract pattern from string                             :memorize:
use expr
http://tldp.org/LDP/abs/html/string-manipulation.html
expr $a : '.*\(Tumor-[0-9]*\).*'
and
http://stackoverflow.com/questions/9597751/using-match-to-find-substrings-in-strings-with-only-bash

use BASH_REMATCH (better, no need to put .* in front and after the regular expression)
[[ $a =~ Tumor-([0-9]*) ]] && echo $BASH_REMATCH
[[ $Stext =~ ^(.[a-z]*) ]] && echo ${BASH_REMATCH[1]}

[[ $1 =~ (Apprentice|Master)?' '?(.*)' ('Level' '[0-9]+')' ]] && echo ${BASH_REMATCH[${#BASH_REMATCH[@]}-1]}
In addition to doing simple matching, bash regular expressions support sub-patterns surrounded by parenthesis for capturing parts of the match. The matches are assigned to an array variable BASH_REMATCH. The entire match is assigned to BASH_REMATCH[0], the first sub-pattern is assigned to BASH_REMATCH[1], etc..
$ AAA=RAxML_log.optimize.RUN.19
$ [[ $AAA =~ RUN.([0-9]+) ]] && echo ${BASH_REMATCH[0]}
RUN.19 
$ [[ $AAA =~ RUN.([0-9]+) ]] && echo ${BASH_REMATCH[1]}
19 

match regular expression with expr
(this is not a good way to match regex in bash, the behavior is weird, use =~)
expr match "$string" '$substring'
expr "$string" : '$substring'
expr match "$string" '.*\([0-9][.][0-9]\)'
$ AAA=RAxML_log.optimize.RUN.19
echo `expr match "$AAA" '.*\.\([0-9]\+\)'`
19

** to extract substring
${string:position}
${string:position:length}
expr substr $string $position $length
match from beginning
expr match "$string" '\($substring\)'
expr "$string" : '\($substring\)'
match from end
expr match "$string" '.*\($substring\)'
expr "$string" : '.*\($substring\)'

** to strip longest/shortest match from the front of a string     :memorize:

${string#substring}
Deletes shortest match of $substring from front of $string.
${string##substring}
Deletes longest match of $substring from front of $string.

stringZ=abcABC123ABCabc
echo ${stringZ#a*C}      # 123ABCabc
echo ${stringZ##a*C}     # abc

** to strip longest/shortest match from the back of a string      :memorize:

Delete shortest match of $substring from back
#+BEGIN_SRC sh
${string%substring}
#+END_SRC

Delete longest match of substring from back
#+BEGIN_SRC sh
${string%%substring}
#+END_SRC

stringZ=abcABC123ABCabc
echo ${stringZ%b*c}      # abcABC123ABCa
echo ${stringZ%%b*c}     # a
** to grep pattern1 or pattern2                                   :memorize:
grep 'pattern1\|pattern2' filename
grep -E 'pattern1|pattern2' filename
egrep 'pattern1|pattern2' filename

grep -E option is for extended regexp. If you use the grep command with -E option, you just need to use | to separate multiple patterns for the or condition.

** to grep pattern1 and pattern2
grep -e pattern1 -e pattern2 filename
** bash arithmetics                                                :unknown:
i=$((j+1))
note that there's no need to put $ before j
echo $(((_va - 1)*10))

(( uid > 1000 ))
(( percent >= 0 && percent <= 100 ))

i=$(( i + 1 ))
let i+=1

i=$((i++))
let i++

Another idiosyncrasy is that so called  R-value rule ($ should be prefixed to the shell variable on the right side of assignment statement to get its value) is optional and generally you should never use $ prefix for variables in ((..)) construct
i=$(( i + $2 ))
let i=i+1

let i+=5

old way
z=`expr $z + 3`
*** to increment a variable in bash                               :unknown:
((++var)) or ((var=var+1)) or ((var+=1))
((var++))
$((var+1))
let "var+=1"
let "var+=10"
see
http://tldp.org/LDP/abs/html/dblparens.html

** how to grep from the end of the file?
tac file | grep

** tertiary operator in bash
#+BEGIN_SRC sh
[ $b = 5] && a="$c" || a="$d"
#+END_SRC

** how to empty/clean a file
清空文件
#+BEGIN_SRC sh
:>data.xxx
# or
cat /dev/null data.xxx
#+END_SRC

** how to get directory name
dirname
** common unintuitive bash pitfalls
no space between "="
count=2;
not count = 2; (bash would confuse count for a command)

don't pipe to the same file
cat mydata.txt | sort | uniq > mydata.txt
bash would open mydata immediately to remove the original text in mydata.txt

always use "" around a string variable
if [ $var = "yes" ], when $var is empty, bash gets
if [ = "yes"] which is an error.

** string concatenation
#+BEGIN_SRC sh
$string=$string" and else";
# or
$string="$string and else";
#+END_SRC

bash also supports += operator
#+BEGIN_SRC sh
$ A="X Y"
$ A+="Z"
$ echo $A
X YZ
#+END_SRC

** how to tell the location of the bash script file sourced
$(dirname ${BASH_ARGV[0]})

** to resolve variable
use ${}
#+BEGIN_SRC sh
echo "MySHELL=>$SHELLCODE<="
# will look for variable $SHELLCODE
echo "MySHELL=>${SHELL}CODE<="
# will look for variable $SHELL
#+END_SRC

** indirect variable referencing with ${!var} (equivalent to \$$)
get the value of a value. ${!var}

#+BEGIN_SRC sh
$ a=letter_of_alphabet
$ letter_of_alphabet=z
$ echo "a = $a" # Direct reference
a=letter_of_alphabet
$ echo "Now a = ${!a}" # Indirect reference
a=z
#+END_SRC

The ${!variable} notation is more intuitive than the old eval var1=\$$var2
** command substitution
*** assign output of a command to a variable
#+BEGIN_SRC sh
var=$(command-name-here)
var=$(command-name-here arg1)
var=$(/path/to/command)
var=$(/path/to/command arg1 arg2)
# or
var=`command-name-here`
var=`command-name-here arg1`
var=`/path/to/command`
var=`/path/to/command arg1 arg2`
#+END_SRC

command substitution can happen directly in echo
#+BEGIN_SRC sh
echo "Today is $(date)"
# or
printf "Today is %s\n" "$(date)"
#+END_SRC
** brace expansion
echo {a,b}$PATH
is equivalent to
echo a$PATH b$PATH
declare -a 'pics=(img{' "$a..$b" '}.png)'; mv "${pics[@]}" ../imgs
echo {I,want,my,money,back}
echo _{I,want,my,money,back}-
outputs: _I- _want- _my- _money- _back-
echo {5..12}
outputs: 5 6 7 8 9 10 11 12
echo {c..k}
outputs: c d e f g h i j k
echo {01..10}
outputs: 01 02 03 04 05 06 07 08 09 10
echo {A..Z}{0..9}
outputs: A0 A1 ... Z9
this combines the iterations
echo {{A..Z},{a..z}}
outputs: A B ... Z a b ... z
wget http://docs.example.com/documentation/slides_part{1..6}.html
mkdir /home/bash/test/{foo,bar,baz,cat,dog}
for i in 0{1..9} 10; do printf "%s\n" "$i";done
printf "%s\n" img{00{1..9},0{10..99},{100..999}}.png
echo $(printf "img%02d.png " {1..99})
repeat arguments
somecommand -v{,,,,}
outputs: somecommand -v -v -v -v -v
echo {1..10..2}
increment by 2

NOTE that brace expansion ({}, or brace expansion) does not expand variables, so you cannot do {$start .. $end}!!! You have to use for ((i=$start;i<=$end;i++)) or for i in eval echo "{$start..$end}"
but eval is not safe.
** how to get the basename of a file path?
There are two ways:
*** using # and % operator
$ x="/foo/fizzbuzz.bar"
$ y=${x%.bar}
$ echo ${y##*/}
${x%.bar} could also be ${x%.*} to remove everything after a dot or ${x%%.*} to remove everything after the first dot.

Example:
$ x="/foo/fizzbuzz.bar.quux"
$ y=${x%.*}
$ echo $y
/foo/fizzbuzz.bar
$ y=${x%%.*}
$ echo $y
/foo/fizzbuzz

*** using "basename"
NAME=`basename /foo/fizzbuzz.bar .bar`
** spaces in function argument:
func arg1 arg2 arg3 : 3 string arguments
func "arg1 arg2 arg3" : 1 string arguments
args="arg1 arg2 arg3"; func $args : 3 arguments

** redirection
redirect error using 2>
kill -HUP 1234 >killout.txt 2>killerr.txt
redirect combined output using >&
kill -1 1234 >killouterr.txt 2>&1
discard output using /dev/null
kill -1 1234 >/dev/null 2>&1
if expr $aaa : "-f" >/dev/null; then echo 'fasdf'; fi

*** bash special variables
$*  a list of all the parameters in a single variable separated by the first character in IFS. if IFS is modified the way $* separates the command into parameters will change.
$@ a variation of $*, does not use IFS to separate parameters
$0 the name of the shell script
$$ the process ID of the shell script
$# the number of parameters passed
PS1,PS2,PS3... how the prompt is displayed
$IFS input field separator
** string manipulation
${#string}
expr length $string
expr "$string" : '.*'
the length of matched regular expression
** bash array
The Ultimate Bash Array Tutorial with 15 Examples
http://www.thegeekstuff.com/2010/06/bash-array-tutorial/

declare a bash array
declare -a arrayname=(element1 element2 element3)
declare -a Unix=('Debian' 'Red hat' 'Red hat' 'Suse' 'Fedora');

length of a bash array
${#arrayname[@]}

Print the entire bash array
echo ${arrayname[@]}
or
echo ${arrayname[*]}

+= usage
aa=(hello world)
aa+=(b c d)
# now aa is (hello world b c d)

extract index
${arrayname[1]}

aa=([hello]=world)
aa+=([b]=c)
# aa now contains 2 items

aa="hello"
aa+=" world"
# aa is now "hello world"

*** to iterate over bash array

#+BEGIN_SRC sh
source=("1" "2" "3" "4" "5")
for ((i=0; i < ${#source[@]}; i++))
do
  echo ${source[$i]}
done
#+END_SRC

*** to append to bash array
ARRAY=()
ARRAY+=('foo')
ARRAY+=('bar')

or
$ declare -a arr
$ arr=("a")
$ arr=("${arr[@]}" "new")
$ echo ${arr[@]}
a new
$ arr=("${arr[@]}" "newest")
$ echo ${arr[@]}
a new newest

** associative array
aka dictionary in Python and hash in Perl):
declare -A aa
aa[hello]=world
aa[ab]=cd
${a[ab]} # access the value cd
aa=([hello]=world [ab]=cd) # set the associative array in one run
if [[ ${aa[hello]} == world ]]; then
echo equal
fi
print out all the keys in b
for k in "${!b[@]}"doecho"$k"done

declare Integer
declare -i varname
readonly
declare -r varname
array
declare -a varname
associative array
declare -A varname
typeset is a synonym of declare
typeset -i i END

** expr
The expr builtin can be used as a simple integer calculator. Results are rounded to the nearest integer and floating point is unknown. Be sure to escape the multiplication asterisks (*) to avoid Bash expansion.
numeric comparison
bash$ expr 1 + 1
2
bash$ expr 3 \* 2
6
bash$ expr 6 / 3
2
bash$ expr 6 % 3
0
bash$ expr 3 / 2
1
bash$ expr 3 / 6
0
bash$ expr 6 \* 3.5
expr: non-numeric argument
expr 1 = 1
This is a judgement. It returns 1 (true).

bootsno=$(expr $bootsno + 1);

string comparison
aaa="-f";
if expr $aaa : "-f";  then echo 'fasdf'; fi

** test: [] and [[]]
test the result 
#+begin_example
[ ... ]
#+end_example

extended test :
#+begin_example
[[ ... ]] 
#+end_example
[[ is equivalent to [ except that [[]] can have &&, ||,< and > work within [[]] test, despite giving an error within a [] construct. And arithmetic evaluation of octal/hexadecimal constants takes place automatically within a [[]] construct


one can do “and” and “or” such as
#+begin_example
[[ -d “$HOME” && -w “$HOME” ]]
#+end_example

** how to test the exit status of a program

#+begin_src sh
  tail -1 myfile | grep -q PARTIAL
  # Note that you cannot test directly $? (e.g., "if $?; ") because bash will treat $? itself as a command and test its success (in this case always sucess)
  if [ $? -eq 0 ]; then
    echo success
  else
    echo fail
  fi
    
#+end_src
** test whether a variable is (not) empty
-n string is not null
#+BEGIN_SRC bash
[[ -n "$string" ]] && echo $string
#+END_SRC
-z string is null or empty

** test a file has non-zero size
#+BEGIN_SRC bash
if [[ -s "$file" ]]
#+END_SRC

** test string equal
= (or ==) strings are equal
!= strings are not equal

** arithmetic comparison
-eq equal
-ne not equal
-gt greater than
-ge greater than or equal to
-lt less than
-le less than or equal to

** echo
*** how to suppress newline in echo
     echo -n取消换行
*** how to print \t in echo
     echo -e "word/tword"
** uncompiled
    ${name:0:3} (0 means starting from position 0. 3 means length 3.)
    [[ is equivalent to [ except that [[]] can have &&, ||,< and > work within [[]] test, despite giving an error within a [] construct. And arithmetic evaluation of octal/hexadecimal constants takes place automatically within a [[...]] construct
    type something用来看something的类型。比如type test或者type '['
    "$variable"有替换，'$variable'无替换。
    \ is used to escape the following character
    echo -n取消换行
    $(command): 比如$(ls f*.sh)返回ls f*.sh的值。
    'command': 比如more 'grep -l POSIX *', 等于more $(grep -l POSIX *)
    exit $?	exit the exit status of the last command executed in the script

    [i]<>filename	opens file filename for reading and writing, and assigns file descriptor i to it.

    echo -e "word/tword"

    清空文件：> /var/log/ooxx.log
    :>data.xxx
    cat /dev/null data.xxx
*** how to get the basename of a file path?
There are two ways:
**** using # and % operator
#+begin_src bash
$ x="/foo/fizzbuzz.bar"
$ y=${x%.bar}
$ echo ${y##*/}
#+end_src
${x%.bar} could also be ${x%.*} to remove everything after a dot or ${x%%.*} to remove everything after the first dot.

Example:
#+begin_src bash
$ x="/foo/fizzbuzz.bar.quux"
$ y=${x%.*}
$ echo $y
/foo/fizzbuzz.bar
$ y=${x%%.*}
$ echo $y
/foo/fizzbuzz
#+end_src

**** using "basename"
#+begin_src bash
NAME=`basename /foo/fizzbuzz.bar .bar`
#+end_src
you can fizzbuzz

*** set
     set -- $planet 	#Parses variable "planet" and sets it to positional parameters.("--" is for preventing nasty surprise if $planet is null or begins with a dash)
     set -o: show all the option flag
     set -v: equivalent to "set -o verbose", to turn on the command echoing.
     set +v: equivalent to "set +o verbose", to turn of the command echoing

*** use ` `
     grep 'pathway_table' `find . -name '*model*'`
     find . -name '*model*' | xargs grep 'pathway_table'
** tell the location of the bash script file sourced
$(dirname ${BASH_ARGV[0]})

** return value of a shell command
cd aaa; echo $?
command1 && command2
if command1 fails, command2 would execute.

** resolve variable ambiguity using ${}
#+BEGIN_SRC sh
echo "MySHELL=>$SHELLCODE<="
# will look for variable $SHELLCODE
echo "MySHELL=>${SHELL}CODE<="
# will look for variable $SHELL
#+END_SRC

** indirect variable referencing with ${!var}
get the value of a value. ${!var}
#+BEGIN_SRC sh
a=letter_of_alphabet
letter_of_alphabet=z
echo "a = $a" # Direct reference
# output: a=letter_of_alphabet
echo "Now a = ${!a}" # Indirect reference
# output: a=z
#+END_SRC
The ${!variable} notation is more intuitive than the old eval var1=\$$var2
** command substitution
*** assign output of a command to a variable
#+BEGIN_SRC sh
var=$(command-name-here)
var=$(command-name-here arg1)
var=$(/path/to/command)
var=$(/path/to/command arg1 arg2)
# or
var=`command-name-here`
var=`command-name-here arg1`
var=`/path/to/command`
var=`/path/to/command arg1 arg2`
#+END_SRC
*** command substitution directly in echo
#+BEGIN_SRC sh
echo "Today is $(date)"
# or
printf "Today is %s\n" "$(date)"
#+END_SRC
** brace expansion
echo {a,b}$PATH
is equivalent to
echo a$PATH b$PATH
declare -a 'pics=(img{' "$a..$b" '}.png)'; mv "${pics[@]}" ../imgs
echo {I,want,my,money,back}
echo _{I,want,my,money,back}-
outputs: _I- _want- _my- _money- _back-
echo {5..12}
outputs: 5 6 7 8 9 10 11 12
echo {c..k}
outputs: c d e f g h i j k
echo {01..10}
outputs: 01 02 03 04 05 06 07 08 09 10
echo {A..Z}{0..9}
outputs: A0 A1 ... Z9
this combines the iterations
echo {{A..Z},{a..z}}
outputs: A B ... Z a b ... z
wget http://docs.example.com/documentation/slides_part{1..6}.html
mkdir /home/bash/test/{foo,bar,baz,cat,dog}
for i in 0{1..9} 10; do printf "%s\n" "$i";done
printf "%s\n" img{00{1..9},0{10..99},{100..999}}.png
echo $(printf "img%02d.png " {1..99})
repeat arguments
somecommand -v{,,,,}
outputs: somecommand -v -v -v -v -v
echo {1..10..2}
increment by 2

NOTE that brace expansion ({}, or brace expansion) does not expand variables, so you cannot do {$start .. $end}!!! You have to use for ((i=$start;i<=$end;i++)) or for i in eval echo "{$start..$end}"
but eval is not safe.

** for loop and control
three expression for loop
for (( EXP1; EXP2; EXP3 ))
do
command1
command2
command3
done

Numerical range (explicit)
for i in 1 2 3 4 5
do
echo "Welcome $i times"
done

numerical range (implicit) bash 3.0+
for i in {1..5}
do
echo "Welcome $i times"
done

numerical range (implicit) {START..END..INCREMENT} bash 4.0+
for i in {0..10..2}
do
echo "Welcome $i times"
done

break
continue

NOTE: the seq command is outdated. use new version (below)
generate a sequence of numbers
for i in `seq 1 $END`; do echo $i; done
equivalent to
for ((i=1;i<=END;++i)); do echo $i; done
** how to know the type of a variable
type something用来看something的类型。比如type test或者type '['

** wildcards
 * everything
? single character
[set] any of a number of single character
{} arbitrary strings together in a set
ls my_{finger, toe}s

** function argument:
func arg1 arg2 arg3 : 3 string arguments
func "arg1 arg2 arg3" : 1 string arguments
args="arg1 arg2 arg3"; func $args : 3 arguments

** redirection
redirect error using 2>
kill -HUP 1234 >killout.txt 2>killerr.txt
redirect combined output using >&
kill -1 1234 >killouterr.txt 2>&1
discard output using /dev/null
kill -1 1234 >/dev/null 2>&1
if expr $aaa : "-f" >/dev/null; then echo 'fasdf'; fi

*** bash special variables
$*  a list of all the parameters in a single variable separated by the first character in IFS. if IFS is modified the way $* separates the command into parameters will change.
$@ a variation of $*, does not use IFS to separate parameters
$0 the name of the shell script
$$ the process ID of the shell script
$# the number of parameters passed
PS1,PS2,PS3... how the prompt is displayed
$IFS input field separator

** string manipulation
${#string}
expr length $string
expr "$string" : '.*'
the length of matched regular expression

*** match regular expression (this is not a good way to match regex in bash, the behavior is weird, use =~)
expr match "$string" '$substring'
expr "$string" : '$substring'
expr match "$string" '.*\([0-9][.][0-9]\)'
$ AAA=RAxML_log.optimize.RUN.19
echo `expr match "$AAA" '.*\.\([0-9]\+\)'`
19

*** regular expression with =~
[[ $1 =~ (Apprentice|Master)?' '?(.*)' ('Level' '[0-9]+')' ]] && echo ${BASH_REMATCH[${#BASH_REMATCH[@]}-1]}
In addition to doing simple matching, bash regular expressions support sub-patterns surrounded by parenthesis for capturing parts of the match. The matches are assigned to an array variable BASH_REMATCH. The entire match is assigned to BASH_REMATCH[0], the first sub-pattern is assigned to BASH_REMATCH[1], etc..
$ AAA=RAxML_log.optimize.RUN.19
$ [[ $AAA =~ RUN.([0-9]+) ]] && echo ${BASH_REMATCH[0]}
RUN.19 
$ [[ $AAA =~ RUN.([0-9]+) ]] && echo ${BASH_REMATCH[1]}
19 

*** index
expr index $string $substring
Numerical position in $string of first character in $substring that matches.

*** substring
${string:position}
${string:position:length}
expr substr $string $position $length
match from beginning
expr match "$string" '\($substring\)'
expr "$string" : '\($substring\)'
match from end
expr match "$string" '.*\($substring\)'
expr "$string" : '.*\($substring\)'

*** string concatenation
#+BEGIN_SRC sh
$string=$string" and else";
# or
$string="$string and else";
#+END_SRC

bash also supports += operator
#+BEGIN_SRC sh
$ A="X Y"
$ A+="Z"
$ echo $A
X YZ
#+END_SRC

** bash array
The Ultimate Bash Array Tutorial with 15 Examples
http://www.thegeekstuff.com/2010/06/bash-array-tutorial/

declare a bash array
declare -a arrayname=(element1 element2 element3)
declare -a Unix=('Debian' 'Red hat' 'Red hat' 'Suse' 'Fedora');

length of a bash array
${#arrayname[@]}

Print the entire bash array
echo ${arrayname[@]}
or
echo ${arrayname[*]}

+= usage
aa=(hello world)
aa+=(b c d)
# now aa is (hello world b c d)

extract index
${arrayname[1]}

aa=([hello]=world)
aa+=([b]=c)
# aa now contains 2 items

aa="hello"
aa+=" world"
# aa is now "hello world"

** Associative array
aka dictionary in Python and hash in Perl):
declare -A aa
aa[hello]=world
aa[ab]=cd
${a[ab]} # access the value cd
aa=([hello]=world [ab]=cd) # set the associative array in one run
if [[ ${aa[hello]} == world ]]; then
echo equal
fi
print out all the keys in b
for k in "${!b[@]}"doecho"$k"done

declare Integer
declare -i varname
readonly
declare -r varname
array
declare -a varname
associative array
declare -A varname
typeset is a synonym of declare
typeset -i i END

** expr
The expr builtin can be used as a simple integer calculator. Results are rounded to the nearest integer and floating point is unknown. Be sure to escape the multiplication asterisks (*) to avoid Bash expansion.
numeric comparison
bash$ expr 1 + 1
2
bash$ expr 3 \* 2
6
bash$ expr 6 / 3
2
bash$ expr 6 % 3
0
bash$ expr 3 / 2
1
bash$ expr 3 / 6
0
bash$ expr 6 \* 3.5
expr: non-numeric argument
expr 1 = 1
This is a judgement. It returns 1 (true).

bootsno=$(expr $bootsno + 1);

string comparison
aaa="-f";
if expr $aaa : "-f";  then echo 'fasdf'; fi

** test: [ ... ]
test the result 
#+begin_example
[ ... ]
#+end_example

extended test :
#+begin_example
[[ ... ]] 
#+end_example
same as test but more flexible

one can do “and” and “or” such as
#+begin_example
[[ -d “$HOME” && -w “$HOME” ]]
#+end_example

*** test the exit status of a program

#+begin_src sh
  tail -1 myfile | grep -q PARTIAL
  # Note that you cannot test directly $? (e.g., "if $?; ") because bash will treat $? itself as a command and test its success (in this case always sucess)
  if [ $? -eq 0 ]; then
    echo success
  else
    echo fail
  fi
    
#+end_src
 
*** file conditions
-s nonzero size
-e file exists
-r has read permission
-f file is regular
-d file is a directory
-w file is writable
-x file is executable

*** string conditions
= (or ==) strings are equal
!= strings are not equal
-n string is not null
-z string is null or empty

*** arithmetic conditions
-eq equal
-ne not equal
-gt greater than
-ge greater than or equal to
-lt less than
-le less than or equal to
* C++
** how to use const keyword
*** The C++ 'const' Declaration: Why & How

The 'const' system is one of the really messy features of C++.

It is simple in concept, variables declared with ‘const’ added become constants and cannot be altered by the program, but, in the way is has to be used to bodge in a substitute for one of the missing features of C++, it gets horridly complicated and frustratingly restrictive. The following attempts to explain how 'const' is used and why it exists.

Simple Use of ‘const’

The simplest use is to declare a named constant. To do this, one declares a constant as if it was a variable but add ‘const’ before it. One has to initialise it immediately in the constructor because, of course, one cannot set the value later as that would be altering it. For example,

const int Constant1=96;
will create an integer constant, unimaginatively called ‘Constant1’, with the value 96.

Such constants are useful for parameters which are used in the program but are do not need to be changed after the program is compiled. It has an advantage for programmers over the C preprocessor ‘#define’ command in that it is understood & used by the compiler itself, not just substituted into the program text by the preprocessor before reaching the main compiler, so error messages are much more helpful.

It also works with pointers but one has to be careful where ‘const’ to determine whether the pointer or what it points to is constant or both. For example,

const int * Constant2
declares that Constant2 is variable pointer to a constant integer and

int const * Constant2
is an alternative syntax which does the same, whereas

int * const Constant3
declares that Constant3 is constant pointer to a variable integer and

int const * const Constant4
declares that Constant4 is constant pointer to a constant integer. Basically ‘const’ applies to whatever is on its immediate left (other than if there is nothing there in which case it applies to whatever is its immediate right).

Use of ‘const’ in Functions Return Values

Of the mixes of pointers and ‘const’, the constant pointer to a variable is useful for storage that can be changed in value but not moved in memory and the pointer (constant or otherwise) is useful for returning constant strings and arrays from functions which, because they are implemented as pointers, the program could otherwise try to alter and crash. Instead of a difficult to track down crash, the attempt to alter unalterable values will be detected during compilation.

For example, if a function which returns a fixed ‘Some text’ string is written like

char *Function1()
{ return “Some text”;}
then the program could crash if it accidentally tried to alter the value doing

Function1()[1]=’a’;
whereas the compiler would have spotted the error if the original function had been written

const char *Function1()
{ return "Some text";}
because the compiler would then know that the value was unalterable. (Of course, the compiler could theoretically have worked that out anyway but C is not that clever.)

Where it Gets Messy - in Parameter Passing

When a subroutine or function is called with parameters, variables passed as the parameters might be read from to transfer data into the subroutine/function, written to to transfer data back to the calling program or both to do both. Some languages enable one to specify this directly, such as having ‘in:’, ‘out:’ & ‘inout:’ parameter types, whereas in C one has to work at a lower level and specify the method for passing the variables choosing one that also allows the desired data transfer direction.

For example, a subroutine like

void Subroutine1(int Parameter1)
{ printf("%d",Parameter1);}
accepts the parameter passed to it in the default C & C++ way which is a copy. Therefore the subroutine can read the value of the variable passed to it but not alter it because any alterations it makes are only made to the copy and lost when the subroutine ends so

void Subroutine2(int Parameter1)
{ Parameter1=96;}
would leave the variable it was called with unchanged not set to 96.

The addition of an ‘&’ to the parameter name in C++ (which was a very confusing choice of symbol because an ‘&’ infront of variables elsewhere in C generate pointers!) like causes the actual variable itself, rather than a copy, to be used as the parameter in the subroutine and therefore can be written to thereby passing data back out the subroutine. Therefore

void Subroutine3(int &Parameter1) 
{ Parameter1=96;}
would set the variable it was called with to 96. This method of passing a variable as itself rather than a copy is called a ‘reference’ in C.

That way of passing variables was a C++ addition to C. To pass an alterable variable in original C, a rather involved method using a pointer to the variable as the parameter then altering what it pointed to was used. For example

void Subroutine4(int *Parameter1) 
{ *Parameter1=96;}
works but requires the every use of the variable in the called routine so altered and the calling routine altered to pass a pointer to the variable which is rather cumbersome.

But where does ‘const’ come into this? Well, there is a second common use for passing data by reference or pointer instead of copy. That is when copying a the variable would waste too much memory or take too long. This is particularly likely with large compound user-defined variable types (‘structures’ in C & ‘classes’ in C++). So a subroutine declared

void Subroutine4(big_structure_type &Parameter1);
might being using ‘&’ because it is going to alter the variable passed to it or it might just be to save copying time and there is no way to tell which it is if the function is compiled in someone else’s library. This could be a risk if one needs to trust the the subroutine not to alter the variable.

To solve this, ‘const’ can be used the in the parameter list like

void Subroutine4(big_structure_type const &Parameter1);
which will cause the variable to passed without copying but stop it from then being altered. This is messy because it is essentially making an in-only variable passing method from a both-ways variable passing method which was itself made from an in-only variable passing method just to trick the compiler into doing some optimization.

Ideally, the programmer should not need control this detail of specifying exactly how it variables are passed, just say which direction the information goes and leave the compiler to optimize it automatically, but C was designed for raw low-level programming on far less powerful computers than are standard these days so the programmer has to do it explicitly.

Messier Still - in the Object Oriented Programming

In Object Oriented Programming, calling a ‘method’ (the Object Oriented name for a function) of an object has gives an extra complication. As well as the variables in the parameter list, the method has access to the member variables of the object itself which are always passed directly not as copies. For example a trivial class, ‘Class1’, defined as

class Class1
{ void Method1();
  int MemberVariable1;}
has no explicit parameters at all to ‘Method1’ but calling it in an object in this class might alter ‘MemberVariable1’ of that object if ‘Method1’ happened to be, for example,

void Class1::Method1()
{ MemberVariable1=MemberVariable1+1;}
The solution to that is to put ‘const’ after the parameter list like

class Class2
{ void Method1() const;
  int MemberVariable1;}
which will ban Method1 in Class2 from being anything which can attempt to alter any member variables in the object.

Of course one sometimes needs to combine some of these different uses of ‘const’ which can get confusing as in

const int*const Method3(const int*const&)const;
where the 5 uses ‘const’ respectively mean that the variable pointed to by the returned pointer & the returned pointer itself won’t be alterable and that the method does not alter the variable pointed to by the given pointer, the given pointer itself & the object of which it is a method!.

Inconveniences of ‘const’

Besides the confusingness of the ‘const’ syntax, there are some useful things which the system prevents programs doing.

One in particular annoys me because my programs often need to be optimized for speed. This is that a method which is declared ‘const’ cannot even make changes to the hidden parts of its object that would not make any changes that would be apparent from the outside. This includes storing intermediary results of long calculations which would save processing time in subsequent calls to the class’s methods. Instead it either has to pass such intermediary results back to the calling routine to store and pass back next time (messy) or recalculate from scratch next time (inefficient). In later versions of C++, the ‘mutable’ keyword was added which enables ‘const’ to be overridden for this purpose but it totally relies on trusting the programmer to only use it for that purpose so, if you have to write a program using someone else's class which uses ‘mutable’ then you cannot guarantee that ‘‘mutable’ things will really be constant which renders ‘const’ virtually useless.

One cannot simply avoid using ‘const’ on class methods because ‘const’ is infectious. An object which has been made ‘const’, for example by being passed as a parameter in the ‘const &’ way, can only have those of its methods that are explicitly declared ‘const’ called (because C++’s calling system is too simple work out which methods not explicitly declared ‘const’ don’t actually change anything). Therefore class methods that don’t change the object are best declared ‘const’ so that they are not prevented from being called when an object of the class has somehow acquired ‘const’ status.

By Andrew Hardwick.
Distributable under GPL freeware licence. 
Written 2001.
Converted to HTML & augmented 2002/3/13.
Updated 2002/8/9, 2004/7/19, 2005/6/9 & 2006/5/22.
Spelling corrections 2008/3/4.
Available on-line at http://duramecho.com.The C++ 'const' Declaration: Why & How

The 'const' system is one of the really messy features of C++.

It is simple in concept, variables declared with ‘const’ added become constants and cannot be altered by the program, but, in the way is has to be used to bodge in a substitute for one of the missing features of C++, it gets horridly complicated and frustratingly restrictive. The following attempts to explain how 'const' is used and why it exists.

Simple Use of ‘const’

The simplest use is to declare a named constant. To do this, one declares a constant as if it was a variable but add ‘const’ before it. One has to initialise it immediately in the constructor because, of course, one cannot set the value later as that would be altering it. For example,

const int Constant1=96;
will create an integer constant, unimaginatively called ‘Constant1’, with the value 96.

Such constants are useful for parameters which are used in the program but are do not need to be changed after the program is compiled. It has an advantage for programmers over the C preprocessor ‘#define’ command in that it is understood & used by the compiler itself, not just substituted into the program text by the preprocessor before reaching the main compiler, so error messages are much more helpful.

It also works with pointers but one has to be careful where ‘const’ to determine whether the pointer or what it points to is constant or both. For example,

const int * Constant2
declares that Constant2 is variable pointer to a constant integer and

int const * Constant2
is an alternative syntax which does the same, whereas

int * const Constant3
declares that Constant3 is constant pointer to a variable integer and

int const * const Constant4
declares that Constant4 is constant pointer to a constant integer. Basically ‘const’ applies to whatever is on its immediate left (other than if there is nothing there in which case it applies to whatever is its immediate right).

Use of ‘const’ in Functions Return Values

Of the mixes of pointers and ‘const’, the constant pointer to a variable is useful for storage that can be changed in value but not moved in memory and the pointer (constant or otherwise) is useful for returning constant strings and arrays from functions which, because they are implemented as pointers, the program could otherwise try to alter and crash. Instead of a difficult to track down crash, the attempt to alter unalterable values will be detected during compilation.

For example, if a function which returns a fixed ‘Some text’ string is written like

char *Function1()
{ return “Some text”;}
then the program could crash if it accidentally tried to alter the value doing

Function1()[1]=’a’;
whereas the compiler would have spotted the error if the original function had been written

const char *Function1()
{ return "Some text";}
because the compiler would then know that the value was unalterable. (Of course, the compiler could theoretically have worked that out anyway but C is not that clever.)

Where it Gets Messy - in Parameter Passing

When a subroutine or function is called with parameters, variables passed as the parameters might be read from to transfer data into the subroutine/function, written to to transfer data back to the calling program or both to do both. Some languages enable one to specify this directly, such as having ‘in:’, ‘out:’ & ‘inout:’ parameter types, whereas in C one has to work at a lower level and specify the method for passing the variables choosing one that also allows the desired data transfer direction.

For example, a subroutine like

void Subroutine1(int Parameter1)
{ printf("%d",Parameter1);}
accepts the parameter passed to it in the default C & C++ way which is a copy. Therefore the subroutine can read the value of the variable passed to it but not alter it because any alterations it makes are only made to the copy and lost when the subroutine ends so

void Subroutine2(int Parameter1)
{ Parameter1=96;}
would leave the variable it was called with unchanged not set to 96.

The addition of an ‘&’ to the parameter name in C++ (which was a very confusing choice of symbol because an ‘&’ infront of variables elsewhere in C generate pointers!) like causes the actual variable itself, rather than a copy, to be used as the parameter in the subroutine and therefore can be written to thereby passing data back out the subroutine. Therefore

void Subroutine3(int &Parameter1) 
{ Parameter1=96;}
would set the variable it was called with to 96. This method of passing a variable as itself rather than a copy is called a ‘reference’ in C.

That way of passing variables was a C++ addition to C. To pass an alterable variable in original C, a rather involved method using a pointer to the variable as the parameter then altering what it pointed to was used. For example

void Subroutine4(int *Parameter1) 
{ *Parameter1=96;}
works but requires the every use of the variable in the called routine so altered and the calling routine altered to pass a pointer to the variable which is rather cumbersome.

But where does ‘const’ come into this? Well, there is a second common use for passing data by reference or pointer instead of copy. That is when copying a the variable would waste too much memory or take too long. This is particularly likely with large compound user-defined variable types (‘structures’ in C & ‘classes’ in C++). So a subroutine declared

void Subroutine4(big_structure_type &Parameter1);
might being using ‘&’ because it is going to alter the variable passed to it or it might just be to save copying time and there is no way to tell which it is if the function is compiled in someone else’s library. This could be a risk if one needs to trust the the subroutine not to alter the variable.

To solve this, ‘const’ can be used the in the parameter list like

void Subroutine4(big_structure_type const &Parameter1);
which will cause the variable to passed without copying but stop it from then being altered. This is messy because it is essentially making an in-only variable passing method from a both-ways variable passing method which was itself made from an in-only variable passing method just to trick the compiler into doing some optimization.

Ideally, the programmer should not need control this detail of specifying exactly how it variables are passed, just say which direction the information goes and leave the compiler to optimize it automatically, but C was designed for raw low-level programming on far less powerful computers than are standard these days so the programmer has to do it explicitly.

Messier Still - in the Object Oriented Programming

In Object Oriented Programming, calling a ‘method’ (the Object Oriented name for a function) of an object has gives an extra complication. As well as the variables in the parameter list, the method has access to the member variables of the object itself which are always passed directly not as copies. For example a trivial class, ‘Class1’, defined as

class Class1
{ void Method1();
  int MemberVariable1;}
has no explicit parameters at all to ‘Method1’ but calling it in an object in this class might alter ‘MemberVariable1’ of that object if ‘Method1’ happened to be, for example,

void Class1::Method1()
{ MemberVariable1=MemberVariable1+1;}
The solution to that is to put ‘const’ after the parameter list like

class Class2
{ void Method1() const;
  int MemberVariable1;}
which will ban Method1 in Class2 from being anything which can attempt to alter any member variables in the object.

Of course one sometimes needs to combine some of these different uses of ‘const’ which can get confusing as in

const int*const Method3(const int*const&)const;
where the 5 uses ‘const’ respectively mean that the variable pointed to by the returned pointer & the returned pointer itself won’t be alterable and that the method does not alter the variable pointed to by the given pointer, the given pointer itself & the object of which it is a method!.

Inconveniences of ‘const’

Besides the confusingness of the ‘const’ syntax, there are some useful things which the system prevents programs doing.

One in particular annoys me because my programs often need to be optimized for speed. This is that a method which is declared ‘const’ cannot even make changes to the hidden parts of its object that would not make any changes that would be apparent from the outside. This includes storing intermediary results of long calculations which would save processing time in subsequent calls to the class’s methods. Instead it either has to pass such intermediary results back to the calling routine to store and pass back next time (messy) or recalculate from scratch next time (inefficient). In later versions of C++, the ‘mutable’ keyword was added which enables ‘const’ to be overridden for this purpose but it totally relies on trusting the programmer to only use it for that purpose so, if you have to write a program using someone else's class which uses ‘mutable’ then you cannot guarantee that ‘‘mutable’ things will really be constant which renders ‘const’ virtually useless.

One cannot simply avoid using ‘const’ on class methods because ‘const’ is infectious. An object which has been made ‘const’, for example by being passed as a parameter in the ‘const &’ way, can only have those of its methods that are explicitly declared ‘const’ called (because C++’s calling system is too simple work out which methods not explicitly declared ‘const’ don’t actually change anything). Therefore class methods that don’t change the object are best declared ‘const’ so that they are not prevented from being called when an object of the class has somehow acquired ‘const’ status.

** basics
*** const pointer and pointer to const
The C++ 'const' Declaration: Why & How

The 'const' system is one of the really messy features of C++.

It is simple in concept, variables declared with ‘const’ added become constants and cannot be altered by the program, but, in the way is has to be used to bodge in a substitute for one of the missing features of C++, it gets horridly complicated and frustratingly restrictive. The following attempts to explain how 'const' is used and why it exists.

Simple Use of ‘const’

The simplest use is to declare a named constant. To do this, one declares a constant as if it was a variable but add ‘const’ before it. One has to initialise it immediately in the constructor because, of course, one cannot set the value later as that would be altering it. For example,

const int Constant1=96;
will create an integer constant, unimaginatively called ‘Constant1’, with the value 96.

Such constants are useful for parameters which are used in the program but are do not need to be changed after the program is compiled. It has an advantage for programmers over the C preprocessor ‘#define’ command in that it is understood & used by the compiler itself, not just substituted into the program text by the preprocessor before reaching the main compiler, so error messages are much more helpful.

It also works with pointers but one has to be careful where ‘const’ to determine whether the pointer or what it points to is constant or both. For example,

const int * Constant2
declares that Constant2 is variable pointer to a constant integer and

int const * Constant2
is an alternative syntax which does the same, whereas

int * const Constant3
declares that Constant3 is constant pointer to a variable integer and

int const * const Constant4
declares that Constant4 is constant pointer to a constant integer. Basically ‘const’ applies to whatever is on its immediate left (other than if there is nothing there in which case it applies to whatever is its immediate right).

Use of ‘const’ in Functions Return Values

Of the mixes of pointers and ‘const’, the constant pointer to a variable is useful for storage that can be changed in value but not moved in memory and the pointer (constant or otherwise) is useful for returning constant strings and arrays from functions which, because they are implemented as pointers, the program could otherwise try to alter and crash. Instead of a difficult to track down crash, the attempt to alter unalterable values will be detected during compilation.

For example, if a function which returns a fixed ‘Some text’ string is written like

char *Function1()
{ return “Some text”;}
then the program could crash if it accidentally tried to alter the value doing

Function1()[1]=’a’;
whereas the compiler would have spotted the error if the original function had been written

const char *Function1()
{ return "Some text";}
because the compiler would then know that the value was unalterable. (Of course, the compiler could theoretically have worked that out anyway but C is not that clever.)

Where it Gets Messy - in Parameter Passing

When a subroutine or function is called with parameters, variables passed as the parameters might be read from to transfer data into the subroutine/function, written to to transfer data back to the calling program or both to do both. Some languages enable one to specify this directly, such as having ‘in:’, ‘out:’ & ‘inout:’ parameter types, whereas in C one has to work at a lower level and specify the method for passing the variables choosing one that also allows the desired data transfer direction.

For example, a subroutine like

void Subroutine1(int Parameter1)
{ printf("%d",Parameter1);}
accepts the parameter passed to it in the default C & C++ way which is a copy. Therefore the subroutine can read the value of the variable passed to it but not alter it because any alterations it makes are only made to the copy and lost when the subroutine ends so

void Subroutine2(int Parameter1)
{ Parameter1=96;}
would leave the variable it was called with unchanged not set to 96.

The addition of an ‘&’ to the parameter name in C++ (which was a very confusing choice of symbol because an ‘&’ infront of variables elsewhere in C generate pointers!) like causes the actual variable itself, rather than a copy, to be used as the parameter in the subroutine and therefore can be written to thereby passing data back out the subroutine. Therefore

void Subroutine3(int &Parameter1) 
{ Parameter1=96;}
would set the variable it was called with to 96. This method of passing a variable as itself rather than a copy is called a ‘reference’ in C.

That way of passing variables was a C++ addition to C. To pass an alterable variable in original C, a rather involved method using a pointer to the variable as the parameter then altering what it pointed to was used. For example

void Subroutine4(int *Parameter1) 
{ *Parameter1=96;}
works but requires the every use of the variable in the called routine so altered and the calling routine altered to pass a pointer to the variable which is rather cumbersome.

But where does ‘const’ come into this? Well, there is a second common use for passing data by reference or pointer instead of copy. That is when copying a the variable would waste too much memory or take too long. This is particularly likely with large compound user-defined variable types (‘structures’ in C & ‘classes’ in C++). So a subroutine declared

void Subroutine4(big_structure_type &Parameter1);
might being using ‘&’ because it is going to alter the variable passed to it or it might just be to save copying time and there is no way to tell which it is if the function is compiled in someone else’s library. This could be a risk if one needs to trust the the subroutine not to alter the variable.

To solve this, ‘const’ can be used the in the parameter list like

void Subroutine4(big_structure_type const &Parameter1);
which will cause the variable to passed without copying but stop it from then being altered. This is messy because it is essentially making an in-only variable passing method from a both-ways variable passing method which was itself made from an in-only variable passing method just to trick the compiler into doing some optimization.

Ideally, the programmer should not need control this detail of specifying exactly how it variables are passed, just say which direction the information goes and leave the compiler to optimize it automatically, but C was designed for raw low-level programming on far less powerful computers than are standard these days so the programmer has to do it explicitly.

Messier Still - in the Object Oriented Programming

In Object Oriented Programming, calling a ‘method’ (the Object Oriented name for a function) of an object has gives an extra complication. As well as the variables in the parameter list, the method has access to the member variables of the object itself which are always passed directly not as copies. For example a trivial class, ‘Class1’, defined as

class Class1
{ void Method1();
  int MemberVariable1;}
has no explicit parameters at all to ‘Method1’ but calling it in an object in this class might alter ‘MemberVariable1’ of that object if ‘Method1’ happened to be, for example,

void Class1::Method1()
{ MemberVariable1=MemberVariable1+1;}
The solution to that is to put ‘const’ after the parameter list like

class Class2
{ void Method1() const;
  int MemberVariable1;}
which will ban Method1 in Class2 from being anything which can attempt to alter any member variables in the object.

Of course one sometimes needs to combine some of these different uses of ‘const’ which can get confusing as in

const int*const Method3(const int*const&)const;
where the 5 uses ‘const’ respectively mean that the variable pointed to by the returned pointer & the returned pointer itself won’t be alterable and that the method does not alter the variable pointed to by the given pointer, the given pointer itself & the object of which it is a method!.

Inconveniences of ‘const’

Besides the confusingness of the ‘const’ syntax, there are some useful things which the system prevents programs doing.

One in particular annoys me because my programs often need to be optimized for speed. This is that a method which is declared ‘const’ cannot even make changes to the hidden parts of its object that would not make any changes that would be apparent from the outside. This includes storing intermediary results of long calculations which would save processing time in subsequent calls to the class’s methods. Instead it either has to pass such intermediary results back to the calling routine to store and pass back next time (messy) or recalculate from scratch next time (inefficient). In later versions of C++, the ‘mutable’ keyword was added which enables ‘const’ to be overridden for this purpose but it totally relies on trusting the programmer to only use it for that purpose so, if you have to write a program using someone else's class which uses ‘mutable’ then you cannot guarantee that ‘‘mutable’ things will really be constant which renders ‘const’ virtually useless.

One cannot simply avoid using ‘const’ on class methods because ‘const’ is infectious. An object which has been made ‘const’, for example by being passed as a parameter in the ‘const &’ way, can only have those of its methods that are explicitly declared ‘const’ called (because C++’s calling system is too simple work out which methods not explicitly declared ‘const’ don’t actually change anything). Therefore class methods that don’t change the object are best declared ‘const’ so that they are not prevented from being called when an object of the class has somehow acquired ‘const’ status.

*** 左右法则 -- 如何理解c和c ++的复杂类型声明
曾经碰到过让你迷惑不解、类似于int * (* (*fp1) (int) ) [10];这样的变量声明吗？本文将由易到难，一步一步教会你如何理解这种复杂的C/C++声明。 

我们将从每天都能碰到的较简单的声明入手，然后逐步加入const修饰符和typedef，还有函数指针，最后介绍一个能够让你准确地理解任何C/C++声明的“右左法则”。 

需要强调一下的是，复杂的C/C++声明并不是好的编程风格；我这里仅仅是教你如何去理解这些声明。注意：为了保证能够在同一行上显示代码和相关注释，本文最好在至少1024x768分辨率的显示器上阅读。 
让我们从一个非常简单的例子开始，如下：
 int n;

这个应该被理解为“declare n as an int”（n是一个int型的变量）。接下去来看一下指针变量，如下： 
 int *p;
这个应该被理解为“declare p as an int *”（p是一个int *型的变量），或者说p是一个指向一个int型变量的指针。我想在这里展开讨论一下：我觉得在声明一个指针（或引用）类型的变量时，最好将*（或&）写在紧靠变量之前，而不是紧跟基本类型之后。这样可以避免一些理解上的误区，比如： 
再来看一个指针的指针的例子： 

 char **argv;

理论上，对于指针的级数没有限制，你可以定义一个浮点类型变量的指针的指针的指针的指针，再来看如下的声明： 

 int RollNum[30][4]; 
int (*p)[4]=RollNum; 
int *q[5];
这里，p被声明为一个指向一个4元素（int类型）数组的指针，而q被声明为一个包含5个元素（int类型的指针）的数组。另外，我们还可以在同一个声明中混合实用*和&，如下： 

 int **p1; 
// p1 is a pointer  to a pointer  to an int. 
int *&p2; 
// p2 is a reference to a pointer  to an int. 
int &*p3; 
// ERROR: Pointer  to a reference is illegal. 
int &&p4;
// ERROR: Reference to a reference is illegal.


注：p1是一个int类型的指针的指针；p2是一个int类型的指针的引用；p3是一个int类型引用的指针（不合法！）；p4是一个int类型引用的引用（不合法！）。 

const修饰符 

当你想阻止一个变量被改变，可能会用到const关键字。在你给一个变量加上const修饰符的同时，通常需要对它进行初始化，因为以后的任何时候你将没有机会再去改变它。例如： 

 const int n=5; 
int const m=10;


上述两个变量n和m其实是同一种类型的——都是const int（整形恒量）。因为C++标准规定，const关键字放在类型或变量名之前等价的。我个人更喜欢第一种声明方式，因为它更突出了const修饰符的作用。当const与指针一起使用时，容易让人感到迷惑。例如，我们来看一下下面的p和q的声明： 

 const int *p; 
int const *q;


他们当中哪一个代表const int类型的指针（const直接修饰int），哪一个代表int类型的const指针（const直接修饰指针）？实际上，p和q都被声明为const int类型的指针。而int类型的const指针应该这样声明： 


 int * const r= &n;
// n has been declared as an int


这里，p和q都是指向const int类型的指针，也就是说，你在以后的程序里不能改变*p的值。而r是一个const指针，它在声明的时候被初始化指向变量n（即r=&n;）之后，r的值将不再允许被改变（但*r的值可以改变）。 

组合上述两种const修饰的情况，我们来声明一个指向const int类型的const指针，如下： 

 const int * const p=&n 
// n has been declared as const int


下面给出的一些关于const的声明，将帮助你彻底理清const的用法。不过请注意，下面的一些声明是不能被编译通过的，因为他们需要在声明的同时进行初始化。为了简洁起见，我忽略了初始化部分；因为加入初始化代码的话，下面每个声明都将增加两行代码。 

 char ** p1; 
//    pointer to    pointer to    char 
const char **p2;
//    pointer to    pointer to const char 
char * const * p3;
//    pointer to const pointer to    char 
const char * const * p4;
//    pointer to const pointer to const char 
char ** const p5;
// const pointer to    pointer to    char 
const char ** const p6;
// const pointer to    pointer to const char 
char * const * const p7;
// const pointer to const pointer to    char 
const char * const * const p8;
// const pointer to const pointer to const char


注：p1是指向char类型的指针的指针；p2是指向const char类型的指针的指针；p3是指向char类型的const指针；p4是指向const char类型的const指针；p5是指向char类型的指针的const指针；p6是指向const char类型的指针的const指针；p7是指向char类型const指针的const指针；p8是指向const char类型的const指针的const指针。 

typedef的妙用 

typedef给你一种方式来克服“*只适合于变量而不适合于类型”的弊端。你可以如下使用typedef： 

 typedef char * PCHAR; 
PCHAR p,q;


这里的p和q都被声明为指针。（如果不使用typedef，q将被声明为一个char变量，这跟我们的第一眼感觉不太一致！）下面有一些使用typedef的声明，并且给出了解释： 

 typedef char * a;
// a is a pointer to a char 

typedef a b();
// b is a function that returns 
// a pointer to a char 

typedef b *c;
// c is a pointer to a function 
// that returns a pointer to a char 

typedef c d();
// d is a function returning 
// a pointer to a function 
// that returns a pointer to a char 

typedef d *e;
// e is a pointer to a function 
// returning a pointer to a 
// function that returns a 
// pointer to a char 

e var[10];
// var is an array of 10 pointers to 
// functions returning pointers to 
// functions returning pointers to chars.


typedef经常用在一个结构声明之前，如下。这样，当创建结构变量的时候，允许你不使用关键字struct（在C中，创建结构变量时要求使用struct关键字，如struct tagPOINT a；而在C++中，struct可以忽略，如tagPOINT b）。 

 typedef struct tagPOINT 
{ 
  int x; 
  int y; 
}POINT; 

POINT p; /* Valid C code */
函数指针

函数指针可能是最容易引起理解上的困惑的声明。函数指针在DOS时代写TSR程序时用得最多；在Win32和X-Windows时代，他们被用在需要回调函数的场合。当然，还有其它很多地方需要用到函数指针：虚函数表，STL中的一些模板，Win NT/2K/XP系统服务等。让我们来看一个函数指针的简单例子：





 int (*p)(char);


这里p被声明为一个函数指针，这个函数带一个char类型的参数，并且有一个int类型的返回值。另外，带有两个float类型参数、返回值是char类型的指针的指针的函数指针可以声明如下： 

 char ** (*p)(float, float);


那么，带两个char类型的const指针参数、无返回值的函数指针又该如何声明呢？参考如下： 

 void * (*a[5])(char * const, char * const);


“右左法则”是一个简单的法则，但能让你准确理解所有的声明。这个法则运用如下：从最内部的括号开始阅读声明，向右看，然后向左看。当你碰到一个括号时就调转阅读的方向。括号内的所有内容都分析完毕就跳出括号的范围。这样继续，直到整个声明都被分析完毕。 

对上述“右左法则”做一个小小的修正：当你第一次开始阅读声明的时候，你必须从变量名开始，而不是从最内部的括号。 

下面结合例子来演示一下“右左法则”的使用。 

 int * (* (*fp1) (int) ) [10];


阅读步骤： 

1. 从变量名开始——fp1 

2. 往右看，什么也没有，碰到了)，因此往左看，碰到一个*——一个指针 

3. 跳出括号，碰到了(int)——一个带一个int参数的函数 

4. 向左看，发现一个*——（函数）返回一个指针 

5. 跳出括号，向右看，碰到[10]——一个10元素的数组 

6. 向左看，发现一个*——指针 

7. 向左看，发现int——int类型 

总结：fp1被声明成为一个函数的指针,该函数返回指向指针数组的指针. 

再来看一个例子： 

 int *( *( *arr[5])())();


阅读步骤： 

1. 从变量名开始——arr 

2. 往右看，发现是一个数组——一个5元素的数组 

3. 向左看，发现一个*——指针 

4. 跳出括号，向右看，发现()——不带参数的函数 

5. 向左看，碰到*——（函数）返回一个指针 

6. 跳出括号，向右发现()——不带参数的函数 

7. 向左，发现*——（函数）返回一个指针 

8. 继续向左，发现int——int类型 

还有更多的例子： 

 float ( * ( *b()) [] )();
// b is a function that returns a 
// pointer to an array of pointers 
// to functions returning floats. 
void * ( *c) ( char, int (*)());
// c is a pointer to a function that takes 
// two parameters: 
// a char and a pointer to a 
// function that takes no 
// parameters and returns 
// an int 
// and returns a pointer to void. 
void ** (*d) (int &, 
char **(*)(char *, char **));
// d is a pointer to a function that takes 
// two parameters: 
// a reference to an int and a pointer 
// to a function that takes two parameters: 
// a pointer to a char and a pointer 
// to a pointer to a char 
// and returns a pointer to a pointer 
// to a char 
// and returns a pointer to a pointer to void 
float ( * ( * e[10]) 
  (int &) ) [5];
// e is an array of 10 pointers to 
// functions that take a single 
// reference to an int as an argument 
// and return pointers to 
// an array of 5 floats.
*** tagname and typedef names
all struct/union/enum/class declarations act like they are implicitly typedef'ed, as long as the name is not hidden by another declaration with the same name
http://stackoverflow.com/questions/612328/difference-between-struct-and-typedef-struct-in-c
That means in C++, when you say
#+begin_src c++
struct Foo { ... };
#+end_src
you don't have to declare object as
#+begin_src c++
struct Foo ob;
#+end_src
all the time (in contrast to pure C). You could just say:
#+begin_src c++
Foo ob;
#+end_src

*** number(integer, float etc) to string
**** method (require c++11)
C++0x introduces std::stoi (and variants for each numeric type) and std::to_string, the counterparts of the C atoi and itoa but expressed in term of std::string
#+begin_src c++
  std::string s = std::to_string(42);
#+end_src
**** method by using ostringstream
#+begin_src c++
  #include <sstream>
  #define SSTR( x ) dynamic_cast< std::ostringstream & >( \
          ( std::ostringstream() << std::dec << x ) ).str()
  // to use
  int x = 42;
  cout << SSTR( "i is: " << x );
  string s = SSTR( i );
  puts( SSTR( i ).c_str() );
#+end_src
or
#+begin_src c++
  #include <sstream>
  template <typename T>
    string NumberToString ( T Number )
    {
       ostringstream ss;
       ss << Number;
       return ss.str();
    }
#+end_src
Here the number can be any type: integer, float etc.
**** method by using lexical_cast from Boost
#+begin_src c++
#include <boost/lexical_cast.hpp>
int num = 4;
std::string str = boost::lexical_cast<std::string>(num);
#+end_src 
** namespace
#+begin_src c++
// both
using namespace std
// and
using std:cin
#+end_src
allows you to use "cin" instead of "std::cin"

** How to dynamically initiate a matrix / 2-d array?
    int **matrix;
    matrix = new int* [num_rows]
    for (int i = 0; i < num_rows; i++)
    *(matrix+i) = new int[num_columns];
    # to use, just type matrix[i][j]
    # or use vector<vector< > >
    vector<vector<double> > matrix;
    matrix.resize(num_rows);
    for(int i = 0; i < num_rows; i++)
    matrix[i].resize(num_columns)

** STL
*** how to reverse a string?
#+begin_src c++
#include <algorithm>
std::reverse(str.begin(), str.end());
#+end_src
** struct vs class in C++
从语法上，在C++中（只讨论C++中）。class和struct做类型定义时只有两点区别：

　　（一）默认继承权限。如果不明确指定，来自class的继承按照private继承处理，来自struct的继承按照public继承处理；

　　（二）成员的默认访问权限。class的成员默认是private权限，struct默认是public权限。

　　除了这两点，class和struct基本就是一个东西。语法上没有任何其它区别。

　　不能因为学过C就总觉得连C++中struct和class都区别很大，下面列举的说明可能比较无聊，因为struct和class本来就是基本一样的东西，无需多说。但这些说明可能有助于澄清一些常见的关于struct和class的错误认识：

　　（1）都可以有成员函数；包括各类构造函数，析构函数，重载的运算符，友元类，友元结构，友元函数，虚函数，纯虚函数，静态函数；

　　（2）都可以有一大堆public/private/protected修饰符在里边；

　　（3）虽然这种风格不再被提倡，但语法上二者都可以使用大括号的方式初始化：A a = {1, 2, 3};不管A是个struct还是个class，前提是这个类/结构足够简单，比如所有的成员都是public的，所有的成员都是简单类型，没有显式声明的构造函数。

　　（4）都可以进行复杂的继承甚至多重继承，一个struct可以继承自一个class，反之亦可；一个struct可以同时继承5个class和5个struct，虽然这样做不太好。

　　（5）如果说class的设计需要注意OO的原则和风格，那么没任何理由说设计struct就不需要注意。

　　（6）再次说明，以上所有说法都是指在C++语言中，至于在C里的情况，C里是根本没有“class”，而C的struct从根本上也只是个包装数据的语法机制。

　　---------------------------------------------------------------

　　最后，作为语言的两个关键字，除去定义类型时有上述区别之外，另外还有一点点：“class”这个关键字还用于定义模板参数，就像“typename”。但关键字“struct”不用于定义模板参数。

　　如果没有多态和虚拟继承，在C++中，struct和class的存取效率完全相同！简单的说就是，存取class的data member和非virtual function效率和struct完全相同！不管该data member是定义在基类还是派生类的。

　　如果不是为了和C兼容，C++中就不会有struct关键字。因此建议是：如果不需要与C兼容或传递参数给C程序，不要在C++中用struct。

　　注意class的data member在内存中的布局可不一定是data member的申明次序。C++只保证处于同一个access section的data member按照申明次序排列。

　　struct所体现的是一种数据结构，而class则是体现OOP思想中的"封装"的特性~~~

　　还有一个区别：struct可以用{}赋初值，而class不行

　　比如声明如下：

　　struct abc{ int m1; float m2; bool m3; }

　　可以这么构造对象：

　　abc abcInstance{ 1, 1.0f, false };

　　struct：属性

　　class：属性+行为

　　注意：在VC6里，class可以与模板关键字typename互换，但是struct好像就不可以，编译好像通不过。对这个问题，我专门查了一些资料，发现网上确实有说 struct不能用于模板关键字而class可以，这似乎应该是他们的一个不同了。然而，我又看了一下 《深度探索C++对象模型》，在书的前几章（好像就是第一章）Lippman说：本来他的编译器是不支持将struct作为模板关键字的，但后来改变了，也就是说struct 和class除了默认的访问属性外，其他场合下真正的完全一样了。对此，我认为这个按理说是这样的，但不同的编译器可能会有自己的处理，就像VC6那样。

　　class中有方法,

　　struct中没有.

　　class是一个扩展的struct

　　array(类型一样)->struct(类型可以不一致)->class(添加方法)

　　虽然两者都可以捆绑行为。

　　但是，理解不一样。

　　struct，就是对程序员全局可见的数据与方法。简化数据逻辑结构的设计。可以说是一种自定义的数据结构。

　　而class，则是将数据与方法封装，即让行为与数据一致。则是一种编程方法。即客观世界在代码世界中的体现。体现的是一种编程思想。

　　在C里面：struct不能包含函数，而class可以。

　　在C++里面：都可以有函数，默认情况下struct中变量是public,而class中是private

　　有一点不明白,class支持的继承和多态,struct也支持??

　　class在赋值运算符右边出现需要有定义的拷贝构造函数,而struct是默认的位拷贝.

　　但是一般从兼容C的角度考虑，struct里面只包含数据成员而不包含成员函数，这只是一个编程习惯问题。

** STL and C++ standard library
neither is a strict superset of the other.
For example, <hash_map> is in STL but not in C++ standard library
<map> is shared between STL and C++ standard library
** qt
*** signal-slot
**** connect
      connect(sender, SIGNAL(signal), receiver, SLOT(slot));
      both sender and receiver are of QObject.
      signal and slot are function name.
      SIGNAL() and SLOT() are macros used for transforming arguments into strings.
***** one signal can be connected to multiple slots
       connect(slider, SIGNAL(valueChanged(int)),
       spinBox, SLOT(setValue(int)));
       connect(slider, SIGNAL(valueChanged(int)),
       this, SLOT(updateStatusBarIndicator(int)));
       // But notice that the order of being called is undetermined.
       // It is not necessarily the order the codes that appear.
***** multiple signals can be connected to one slot
       connect(lcd, SIGNAL(overflow()), this, SLOT(handleMathError()));
       connect(calculator, SIGNAL(divisionByZero()), this, SLOT(handleMathError()));
***** a signal can be connected to another signal
       connect(lineEdit, SIGNAL(textChanged(const QString &amp;)),
       this, SIGNAL(updateRecord(const QString &amp;)));
***** a signal can be disconnected
       disconnect(lcd, SIGNAL(overflow()), this, SLOT(handleMathError()));
**** 当一个对象 delete 之后,Qt 自动取消所有连接到这个对象上面的槽。
**** 为了正确的连接信号槽,信号和槽的参数个数、类型以及出现的顺序都必须相同.
      connect(ftp, SIGNAL(rawCommandReply(int, const QString& amp;)),
      this, SLOT(processReply(int, const QString& amp;)));  
      有一种例外情况,如果信号的参数多于槽的参数,那么这个参数之后的那些参数都会被忽略掉
*** compile
    1. create .pro file
       qmake -project
       the name of the .pro file follows the name of the current working directory
    2. create Makefile
       qmake -unix -o Makefile mycode.pro (or in short "qmake", which will look for the .pro file in the current directory.)
       Or, qmake -tp vc helloworld.pro (this generate nmake file)
    3. make
       make
*** QPainter
**** void QPainter::drawEllipse (const QRectF & rectangle )
      Draws the ellipse defined by the given rectangle.
***** example
       QRectF rectangle(10.0, 20.0, 80.0, 60,0);
       QPainter painter(this);
       painter.drawEllipse(rectangle)
**** void QPainter::drawEllipse (int x, int y, int width, int height)
      Draws the ellipse defined by the rectangle beginning at (x, y) with the given width and height.
**** void QPainter::drawEllipse (const QPointF &center, qreal rx, qreal ry)
      Draws the ellipse positioned at center with radii rx and ry.
**** void QPainter::drawEllipse (const QPoint &center, int rx, int ry)
      Draws the ellipse positioned at center with radii rx and ry.
*** QRectF
     rectangle using float precision
*** QRect
     rectangle using integer precision
*** QVariant
**** int QVariant::toInt ( bool * ok = 0 ) const
      Returns the variant as an int if the variant has type() Int, Bool, ByteArray, Char, Double, LongLong, String, UInt, or ULongLong; otherwise returns 0.
*** QWidget
**** QComboBox
***** QVariant QComboBox::itemData(int index, int role = Qt::UserRole) const
       Returns the data for the given role in the given index in the combobox
*** layout
     setLayout(someLayout);
**** QHBoxLayout
**** QVBoxLayout
**** QGridLayout
***** void QGridLayout::addWidget(QWidget * widget, int row, int column, Qt::Alignment alignment = 0)
       add the given widget to the cell grid at row, column. (0,0) is by default.
       alignment = 0 means the widget fills the entire cell.
***** void QGridLayout::addWidget(QWidget * widget, int fromRow, int fromColumn,int rowSpan, int columnSpan, Qt::Alignment alignment = 0)
       add the given widget to the cell grid, spanning multiple rows/column. The cell will start at (fromRow, fromColumn) spanning rowSpan rows and columnSpan columns.
       if rowSpan/columnSpan is -1, then the widget will extend to the bottom/right edge.
***** void QGridLayout::setColumnStretch (int column, int stretch)
       the stretch factor is relative to the other columns in this grid.
       Columns with a higher stretch factor take more of the available space.
       default stretch factor is 0. if 0, then no other column in this table can grow at all, the column may still grow.
****** example
       	mainLayout->setColumnStretch(0, 1);
***** void QGridLayout::setRowMinimumHeight(int row, int minSize)
       set the minimum height of row row to minSize pixels
****** example
       	mainLayout->setRowMinimumHeight(8, 6);
** boost
*** boost/graph
**** boost/graph/adjacency_list.hpp
***** add_edge(int node1, int node2, g)
***** [template] adjacency_list
****** edge iterator constructor
       	adjacency_list(
       	EdgeIterator first, 
       	EdgeIterator last, 
       	EdgePropertyIterator ep_iter,
       	vertices_size_type n,
       	edges_size_type m = 0,
       	const GraphProperty& p = GraphProperty()
       	)
**** boost/graph/graph_traits.hpp
***** [template] graph_traits
*** boost/property_map
**** boost/property_map/property_map.hpp
***** get(p,g)
       return the property map for the property specified by the PropertyTag type p.
       return boost::property_map<G, PropertyTag>::type is g is mutable.
       return boost::property_map<G, PropertyTag>::const_type if g is not mutable.
***** get(p,g,x)
       return the property value associated with object x (a vertex or edge). 
       equivalent to get(get(p,g), x)
       return type: boost::property_traits<Map>::value_type
       where Map is boost::property_map<G,Property>::const_type
***** put(p,g,x,v)
       set the property (specified by the PropertyTag type) associated with object x (a vertex or an edge) to the value v.
       equivalent to: put(get(p,g), x,v)
*** boost/tuple/tuple.hpp
**** tie
*** boost/algorithm/string
**** split
*** boost/foreach.hpp
*** boost/tokenizer.hpp
     BOOST_FOREACH
     #define foreach BOOST_FOREACH
*** boost/utility.hpp
**** next(p) 不用改变iterator p，而访问p的下一个或前一个变量。
      因为p++会改变p。
***** usage
       const std::list<T>::iterator p = get_some_iterator();
       const std::list<T>::iterator prev = boost::prior(p);
       const std::list<T>::iterator next = boost::next(prev, 2);
** cmake
*** what is in-source build
     cd Hello
     ccmake . ## this will look for the CMakeLists.txt 
     ## or one might explicitly specify that.
     make
*** what is out-of-source build
     mkdir HelloBuild
     cd HelloBuild
     ccmake ../Hello
     make
     Note: Before performing an out-of-source build ensure that any possible CMake generated in-source build information is removed from the source directory, e.g., CMakeFiles directory, and CMakeCache.txt.
     An in-source build CMakeCache.txt file can make an out-of-source build ineffective.
*** why the build tree CANNOT be copied or moved
     since CMake use absolute path.
*** cmake -DCMAKE_BUILD_TYPE=Debug ## debug build
*** cmake -DCMAKE_BUILD_TYPE=Release ## release build
*** commands
**** add_executable
      Add an executable to the project using the specified source files.
      add_executable(<name> [WIN32] [MACOSX_BUNDLE] [EXCLUDE_FROM_ALL] source2 ... sourceN)
**** add_subdirectory
**** add_library
      add_library(pcre ${PCRE_HEADERS} ${PCRE_SOURCES})
      add_library(pcreposix ${PCREPOSIX_HEADERS} ${PCREPOSIX_SOURCES})
      target_link_libraries(pcreposix pcre)
**** add_definitions
      Adds -D define flags to the compilation of source files.
      add_definitions(-DFOO -DBAR ...)
      will result in gcc -D FOO
      Adds flags to the compiler command line for sources in the current directory and below. 
**** checkincludefile
**** execute_process
**** message
      message(STATUS "The build type is ${CMAKE_BUILD_TYPE}")
      message(WARNING "")
      message(AUTHOR_WARNING "")
**** file
***** file(GLOB variable [RELATIVE path] [globbing expressions]...)
       GLOB will generate a list of all files that match the globbing expressions and store it into the variable.
**** get_target_property
      get_target_property(DFTABLE_EXE dftables LOCATION)
      LOCATION is a property
      DFTABLE is a variable to store the property information
      dftable is the target.
**** string
***** string(REGEX MATCH <regular_expression> <output variable> <input> [<input>...])
***** string(REGEX REPLACE <regular_expression> <replace_expression> <output variable> <input> [<input>...])
       REGEX REPLACE will match the regular expression as many times as possible and substitute the replacement expression for the match in the output. The replace expression may refer to paren-delimited subexpressions of the match using \1, \2, ..., \9.
**** include
      read CMakeList file code from another file.
      If a module is specified instead of a file, the file with name <modulename>.cmake is searched in the CMAKE_MODULE_PATH.
**** option
      set a boolean value
      option(<option_variable> "help string describing option" [initial value])
**** configure_file
     configure_file(<input> <output> [COPYONLY] [ESCAPE_QUOTES] [@ONLY])
     Copies a file <input> to file <output> and substitutes variable values referenced in the file content.
     If <input> is a relative path it is evaluated with respect to the current source directory. The <input> must be a file, not a directory. If <output> is a relative path it is evaluated with respect to the current binary directory.
     This command replaces any variables in the input file referenced as ${VAR} or @VAR@ with their values as determined by CMake. If a variable is not defined, it will be replaced with nothing.
     e.g.:
     configure_file(
     "${CMAKE_MODULE_PATH}/cmake_uninstall.cmake.in"
     "${CMAKE_CURRENT_BINARY_DIR}/cmake_uninstall.cmake"
     IMMEDIATE @ONLY)
**** install
      install(TARGETS targets... [EXPORT <export-name>]
      [[ARCHIVE|LIBRARY|RUNTIME|FRAMEWORK|BUNDLE|
      PRIVATE_HEADER|PUBLIC_HEADER|RESOURCE]
      [DESTINATION <dir>]
      [PERMISSIONS permissions...]
      [CONFIGURATIONS [Debug|Release|...]]
      [COMPONENT <component>]
      [OPTIONAL] [NAMELINK_ONLY|NAMELINK_SKIP]
      ] [...])
**** link_directories
      link_directories(directory1 directory2 ...)
      Specify directories in which the linker will look for libraries.
      Specify the paths in which the linker should search for libraries. The command will apply only to targets created after it is called.
**** list
**** target_link_libraries
      target_link_libraries: Link a target to given libraries.
      target_link_libraries(<target> [item1 [item2 [...]]]
      [[debug|optimized|general] <item>] ...)
***** list(REMOVE_ITEM <list> <value> [<value> ...])
**** get_directory_property
      Get a property of DIRECTORY scope.
      get_directory_property(<variable> [DIRECTORY <dir>] <prop-name>)
      Store a property of directory scope in the named variable.
      The properties include: 
      INCLUDE_DIRECTORIES, 
      LINK_DIRECTORIES, 
      INCLUDE_REGULAR_EXPRESSION, 
      and ADDITIONAL_MAKE_CLEAN_FILES.
      ADDITIONAL_MAKE_CLEAN_FILES is a list of files that will be cleaned as a part of "make clean" stage.
**** set_directory_properties
      Set a property of the directory.
      set_directory_properties(PROPERTIES prop1 value1 prop2 value2)
      Set a property for the current directory and subdirectories.
**** set
      set(PCRECPP_HEADERS pcrecpp.h pcre_scanner.h ${CMAKE_BINARY_DIR}/pcrecpparg.h
      ${CMAKE_BINARY_DIR/pcre_stringpiece.h} )
      set(PCRECPP_SOURCES pcrecpp.cc pcre_scanner.cc pcre_stringpiece.cc)
*** target properties
**** MACOSX_BUNDLE
      Build an executable as an application bundle on Mac OS X.
**** WIN32_EXECUTABLE
      Build an executable with a WinMain entry point on windows.
      When this property is set to true the executable when linked on Windows will be created with a WinMain() entry point instead of of just main().This makes it a GUI executable instead of a console application.
*** standard CMake Modules
**** CheckCXXCompilerFlag
      Check whether the CXX compiler supports a given flag
**** CMakeForceCompiler
      CMAKE_FORCE_C_COMPILER
      CMAKE_CXX_COMPILER
      INCLUDE (CMakeForceCompiler)
**** findqt4
***** macro QT4_AUTOMOC(sourcefile1 sourcefile2 ... )
       This macro is still experimental.
       It can be used to have moc automatically handled.
***** macro QT_LIBRARIES
*** variables that describe the system
**** UNIX
      Set to true when the target system is UNIX or UNIX like (i.e. APPLE and CYGWIN).
**** WIN32
      Set to true when the target system is Windows and on cygwin.
*** CMAKE_SYSTEM_NAME
     Linux, Windows, Darwin for MacOSX etc.
*** compatibility commands
**** exec_program
      same as execute_process
*** example
**** 06-01-2010
      string(REPLACE "main.cpp" "" CXX_NO_MAIN "${CXX_FILES}")
      MESSAGE("\n\n moc files: ${MOC_FILES}\n\n")
      string(REPLACE "(*.cpp
      file(GLOB H_FILES "*.h")
      QT4_WRAP_CPP(MOC_FILES ${CXX_QOB_HEADERS})
      MESSAGE("\n\n moc files: ${MOC_FILES}\n\n")
      
      string(REPLACE "__/" "" MOC_FILES "${MOC_FILES}")
      MESSAGE("\n\n moc files: ${MOC_FILES}\n\n")
      FOREACH(MOC_FILE ${MOC_FILES})
      ADD_CUSTOM_COMMAND(
      OUTPUT $(CMAKE_CURRENT_BINARY_DIR)/MOC
      ADD_CUSTOM_TARGET(MOC_FILE DEPENDS $
      ENDFOREACH(MOC_FILE ${MOC_FILES})
      
      FOREACH(MOC_FILE ${MOC_FILES})
      add_dependencies(MOC_FILE MolAligner)
      ENDFOREACH(MOC_FILE ${MOC_FILES})
** include的原则
    在.h里面尽量使用forward declaration
    在.cpp里，多include各种user header
** gcc编译选项：
*** -Dname / -D name / -D name = definition
     定义宏，相当于 #define name 1
*** -fomit-frame-pointer
     忽略栈帧指针，在程序中不需要保存，安装，和恢复ebp
*** -O3 3rd level optimization.
*** -Wl,option 向linker传送option
     -Wl,-Map=output.map 传送自变量: -Map=output.map
     -Wl,-z,relro
*** -W warning
**** -Wall 启用所有warning.
**** -Wchar-subscripts
      warn if an array subscript has type "char"
** g++编译选项
*** -W warning
**** -Wnon-virtual-dtor
      Warn when a class has virtual functions and accessible non-virtual destructor, in which case it would be possible but unsafe to delete an instance of a derived class through a pointer to the base class.
**** -Wlong-long / -Wno-long-long
      default
      warn if long long type is used.
      To disable, use -Wno-long-long
**** -Wcast-align

*** -ansi
** function pointer
*** to declare
     a function pointer called my_func_ptr that points to a function taking an int and a char * and returning a float, is declared like this:
     float (*my_func_ptr)(int, char *)
*** to declare with typedef
     typedef float (*MyFuncPtrType)(int, char *)
     MyFuncPtrType my_func_ptr;
*** to point
     float some_func(int, char*);
     my_func_ptr = &some_func
     // You are allowed to ommit the "&" operator on most compilers
     // but it's not recommended.
*** to call
     (*my_func_ptr)(7, "Arbitrary String")
*** the use of function pointers
     In C, the most common uses of function pointers are as parameters to library functions like qsort, and as callbacks for Windows functions, etc. 
*** The compiler implementation of function pointers
     They are just "code pointers": they hold the starting address of an assembly-language routine. The different types of function pointers exist only to ensure that the correct calling convention is used.
*** restriction
     You are allowed to cast from one type of function pointer to another. But you are not allowed to cast a function pointer to a void * data pointer. 
     A function pointer can be set to 0 to mark it as a null pointer. 
     The full range of comparison operators are available (==, !=, <, >, <=, >=), and you can also test for null pointers using ==0 or via an implicit cast to bool. 
     A function pointer can be used as a non-type template parameter. This is fundamentally different from a type parameter, and is also different from an integral non-type parameter. It is instantiated based on name rather than type or value. Name-based template parameters are not supported by all compilers, not even by all those with support for partial template specialization.
** member function pointer
    You are not allowed to use an ordinary function pointer to point to a member function; instead, you have to use a member function pointer. 
*** to declare
     float(SomeClass::*my_memfunc_ptr)(int, char*);
     float(SomeClass::*my_const_memfunc_ptr)(int,char*) const;

     Notice that a special operator (::*) is used
*** to point
**** ordinary member function
      my_memfunc_ptr = &SomeClass::some_member_func;
**** operators
      my_memfunc_ptr = &SomeClass::operator !;
**** cannot take the address of a constructor or destructor
*** to call
**** on heap (->*)
      SomeClass *x = new SomeClass;
      (x->*my_memfunc_ptr)(6, "Another Arbitrary Parameter");
**** on stack (.*)
      SomeClass y;
      (y.*my_memfunc_ptr)(15, "Different parameters this time");
*** operators
     A member function pointer can be set to 0, and provides the operators == and !=, but only for member function pointers of the same class.
     Any member function pointer can be compared with 0 to see if it is null.  
     Unlike simple function pointers, the inequality comparisons (<, >, <=, >=) are not available.
*** as template type
     Like function pointers, they can be used as non-type template parameters, but this seems to work on fewer compilers.
*** restrictions
     Member function pointer can only point to member functions of a single class.
     you can't use a member function to point to a static member function.
     when dealing with derived classes, there are some surprises. 

*** static member function
     have to use a normal function pointer.
     Cannot use a member function pointer to point to a static member function.
*** inheritance
     member function pointer of a base_class member function cannot point to a member function of a publicly derived class. This is different from normal pointers.class.
**** example code
      SomeClass {
      public: 
      virtual void some_member_func(int x, char *p) {
      printf("In SomeClass"); };
      };
      
      class DerivedClass : public SomeClass {
      public:
      // If you uncomment the next line, the code at line (*) will fail!
      //    virtual void some_member_func(int x, char *p) { printf("In DerivedClass"); };
      };
      
      int main() {
      // Declare a member function pointer for SomeClass
      
      typedef void (SomeClass::*SomeClassMFP)(int, char *);
      SomeClassMFP my_memfunc_ptr;
      my_memfunc_ptr = &DerivedClass::some_member_func; // ---- line (*)
      }
** delegate
** polymorphism
*** essential
     = public inheritance 
     + base class pointers (or references) 
     + virtual functions.
*** example
     void func(const base_class &base_obj) {
     // do something
     }
     // even if we pass a derived_class object, 
     // the right function (derived version) will be called.
     func(base_obj);
     func(derived_obj);
** virtual methods
*** background
     a pointer to the base class can point to a derived class
     a reference to the base class can reference a derived class
*** when to use
     if a base class pointer (or reference) points to (or references) a derived class object and call a member function that is overriden in the derived class, the base member function will still be used. So if you want to use the derived version, you need to specify the method in the base class to be "virtual".
*** virtual methods called from other methods is still polymorphic
     http://www.cs.bu.edu/teaching/cpp/polymorphism/intro/ part 11
     virtual float base_class::member_func_a() const {
     }
     void base_class::member_func_b() const {
     member_func_a;
     }
     float derived_class::member_func_a() const {
     // something overriding the base_class definition
     }
     // then we call member_func_b on derived_class pointer or reference
     // which version of member_func_a will be called?

     // answer: the derived_class version. since member_func_a is essentially
     // this->member_func_a. and "this" is pointing to a derived_class object.
*** once a method is virtual, it is virtual in all derived classes as well.
*** it's a good practice to explicitly label it as virtual in derived classes as a reminder.
** class
*** private vs. protected
     protected members are acessible from derived classes and private are not
** inheritance
*** Public inheritance
     public remain public
     protected remain protected
     private not accessible
**** pointer to base class can point to an object of itself and of any publicly-derived class.
**** if a base class pointer points to a derived class object and call a member function that is overriden in the derived class, the base member function will be used.
      it is the type of the pointer (i.e., Employee), not the type of the object it points to (i.e., possibly Manager) that determines which version will be called.
*** Private inheritance
     Public become private
*** Protected inheritance
     Public become protected
** enum
    There are two kinds of enum type declarations. One kind creates a named type, as in
    enum MyEnumType { ALPHA, BETA, GAMMA };
    If you give an enum type a name, you can use that type for variables, function arguments and return values, and so on:
    enum MyEnumType x;  /* legal in both C and C++ */
    MyEnumType y;       // legal only in C++
    The other kind creates an unnamed type. This is used when you want names for constants but don't plan to use the type to declare variables, function arguments, etc. For example, you can write
    enum { HOMER, MARGE, BART, LISA, MAGGIE };
    enum TYPE {A,B,C,D}
    typedef enum {EMPL_PLAIN, EMPL_MANAGER} KindOfEmployee;
    KindOfEmployee kind;
    // then "kind" can only take EMPL_PLAIN(0) or EMPL_MANAGER(1).
** namespace
*** to declare
     namespace net_connect
     {
     int make_connection();
     int test_connection();
     // so forth...
     }
*** to use
     net_connect::make_connection()
*** using namespace
     using namespace_name; // used only in the current scope
     using namespace namespace_name; // used globally
     using namespace_name::thing;
*** anonymous namespace
     when the program is compiled, the anonymous namespace will be accessible within the same file. In effect, it's as though an additional "using" clause was included implicitly. This effectively limits the scope of anything in the namespace to the file level (so you can't call the functions in that namespace from another other file).
     相当于C的static.
**** definition
      namespace
      {
      // functions
      }
*** rename namespace
     if a namespace is too long, one can:
     namespace <new> = <old>
** random number generation
    random()
** using a variable number of arguments
    #include <cstdarg>
    va_list
    va_start
    va_arg
    va_end
** <iomanip>
    cout << setw(10) << "ten" << "four" << "four";
    ten          fourfour
    cout << setfill('-') << setw(80) << "-" << endl;
    ----------------------------------------------
    
** <iostream>
    设置左右对齐。
    cout.width(6);
    cout << left << n << endl;
    cout << internal << n << endl;
    cout << right << n << endl;
    设置等宽table column打印
    cout << setw(field_one_width) << left << first_string;
    cout << " ";
    cout << setw(field_two_width) << left << second_string;
    设置浮点数精度
    cout << setprecision(3) << 2.71828
    设置八进制和十六进制
    std::cout << setbase(16) << 342;
    std::dec, std::oct and std::hex are shorthands for setbase(10), setbase(8) and setbase(16)
    打印前缀0 and Ox
    cout << hex << showbase << n << endl;
    cout << hex << noshowbase << n << endl;
    cout.flags( ios::right | ios::hex | ios::showbase )
* LaTeX
** 用pdflatex emacs写文档
    C-c C-e document 自动提示documentclass，填充\begin{document} \end{document}
    
** minipage with borders
\fbox{
\begin{minipage}
...
\end{minipage}
}

*** \usepackage{fullpage}   全页显示
算法
需安装texlive-science

** 目录 tableofcontents:
\tableofcontents        通常放在标题页和摘要的后面
须运行两次latex以便更新tableofcontent
\setcounter{tocdepth}{2}        出现在目录表中的章节深度
深度计数从chapter（report and book)，section(article) 开始
\secnumdepth
include references into Table of contents:
right before the bibliograph command add,
\addcontentsline{toc}{section}{References}

To add a list of tables, after \tableofcontents, add,
\listoftables

To add a list of figures, after \tableofcontents, add,
\listoffigures

If you don't want some headings to be included,e.g. if you don't want Preface to be numbered, you'd use commands:
\subsection*{Preface}
\addcontentsline{toc}{subsection}{Preface}

** 字体
\small
\bfseries - 黑体
\em     - 斜体
** 段落
\setlength{\长度命令｝｛已定义的长度｝  /*FMARK_START*/
\setlength{\parindent}{0.5cm}   第一行的缩进设为0.5cm
\setlength{\textwidth}{12.5cm}  一行的长度
\setlength{\parskip}{1ex plux0.5ex minus0.2ex} - 两行间距（\parskip)等于当前字体中x的高度，但是可以伸长0.5或收缩0.8 (橡皮长度)
\renewcommand{\baselinestretch}{1.5}  字母基线间距(忽略诸如g,y字母下挂) 增加至1.5倍
\baselineskip   绝对行距(== \baselinestretch * 给定字体的默认行距)
\fill   正常长度0,可伸展至任何长度。
\setlength{\oddsidemargin}  奇数页左边界
\setlength{\evensidemargin} 偶数页左边界
\setlength{\topmargin}      页顶到页眉
\setlength{\headheight}     页眉高度
\setlength{\headsep}        页眉基线到正文顶部
\setlength{\headheight}     页眉高度
\setlength{\topskip}        正文顶部到正文第一行基线距离
\setlength{\textheight}     正文高度
\setlength{\textwidth}      正文宽度
\setlength{\footskip}       正文底部到页脚底部
*** 设置单词断点
man\-u\-script
*** 单双列页面
\twocolumn[整列文字]
\onecolumn
** 宏包使用
\usepackage[选项1，选项2...]{宏包1，宏包2, ...}
** 命令字符, 需要escape的字符:
\$
\&
\%
\#
\_
\{
\}
** 数学
*** 特殊数学集合符号：
     Special symbols
     \usepackage{amsfonts} 或者 \usepackage{amssymb}.
     \mathbb{set}
     Examples:
     \mathbb{P} for prime numbers
     \mathbb{N} for natural numbers
     \mathbb{Z} for integers
     \mathbb{I} for irrational numbers
     \mathbb{Q} for rational numbers
     \mathbb{R} for real numbers
     \mathbb{C} for complex numbers
     Positive real numbers can now be easily expressed :
     \mathbb{R}_{\geq0} by typing \mathbb{R}_{\leq0}.
*** dispay math mode
     \[ ... \] or \begin{equation} ... \end{equation}
     or \begin{align} ... \end{align}
*** 定理和证明
     \usepackage{amsthm}
**** 定理
      \newtheorem{定理标签}{定理名}
      \begin{定理标签}
      \end{定理标签}
      定理名是在pdf中显示的title
***** 自定义定理
       \newtheoremstyle{stylename}% name of the style to be used
       {spaceabove}% measure of space to leave above the theorem. E.g.: 3pt
       {spacebelow}% measure of space to leave below the theorem. E.g.: 3pt
       {bodyfont}% name of font to use in the body of the theorem
       {indent}% measure of space to indent
       {headfont}% name of head font
       {headpunctuation}% punctuation between head and body
       {headspace}% space after theorem head; " " = normal interword space
       {headspec}% Manually specify head
       以上需要按顺序输入，空变量表示使用默认值。
       e.g., headspec可以是：\thmname{#1}\thmnumber{#2}:\thmnote{#3}
       参看：http://en.wikibooks.org/wiki/LaTeX/Theorems
**** 证明
      \begin{proof}
      \end{proof}
***** 自定义证明标题
       \begin{proof}{Proof for an important theorem}
       \end{proof}
*** 大括号样式
** 文档类
    \documentclass[选项]{类}
    类:
    可以是 book report article 或 letter.
    article:  parts sections subsections
    report:   chapter part
    book:     book，对奇偶页区别对待并打印页眉页脚
*** 选项:
- 字体  10pt(默认) 或 11pt 12pt
- 纸张大小 letterpaper(默认) a4paper a5paper legalpaper executivepaper b5paper
- 默认竖直，可用landscape调为横向。
- 页面格式 onecolumn twocolumn (\columnsep 和 \columnseprule 调整列间距)
- 区分奇偶页 oneside twoside(页码在右当奇数，页码在左当偶数）
- 每章开始位置 openright openany
- 标题页 notitlepage(标题非单独一页） titlepage(标题单独一页）
- 公式编号在左 leqno
- 公式左对齐而不是居中fleqn (\mathindent 调整缩进大小）
\setlength{\mathindent}{2.5cm}
- 参考文献格式 openbib
- draft 在右边界突出的文本会加上粗黑标识。
- final 无粗黑标识
**** 例子:
\documentclass[11pt,twoside,fleqn]{article}
*** 页面样式, 页眉
\pagestyle{样式｝
样式可取：
plain(默认)     无页眉，中间页码
empty           无页眉也无页码
headings        页眉为章节标题。页脚为空
myheadings      页眉自定义，使用markright和markboth
\markright{右页眉}
\markboth{左页眉}{右页眉｝  - 适用于\twoside
\thispagestyle  同\pagestyle 但仅对当前页起作用
*** 页编号, 页码
\pagenumbering{数字形式}
数字形式可取
arabic
roman   小写罗马数字
Roman   大写罗马数字
alpha   小写字母
Alpha   大写字母
常用的样式是在前言设罗马数字
\pagenumbering{roman}
紧接第一个\chapter用\pagenumbering{arabic}
*** 标题
\title{My Title}
\author{You \thanks{Tel.00001-1234 FRG} \\
Max--Planck--Insitute \\
\and 
Me
\thanks{Tel.
4131231-32 USA
University of Iowa}
\date{日期文本}
\maketitle
\and 可用 \\ 取代，这样多作者上下排列
\thanks{脚注文本}
*** 摘要 abstract
\begin{abstract}
\end{abstract}
*** 章节
\章节命令［短标题］｛标题｝
\章节命令＊{标题}        - 不打印节号，不出现在目录中
章节命令包括 \part \chapter \section \subsection \subsubsection \paragraph \subparagraph
短标题用于目录和页眉
\part 对命令编号没有影响.
article 中， \part 第0层
book and report 中， \chapter 第0层
改变编号深度 \setcounter{secnumdepth}{数}
设定某章节名称的计数器:
\setcounter{章节名称}{数}
e.g. \setcounter{chapter}{2}    设定chapter的计数器为2 (下次调用chapter时，将看到chapter 3)
** 附录
\appendix
或者
\begin{appendix}
\end{appendix}
** 引用
    \label{mylabel}
    \ref{mylabel}
    \pageref{mylabel}
** 书的结构
\frontmatter    前言，目录      罗马数字页码，无章编号
\mainmatter     主体            阿拉伯数字页码，有章编号
\backmatter     参考文献，索引，版本记录        无章编号
** figure
figure caption
\caption[short caption]{full caption}
\rule[提升高度]{宽度｝｛高度｝  生成黑矩形
** quote 环境
\begin{quote}
\end{quote}
white space:
'\ '
\hspace{20pt}
\vspace{20pt}
** 日期
\today
\month
\year
** units
pt  point         (1 in = 72.27 pt)
pc  pica          (1 pc = 12 pt)
in  inch          (1 in = 25.4 mm)
bp  big point     (1 in = 72 bp)
cm  centimetre    (1 cm = 10 mm)
mm  millimetre
dd  didot point   (1157 dd = 1238 pt)
cc  cicero        (1 cc = 12 dd)
sp  scaled point  (65536 sp = 1 pt)
em  字体相关的尺寸,大写M的宽度
ex  x的高度
** dealing with floating objects:
\usepackage{placeins}. If you get the "too many unprocessed floats" throw in a \FloatBarrier and it forces LaTeX to dump all the floats before this point. I have an Appendix with 20 figures one after the other, I threw a \FloatBarrier in the middle and it solves all the issues. FloatBarrier is also great to get all floats within a section rather than letting it flow to the next section.
Or use \clearpage or \cleardoublepage
** table with cell auto-wrap:
use p{dimen} as the column specification.
Actually, this command specified the width of the cell
\begin{tabular}{|l|l|r|p{2in}|}
insert packages:
pressing <C-j> in insert mode takes you to the next <++> in the text
"amsmath" + <F5>
insert equations:
"eqnarray" + <F5>
shortcuts:
e^^ => e^{}
\pi => `p
&=& => ==
*** fold in latex-suite:
\rf: fold entire file/refresh folds.
za: toggle between fold and unfold.
*** add citation:
\ref{} or \cite{} in the curly braces, press <F9> is gonna reference bibliographic entries.
<F9> also work for searching filenames for \inputgraphics command and just plain searching for words.
*** compilation:
\ll compile the tex file into .dvi.
\lv view the dvi file created.
To change default to pdf format:
add:
    let Tex_DefaultTargetFormat="pdf"
    let g:Tex_DefaultTargetFormat="pdf"
    let g:Tex_ViewRule_pdf="xpdf"
    let g:Tex_CompileRule_pdf="pdflatex -interaction=nonstopmode $*"
    set sw=2
    set iskeyword+=:
    let g:Tex_MultipleCompileFormats="pdf"      # this one run bibtex/pdflatex sufficiently many times.
in .vim/ftplugin/tex.vim
*** view latex-suite help file:
help latex-suite.txt
help latex-suite-quickstart.txt
**** add Template:
:TTemplate
others: 
{\LaTeX}
{\LaTeXe}
或\LaTeX{} 或\LaTeX\
** long equation breaking techniques
*** use split package from amsmath
    \begin{equation}
    \begin{split}
    E_{cp}=\{\{u,v\} \mid \{u,v\} \subseteq E_t  \textrm{ or } \{u,v\} \subseteq E_h \\
    \textrm{ for some }(E_t,E_h) \in \mathcal{E}\}.
    \end{split}
    \end{equation}
*** use eqnarray
     \begin{eqnarray}\nonumber
     N(v) &=& \{v\} \cup \{u \in V \mid \{u,v\} \subseteq E \\
     && \textrm{ for some }E \in \mathcal{E}\}.
     \end{eqnarray}
**** nonumber guarantee that there is no label for the first part
** multiline subscript
*** use substack
     \sum_{\substack{0\le i\le m\\  0<j<n}} P(i,j)
*** use subarray environment
     \sum_{\begin{subarray}{l}
     i\in\Lambda\\  0<j<n
     \end{subarray}}
     P(i,j)
* rsync
Rsync
behave like cp, you can use it as you use cp
rsync options source destination
the most commonly seen options are -avz –progress
rsync is slower than cp but safer because it computes checksum as it goes at both source and destination

** rsync
PROJECTNAMECODE="project_sv"
rsync -Pravdtuze ssh /home/wzhou1/$(PROJECTNAMECODE)/src/ ris:/RIS/home/wzhou1/$(PROJECTNAMECODE)/src/ --exclude=pathsetup.py --exclude=*~ --exclude=*.pyc
rsync -Pravdtuze ssh ris:/RIS/home/wzhou1/$(PROJECTNAMECODE)/src/ /home/wzhou1/$(PROJECTNAMECODE)/src/ --exclude=pathsetup.py --exclude=*~ --exclude=*.pyc
rm -rf /home/wzhou1/$(PROJECTNAMECODE)/src/*~ /home/wzhou1/$(PROJECTNAMECODE)/src/*.pyc
ssh ris "rm -rf /RIS/home/wzhou1/$(PROJECTNAMECODE)/src/*~ /RIS/home/wzhou1/$(PROJECTNAMECODE)/src/*.pyc"
rsync -Pravdtuze ssh ris:/RIS/home/wzhou1/$(PROJECTNAMECODE)/fig/ /home/wzhou1/$(PROJECTNAMECODE)/hpcfig/
rsync -Pravdtuze ssh ris:/RIS/Scratch/wzhou1/$(PROJECTNAMECODE)/data /projects/wzhou1/$(PROJECTNAMECODE)/hpcdata
** use rsync as an alias for scp
#+begin_src sh
alias scp="rsync -Pravdtze ssh"
#+end_src
** most commonly used
rsync -auv --stats source destination 
(no need to have z, no need to have –progress either)
-u never replace new version with an old version

** preserves the time stamp
rsync -a preserves the time stamp
rsync -avz /var/opt/installation/inventory/ /root/temp/
cp -a preserves time stamps too.
cp -u only copy file when source's time stamp is newer than the destination's.

** Synchronize destination directory from the source directory
rsync -zvr /var/opt/installation/inventory/ /root/temp
-z enable compression
-v verbose
-r recursive (-a can replace this option, -a is recursive by default)

** remote synchronization
synchronize from local to remote
sync -avz /root/temp/ thegeekstuff@192.168.200.10:/home/thegeekstuff/temp/
synchronize from remote to local
rsync -avz thegeekstuff@192.168.200.10:/var/lib/rpm /root/temp

remote shell
rsync -avz -e ssh thegeekstuff@192.168.200.10:/var/lib/rpm /root/temp

** no overwrite modified files at the destination
rsync -u
rsync -avzu thegeekstuff@192.168.200.10:/var/lib/rpm /root/temp

** synchronize only the directory tree (not the files)
rsync -v -d thegeekstuff@192.168.200.10:/var/lib/ .

** View progress
View the rsync progress during transfer
rsync –progress
rsync -avz --progress thegeekstuff@192.168.200.10:/var/lib/rpm/ /root/temp/

** NOTE on the usage of / in address
rsync -avz --progress sourcedir/ destdir/
and
rsync -avz --progress sourcedir destdir/
are different!
The second ends up with destdir/sourcedir/...

** dry run
use rsync dry-run (without making changes) to detect directory differences
rsync -nric  dev2/py/lib/sysami/ dev/py/lib/sysami/
-i: output a change-summary for all updates
-c: skip based on checksum, not mod-time & size

-x: do not cross file system boundary (only on one file system)

** Delete the Files Created at the Target
–delete
if target has the new file called new-file.txt, when synchronize with the source with –delete option, it removed the file new-file.txt
rsync -avz --delete thegeekstuff@192.168.200.10:/var/lib/rpm/ .

** Do not create new file at the target
--existing only update the existing files
rsync -avz --existing root@192.168.1.2:/var/lib/rpm/ .

** View changes between source and destination
Similar to diff -r
rsync -avzi thegeekstuff@192.168.200.10:/var/lib/rpm/ /root/temp/
-i itemize the differences
no need to have -r because -a is recursive by default
or 
rsync -nric thegeekstuff@192.168.200.10:/var/lib/rpm/ /root/temp/
only compare the files, but not the time stamp and user information
-n: dry run (no change)
-i: output a change-summary for all updates
-c: skip based on checksum, not mod-time & size
output interpretation:
f represents that it is a file.
s represents size changes are there.
t represents timestamp changes are there.
o owner changed
g group changed.

REMEMBER to put / at the end of the source directory
An easy nemo is to append / to the end of both directories

** Include and Exclude Pattern during File Transfer
rsync -avz --include 'P*' --exclude '*' thegeekstuff@192.168.200.10:/var/lib/rpm/ /root/temp/
In the above example, it includes only the files or directories starting with ‘P’ (using rsync include) and excludes all other files. (using rsync exclude ‘*’ )

** Do Not Transfer Large Files
--max-size
rsync -avz --max-size='100K' thegeekstuff@192.168.200.10:/var/lib/rpm/ /root/temp/
max-size=100K makes rsync to transfer only the files that are less than or equal to 100K. You can indicate M for megabytes and G for gigabytes.

** Transfer the Whole File
One of the main feature of rsync is that it transfers only the changed block to the destination, instead of sending the whole file.

If network bandwidth is not an issue for you (but CPU is), you can transfer the whole file, using rsync -W option. This will speed-up the rsync process, as it doesn’t have to perform the checksum at the source and destination.

rsync -avzW  thegeekstuff@192.168.200.10:/var/lib/rpm/ /root/temp

** resume scp transfer
rsync --partial --progress --rsh=ssh user@host:remote_file local_file
* MySQL
** starting mysql server
*** Fedora/CentOS/RedHat/OpenSuse
sudo service mysqld start
sudo service mysqld stop
sudo service mysqld restart
*** Ubuntu/Debian
sudo service mysql start
sudo service mysql stop
sudo service mysql restart
** SQL
*** create user
   create user 'wanding'@'localhost' identified by 'wanding'
*** add foreign key
   ALTER TABLE ORDERS ADD FOREIGN KEY (customer_sid) REFERENCES CUSTOMER(sid);
*** drop foreign key
   ALTER TABLE tbl_name DROP FOREIGN KEY fk_symbol;
   use show create table to find out fk_symbol
*** copy table
   create table t1 select * from t2
*** copy only table structure
   create table t1 like t2
*** copy table if not exist
   create table if not exists t1 select * from t2
*** drop table if exists
   drop table if exists t1
*** set password
   set PASSWORD for `wanding`@localhost = PASSWORD('wanding')
** If you are getting timeout errors, check to see if your SQL server has any orphaned threads.
  mysql --user=root bioseqdb -e "SHOW INNODB STATUS\G" | grep "thread id"
** how to deal with large datasets
  http://www.mysqlperformanceblog.com/2007/07/05/working-with-large-data-sets-in-mysql/
** how to split data into smaller tables
  http://www.mysqlperformanceblog.com/2006/10/08/small-things-are-better/
** difference between PRIMARY and UNIQUE
  PRIMARY refers to a field or a set of fields used to identify each row in a table. UNIQUE refers to a field that cannot be repeated in a table. The primary key is, by necessity, a unique identifier, but any field may be defined as unique. The primary key can be a composite, consisting of more than one field. Each field within a composite key can be repeated, but each combination of fields must be unique.
** how to find the size of a table
  http://lists.mysql.com/mysql/197563
** create index
  index doesn't have to be unique. There's a "unique index" which is indeed unique.
*** create multicolumn index
   CREATE INDEX idx_ftp_files_command_and_dir
   ON ftp_files(ftp_command, ftp_root_dir);
   This creates an index over the two fields ftp_command and ftp_root_dir. This index will improve the performance of any queries run against these fields. 
*** create single column index
   CREATE INDEX idx_ftp_files_username
   ON ftp_files(ftp_username);
   This index would be helpful if I was running queries against this field, like this:
   SELECT * FROM ftp_files 
   WHERE UPPER(ftp_username)='FRED'
** handling duplicate
  1. use "insert ignore"
     if a record doesn't duplicate an existing record, MySQL inserts it as usual. If the record is a duplicate, the IGNORE keyword tells MySQL to discard it silently without generating an error.
  2. on duplicate update oldkey=newkey
  3. use "replace" rather than "insert"
     If the record is new, it's inserted just as with INSERT. If it's a duplicate, the new record replaces the old one.
  4. add "unique index"
     create table person_tbl (
     first_name CHAR(20) NOT NULL,
     last_name CHAR(20) NOT NULL,
     sex CHAR(10)
     UNIQUE (last_name, first_name)
     );
  5. counting duplicates
     select count(*) as repetitions, last_name, first_name from person_tbl group by last_name, first_name having repetitions > 1
  6. remove duplicates from a table
     mysql> CREATE TABLE tmp SELECT last_name, first_name, sex
     ->                  FROM person_tbl;
     ->                  GROUP BY (last_name, first_name);
     mysql> DROP TABLE person_tbl;
     mysql> ALTER TABLE tmp RENAME TO person_tbl;
  7. An easy way of removing duplicate records from a table is that add an INDEX or PRIMAY KEY to that table.
     mysql> ALTER IGNORE TABLE person_tbl
     -> ADD PRIMARY KEY (last_name, first_name); 
** update a table using a column in another table
  UPDATE items,month SET items.price=month.price
  WHERE items.id=month.id;
** kill thread
  And if there are, assuming you are the only person using this database, you might try killing them off using the thread id given by the above command:
  mysql --user=root bioseqdb -e "KILL 123;"
** import schema using mysql command
  mysql -h localhost -uwanding -pwanding biosql < biosqldb-mysql.sql
** mysqlimport

** 数据和Table分开dump，用mysqlimport载入
  tar -zxvf go-YYYYMM-TYPE-tables.gz
  cd <releasedir>
  echo "create database go" | mysql
  cat *.sql | mysql -u wanding -pwanding go
  mysqlimport -u wanding -pwanding -L mygo *.txt
** show create table mytable
** 杀query进程
  SHOW PROCESSLIST;
  KILL #; (where # is the process id)
** create a table with a particular storage engine
  create table example (id INT, data VARCHAR(100)) TYPE=innodb;
** install MySQL and beginning
  sudo aptitude install mysql-server
  input the adminitration password
  现在就以root户用登陆MySQL来使用（root密码为安装时你设置的密码，默认密码为空，建议为了安全改掉。）：
  #mysql -uroot -p (回车后输入密码，即可进入mysql)
  1、显示数据库列表
  mysql>show databases;
  
  （注：MySQL语句分隔符为“；”）
  默认有三个数据库：information_schema、mysql和test。information_schema库为MySQL默认字典库，mysql库很重要它里面有MYSQL的系统信息，我们改密码和新增用户，实际上就是用这个库进行操作。
  
  2、显示库中的数据表：
  mysql>use mysql；(指定mysql库)
  mysql>show tables;
  
  3、显示数据表的结构：
  mysql>describe yourtablename;

  4、建库：
  mysql>create database yourdbname;

  5、建表：
  mysql>use yourdbname；
  mysql>create table yourtablename (columnname colunmtype, ...)；
  
  6、删库和删表:
  mysql>drop database yourdbname;
  mysql>drop table yourtablename；

  7、将表中记录清空：
  mysql>delete from yourtablename;
  
  8、显示表中的记录：
  mysql>select * from yourtablename;

  9、举个例子：一个建库和建表以及插入数据的实例
  mysql>create database world; //建立库world
  mysql>use world; //打开库world
  mysql>create table city //建立表city
　　    (ID int(3) not null auto_increment ,
     　　Name char(30) not null default '',
            CountryCode char(3) not null default '',
            District char(20) not null default '',
　    　 Population integer not null default '0',
            Primary key ('ID')
　　    ); //建表结束
   //以下为插入字段
   mysql>insert into city values('','Kabul','AFG','Kabol','1780000');
   mysql>insert into city values('','Beijing','CHN','Beijing','1780001');
   
   五、工具连接MySQL
   用工具会使MySQL以图形化方式展现的更好，操作起来也更方便。
   MySQL官方自身有自带的图形化工具MySQL Control Center（已退休），MySQL GUI Tools（包含Mysql Query Browser，Mysql Administrator，MySQL Migration Toolkit ），挺好用，就是对中文的支持不够好。
   官方下载地址：http://dev.mysql.com/downloads/gui-tools/5.0.html
   当然还有更多第三方的客户端工具可以用，如EMS、SQLyog、phpMyAdmin、Navicat、Toad等众多的图形管理工具。各有各的特点，都很好用，此处推荐EMS。
   
   六、管理MySQL
   启动关闭：
   进入Windows服务管理器中，找到‘MySQL’服务。右键菜单启动停止MySQL服务。
   [开始/控制面板/服务/MySQL...]
   
   七、备份恢复MySQL
   MySQL本身提供一个很好的备份工具，效率很高。下面以备份恢复一个数据库为例说明。
   
   备份：
   #mysqldump -u youruser -p  --database  dbname > /root/filename.sql
   (因为密码为空，直接回车。filename.sql备份中包含了创建数据库及表的脚本。)
   
   恢复：
   #mysql -uroot  -p < /root/filname.sql
** SQL language basics
*** delete a table
   DROP TABLE `model_director`, `model_movie`;
*** browse a part of the table (from row 130 to 130+30)
   SELECT *
   FROM `MoleculeName`
   LIMIT 130 , 30
*** filter
   SELECT *
   FROM `edge`
   WHERE `repID` = 2
   LIMIT 0,30
*** tutorial website read
   the following is a brief sql syntax illustration
   http://sql.1keydata.com/cn/sql-syntax.php
*** wildcards
   see http://sql.1keydata.com/cn/sql-like.php
*** built-in function
   see http://sql.1keydata.com/cn/sql-functions.php
*** find how many different entries does the database have
   SELECT COUNT(DISTINCT store_name)
   FROM Store_Information
*** the setup of primary key and foreign key
   CREATE TABLE ORDERS
   (Order_ID integer,
   Order_Date date,
   Customer_SID integer,
   Amount double,
   Primary Key (Order_ID),
   Foreign Key (Customer_SID) references CUSTOMER(SID));
   # CUSTOMER is another table whose SID is one of its primary keys
   # Or you can use ALTER command
   ALTER TABLE ORDERS
   ADD FOREIGN KEY (customer_sid) REFERENCES CUSTOMER(sid);
*** TRUNCATE
   TRUNCATE TABLE customer
   # this clean the table customer
*** INSERT INTO
   INSERT INTO Store_Information (store_name, Sales, Date)
   VALUES ('Los Angeles', 900, 'Jan-10-1999')
*** PRIMARY KEY
   PRIMARY KEY ('nameID')
*** DESCRIBE
   The describe SQL command is used to list all of the fields in a table and the data format of each field. It is phrased as: 
   DESCRIBE [TableName];
   Examples: This would list all of the fields and their formats for the table `Colors`:
   describe Colors;
*** ROLLBACK
   causes all data changes since the last BEGIN WORK, or START TRANSACTION to be discarded by the relational database.
*** INNER JOIN/LEFT JOIN/RIGHT JOIN/FULL JOIN
**** example No. 1
    SELECT Persons.LastName, Persons.FirstName, Orders.OrderNo
    FROM Persons
    INNER JOIN Orders
    ON Persons.P_Id = Orders.P_Id
    ORDER BY Persons.LastName
**** example No. 2
    SELECT `node`.`nodeID`,`node`.`repID`,`node`.`element`,`rep`.`repID`
    FROM `node`
    INNER JOIN `rep`
    ON `node`.`repID` = `rep`.`repID`
    WHERE `rep`.`Cnumber` = 'C05710'
    ORDER BY `node`.`repID`
*** UNION
   SELECT column_name(s) FROM table_name1
   UNION
   SELECT column_name(s) FROM table_name2
   Note: The column names in the result-set of a UNION are always equal to the column names in the first SELECT statement in the UNION.
** administration
  GRANT ALL PRIVILEGES ON `molecules` . * TO 'wanding'@'localhost' WITH GRANT OPTION ;
** python API to mysql
  sudo aptitude install python-mysqldb python-elixir python-sqlalchemy
  sqlalchemy.create_engine(URL)
  URL格式为 dialect://user:password@host/dbname
  dialect = enum{firebird, sqlite, mysql, PostgreSQL, Oracle}
** install phpmyadmin
  sudo apt-get install phpmyadmin
  提示安装apache2
  输入链接mysql的密码
  ===============
  安装完成后可在firefox中查看/localhost/phpadmin/
  本机上查看以下地址：
  /var/www/
  /etc/apache2/sites-available
  /etc/apache2/sites-enabled
** datatype
*** TEXT TYPES
   CHAR( )    A fixed section from 0 to 255 characters long.
   VARCHAR( ) A variable section from 0 to 255 characters long.
   TINYTEXT   A string with a maximum length of 255 characters.
   TEXT       A string with a maximum length of 65535 characters.
   BLOB       A string with a maximum length of 65535 characters.
   MEDIUMTEXT A string with a maximum length of 16777215 characters.
   MEDIUMBLOB A string with a maximum length of 16777215 characters.
   LONGTEXT   A string with a maximum length of 4294967295 characters.
   LONGBLOB   A string with a maximum length of 4294967295 characters.
*** NUMBER TYPES
   TINYINT( )   -128 to 127 normal 0 to 255 UNSIGNED.
   SMALLINT( )  -32768 to 32767 normal 0 to 65535 UNSIGNED.
   MEDIUMINT( ) -8388608 to 8388607 normal 0 to 16777215 UNSIGNED.
   INT( )       -2147483648 to 2147483647 normal 0 to 4294967295 UNSIGNED.
   BIGINT( )    -9223372036854775808 to 9223372036854775807 normal 0 to 18446744073709551615 UNSIGNED.
   FLOAT        A small number with a floating decimal point.
   DOUBLE( , )  A large number with a floating decimal point.
   DECIMAL( , ) A DOUBLE stored as a string , allowing for a fixed decimal point.
*** DATE TYPES
   DATE      YYYY-MM-DD.
   DATETIME  YYYY-MM-DD HH:MM:SS.
   TIMESTAMP YYYYMMDDHHMMSS.
   TIME      HH:MM:SS.
*** MISC TYPES
   ENUM specified possible values.
   SET Similar to ENUM except each column may have more than one of the specified possible values.
** using sqlalchemy
*** a basic tutorial
   import sqlalchemy
   sqlalchemy.__version__
   from sqlalchemy import create_engine
   engine=create_engine('mysql://wanding:wanding@localhost/movies',echo=True)
   from sqlalchemy import Table, Column, Integer, String, MetaData, ForeignKey
   metadata = MetaData()
**** Define a table
    users_table = Table('users', metadata, 
        Column('id', Integer, primary_key=True), 
        Column('name',String(50)),
        Column('fullname',String(50)),
        Column('password', String(50))
    )
**** Define a Python Class
    class User(object):
        def __init__(self, name, fullname, password):
	    self.name = name
	    self.fullname = fullname
	    self.password = password
	def __repr__(self):
	    return "<User('%s','%s','%s')>" % (self.name, self.fullname, self.password)
**** Setting up a ORM mapping
    from sqlalchemy.orm import mapper
    mapper(User, users_table)
    注意: 在此User class同时具备了users_table定义的每一个列。即使其本身没有定义。
**** Use declarative (in this case use elixir instead)
    from sqlalchemy.ext.declarative import declarative_base
    Base = declarative_base()
    class User(Base):
        __tablename__ = 'users'

	id = Column(Integer, primary_key=True)
	name = Column(String)
	fullname = Column(String)
	password = Column(String)

	def __init__(self, name, fullname, password):
	    self.name = name
	    self.fullname = fullname
	    self.password = password
	def __repr__(self):
	    return "<User('%s','%s','%s')>" % (self.name, self.fullname, self.password)
     metadata = Base.metadata
**** create a session
    from sqlalchemy.orm import sessionmaker
    Session = sessionmaker(bind=engine)
    Session = sessionmaker() # if you don't have an engine available
    Session.configure(bind=engine) # after you create_engine, you can rebind
    session = Session()
**** add a new object
    ed_user = User('ed', 'Ed Jones', 'edspassword')
    session.add(ed_user)
    # note that upto now the session is pending
**** commit
1. By querying
   our_user = session.query(User).filter_by(name='ed').first()
   ed_user is our_user
2. By commiting
   session.commit()
**** add more rows
    session.add_all([User('wendy','Wendy Williams', 'foobar'),
        User('mary', 'Mary Contrary', 'xxg527'),
	User('fred', 'Fred Flinstone', 'blah')])
    ed_user.password = 'f8s7ccs'
**** see what have been changed
    session.dirty # Note that session.dirty is a member data not a function
    # one can not call session.dirty()
**** see what is new
    session.new # again, a member data instead of a function
**** roll back
    ed_user.name = 'Edwardo'
    fake_user = User('fakeuser','Invalid', '12345')
    session.add(fake_user)
    session.query(User).filter(User.name.in_(['Edwardo','fakeuser'])).all()
    session.rollback()
    ed_user.name # give 'ed'
    fake_user in session # give false
    session.query(User).filter(User.name.in_(['ed','fakeuser'])).all()
    # this gives [<User('ed','Ed Jones', 'f8s7ccs')>]
**** query
    for instance in session.query(User).order_by(User.id):
        print instance.name, instance.fullname
    for row in session.query(User, User.name).all(): # treated as a tuple
        print row.User, row.name
    from sqlalchmey.orm import aliased
    user_alias = aliased(User, name='user_alias')
    for row in session.query(user_alias, user_alias.name.label('name_label'))
        print row.user_alias, row.name_label
    for u in session.query(User).order_by(User.id)[1:3]:
        print u
    for name, in session.query(User.name).filter_by(fullname='Ed Jones'):
        print name
    for user in session.query(User).filter(User.name=='ed').filter(User.fullname=='Ed Jones'): # 多重filter
        print user
***** Common Filter Operators
     query.filter(User.name == 'ed')
     query.filter(User.name != 'ed')
     query.filter(User.name.like('%ed%'))
     query.filter(User.name.in_(['ed','wendy','jack']))
     query.filter(User.name.in_(session.query(User.name).filter(User.name.like('%ed%'))))
     query.filter(~User.name.in_(['ed','wendy','jack']))
     query.filter(User.name == None)
     query.filter(User.name != None)
     query.filter(and_(User.name == 'ed', User.fullname == 'Ed Jones'))
     query.filter(User.name =='ed').filter(User.fullname == 'Ed Jones')
     query.filter(or_(User.name == 'ed', User.name =='wendy'))
     query.filter(User.name.match('wendy')) 
***** Return Lists and Scalars
     query=session.query(User).filter(User.name.like('%ed')).order_by(User.id)
     query.all()
     query.first()
     from sqlalchemy.orm.exc import MultipleResultsFound
     try:
         user = query.one()
     except MultipleResultsFound, e:
         print e
     from sqlalchemy.orm.exc import NoResultFound
     try:
         user = query.filter(User.id == 99).one()
     except NoResultFound, e:
         print e
***** Using Literal SQL
     for user in session.query():
     print user.name
     session.query(User).filter('id<:value and name=:name').\
     params(value=224, name='fred').order_by(User.id).one()
     session.query(User).from_statement("SELECT * FROM users where name=:name").params(name='ed').all()
     session.query("id","name","thenumber12").from_statement("SELECT id, name, 12 as thenumber12 FROM users where name=:name").params(name='ed').all()
***** Counting
     # determine how many rows the SQL statement would return
     session.query(User).filter(User.name.like('%ed')).count()
     session.query(User.id, User.name).filter(User.name.like('%ed')).count()
     session.query(User.name).group_by(User.name).count()
     from sqlalchemy import func
     session.query(func.count(User.name), User.name).group_by(User.name).all()
**** Build a relation
***** Define a second class using declarative
     from sqlalchemy import ForeignKey
     from sqlalchemy.orm import relation, backref
     class Address(Base):
         __tablename__ = 'addresses'
	 id = Column(Integer, primary_key=True)
	 email_address = Column(String, nullable=False)
	 user_id = Column(Integer, ForeignKey('users.id'))
	 # now users is the name of the table that appears in the
	 # actual database instead of in the program
	 # this is many-to-one because ForeignKey need to be unique.
	 
	 user = relation(User, backref = backref('addresses', order_by = id))
	 # now 'addresses' would appear on the User class.
	 # We can define it on the user class by
	 # addresses = relation(Address, order_by=Address.id, backref="user")

	 def __init__(self, email_address):
	     self.email_address = email_address
	 def __repr__(self):
	     return "<Address('%s')>" % self.email_address
***** create engine again
     metadata.create_all(engine)
***** working with related objects
     jack = User('jack','Jack Bean','gjffdd')
     jack.addresses = [Address(email_address='jack@google.com'),Address(email_address = 'j25@yahoo.com')]
     jack.addresses[1]
     jack.addresses[1].user
     session.add(jack)
     session.commit()
     # query object
     jack=session.query(User).filter_by(name='jack').one()
     jack.addresses
     # In this case, jack.addresses is loaded on the fly. In other word,
     # jack.addresses is a another query, jack.addresses[1] is another query
     # again. this increase the number of queries. [Lazy Load relation]
     # We can make it load when jack is returned from the query by using
     # eagerload
     from sqlalchemy.orm import eagerload
     jack = session.query(User).options(eagerload('addresses')).filter_by(name=
     'jack').one()
     jack.addresses
     # Now the previous command is no longer a query. It becomes a so-called
     # 'outer-join'
***** Querying with Joins
     
**** Building a Many To Many Relation
    from sqlalchemy import Text
    # this is for defining the many to many relationship
    post_keywords = Table('post_keywords', metadata, 
    Column('post_id', Integer, ForeignKey('posts.id')),
    Column('keyword_id',Integer, ForeignKey('keywords.id'))
    )

    class BlogPost(Base):
        __tablename__ = 'posts'

	id = Column(Integer, primary_key = True)
	user_id = Column(Integer, ForeignKey('users.id'))
	headline = Column(String(255), nullable = False)
	body = Column(Text)

	# many to many BlogPost<->Keyword
	keywords = relation('Keyword', secondary=post_keywords, backref='posts')

	def __init__(self, headline, body, author):
	    self.author = author
	    self.headline = headline
	    self.body = body

	def __repr__(self):
	    return "BlogPost(%r,%r,%r)" % (self.headline, self.body, self.author)
     class Keyword(Base):
         __tablename__ = 'keywords'
	 id = Column(Integer, primary_key = True)
	 keyword = Column(String(50), nullable = False, unique = True)

	 def __init__(self, keyword):
	     self.keyword = keywo
**** insert
    >>> ins = users_table.insert()
    # users_table is a Table() object
    >>> str(ins)
*** some basic concepts
**** metadata
    for issuing commands such as CREATE, DROP and ALTER
    see /doc/metadata.html in the release document
***** declaration
     from sqlalchemy import MetaData
     meta = MetaData()
***** accessing
     metadata.sorted_tables gives a list(ordinary list) of table objects(module level, see below)
     for t in metadata.sorted_tables:
         print t.name
***** CREATE
     meta.create_all()
     here's an important note:
     =====================
     create_all(engine) creates foreign key constraints between tables usually inline with the table definition itself, and for this reason it also generates the tables in order of their dependency. There are options to change this behavior such that ALTER TABLE is used instead. (use_alter)
     =====================
     tableobject.meta.create(engine) is for creating one table
     it's highly recommended to use the checkfirst flag in the create() function
     tableobject.meta.create(engine, checkfirst=True)
***** DROP
     meta.drop_all(engine)
     the presence of each table is checked first, and tables are dropped in reverse order of dependency
     tableobject.meta.drop(engine) is for dropping one table, where tableobject is the object created using Table()
     again,
     tableobject.meta.drop(engine, checkfirst=True) is highly recommended.
***** Use MetaData object without 'engine'
     meta = MetaData()
     meta.bind = engine
     # Now we can call create_all() and drop_all() without the engine
     # describe a table called 'users', query the databases for its columns
     users_table = Table('users',meta,autoload=True)
     result = users_table.select().execute()
     # without binding the engine
     users_table = Table('users', meta, autoload=True, autoload_with=engine)
     users_table = engine.execute(users_table.select())
     ## if you don't have multiple engines and don't have the need to reference connections explicitly, you should bind engines.
***** reflecting tables
     You need to specify the table name, a MetaData object and the autoload=True flag.
     message = Table('messages', meta, autoload=True, autoload_with=engine)
     Here's a paragraph on what exactly reflection is:
     ==================
     When tables are reflected, if a given table references another one via foreign key, a second Table object is created within the MetaData object representing the connection. Below, assume the table shopping_cart_items references a table named shopping_carts. Reflecting the shopping_cart_items table has the effect such that the shopping_carts table will also be loaded
     >>> shopping_cart_items = Table('shopping_cart_items', meta, autoload=True, autoload_with=engine)
     >>> 'shopping_carts' in meta.tables
     we can access the already generated shopping_carts table just by naming it:
     >>> shopping_carts = Table('shopping_carts',meta)
     ==================
     see overriding reflection in the doc/metadata.html
     (THIS SECTION IS STILL NOT VERY CLEAR)
***** ondelete and onupdate option
     These are options in ForeignKey() or ForeignKeyConstraint().
     it can be RESTRICT | CASCADE | SET NULL | NO ACTION
****** ON DELETE CASCADE
      FOREIGN KEY ('something') REFERENCES molecule ('Cnumber') ON DELETE CASCADE
      onupdate='CASCADE' means the remote side of the foreign key would change corresponding to the update action issued here.
      ondelete = 'CASCADE' means the row would be deleted if the remote side get deleted.
****** ON DELETE SET NULL
      ondelete = 'SET NULL' means the key would be set to NULL if the remote side is deleted.
      These required InnoDB in mysql.
      Use these two options with care, they might cause unexpected result. e.g., 'CASCADE' means if one is changed, the other would be changed as well.
      see http://www.oreillynet.com/onlamp/blog/2004/10/hey_sql_fans_check_out_foreign.html
***** datatype
     Float() is FLOAT(10) by default
     String(N) is in most cases VARCHAR(N) where N is some integer and cannot be omitted.
***** primary key
     primary key is set NOT NULL automatically
     integer primary key is set auto_increment by default
***** use_alter
     The following illustrate the use_alter difference:
     usage: Column('Cnumber', String(6), ForeignKey('Cnumber', use_alter=True and name='fk_Cnumber'))
****** with use_alter=True and name='fk_Cnumber'
      CREATE TABLE `moleculeName` (
      `Cnumber` VARCHAR(6), 
      `nameID` INTEGER NOT NULL AUTO_INCREMENT, 
      name VARCHAR(70), 
      PRIMARY KEY (`nameID`)
      )ENGINE=InnoDB
      
      # after all the tables are defined.
      ALTER TABLE `moleculeName` ADD CONSTRAINT `fk_Cnumber` FOREIGN KEY(`Cnumber`) REFERENCES molecule (`Cnumber`) ON DELETE CASCADE
****** without use_alter=True and name='fk_Cnumber'
      CREATE TABLE `moleculeName` (
      `Cnumber` VARCHAR(6), 
      `nameID` INTEGER NOT NULL AUTO_INCREMENT, 
      name VARCHAR(70), 
      PRIMARY KEY (`nameID`), 
      FOREIGN KEY(`Cnumber`) REFERENCES molecule (`Cnumber`) ON DELETE CASCADE
      )ENGINE=InnoDB
***** post_update
     a flag defined in mapper(properties={'something':relation(Object, post_update=True})
     the document says 'postupdate' means a propertyloader is telling us, "yes i know you already inserted/updated this row but I need you to update one more time"
     some other post says, sometimes two instances are dependent on eacth other(in some self referencing table, for example) and cannot be INSERTed whole. the "postupdate" flag is used to remedy this situation which will issue a second UPDATE statement to associate the two rows togeter after they have been INSERTed.
     see document /sqlalchemy.orm/mapper properties/sqlalchemy.orm.relation/parameters/post_update bullet
     """
     this indicates that the relationship should be handled by a second UPDATE statement after an INSERT or before a DELETE. Currently, it also will issue an UPDATE after the instance was UPDATEd as well, although this technically should be improved. This flag is used to handle saving bi-directional dependencies between two individual rows (i.e., each row references the other), where it would otherwise be impossible to INSERT or DELETE both rows fully since one row exists before the other. Use this flag when a particular mapping arrangment will incur two rows that are dependent on each other, such as a table that has one-to-many relationship to a set of child rows, and also has a column that references a single child row within that list. ... If a flush() operation returns an error that a "cyclical dependency" was detected, this is a cue that you might want to use post_update to "break" the cycle.
     """
   
*** using in-memory SQLite database (so that testing would be easier).
   It can be in-memory:
   from sqlalchemy import create_engine
   engine = create_engine('sqlite:///:memory:', echo=True)
** about composite foreign key
  all of the elements in the composite foreign key needs to be composite primary key. otherwise, there's no need to composite. (It seems to be a weak reason. but it is a reason.)
** DROP TABLE error in InnoDB
  set foreign_key_checks=0;
  then you can drop the table.
** restrictions in InnoDB
*** cannot put composite keys where one of the components is auto-incrementing
** backup and restore
  use phpmyadmin
  mysqldump -u [username] -p [password] [databasename] > [backupfile.sql]
  --add-drop-table option create a backup file which can rewrite an existing database without having to delete the older database mannually first.
  e.g., mysqldump --add-drop-table -u sadmin -p pass21 Customers > custback.sql
  mysql -h localhost -uwanding -p coworkers < coworkers.sql
*** backup only specified tables
   mysqldump --add-drop-table -u sadmin -p pass21 Customers customer_master customer_details > custback.sql
   mysqldump -u [username] -p [password] [databasename] [table1 table2 ...]
  
** show engines
** mysql -uwanding -p
** 主配置文件 /etc/mysql/my.cnf
** restart mysqld
  kill mysqld
  sudo mysqld
** set InnoDB as the default storage engine
  put:
  default-storage-engine=innodb
  in /etc/mysql/my.cnf
  or commandline argument when starting mysqld
  sudo mysqld --default-storage-engine innodb &
** select version(), current_date;
** select sin(pi()/4), (4+1)*5;
** select now();
** select user();
** \c
** \q
** show databases;
** use a database from command line argument
  mysql -h host -u user -p mydatabase (notice here mydatabase is not an argument to -p, a password has to be passed by -pmypassword)
  
** add constraint
  alter table publication add unique (name, paper);
** having
*** 1.
   SELECT votes.songID,SUM(votes.Vote),COUNT(votes.Vote) AS count,AVG(votes.Vote) AS average,songs.songTitle,songs.songID
   FROM user_song_votes as votes,songs 
   WHERE (votes.songID = songs.songID) 
   GROUP BY votes.songID HAVING COUNT(votes.Vote) > 8 
   ORDER BY average DESC, count DESC 
   LIMIT $start, " . ($limit + 1);
*** 2.
   SELECT cog.COGId, count(distinct cog_gene.org) as cnt FROM cog join cog_gene on cog.COGId=cog_gene.COGId  group by cog.COGId having cnt > 50
   
** create view 
* Lisp
** Query PGDB from pathway-tools
A sample session
$ pathway-tools -lisp
;; Select E. coli as the current organism 
EC(1): (select-organism :org-id 'ecoli)
ECOLI

;; Count the number of genes in the KB.
EC(1): (length (get-class-all-instances '|Genes|))
4425

;; Find all genes whose nucleotide position on the chromosome is
;; less than 10,000.
EC(3):  (loop for x in (get-class-all-instances '|Genes|)
for pos = (get-slot-value x 'left-end-position)
when (and pos (< pos 10000))
collect x)
(EG11512 EG11555 EG10998 EG11000 EG10999 EG11556
 EG11511 G6081 EG10011 EG11277)

;; Set variable g to the previous result.
EC(4): (setq g *)
(EG11512 EG11555 EG10998 EG11000 EG10999 EG11556
 EG11511 G6081 EG10011 EG11277)
;; Load the example.lisp file from the PTools web site, which

;; defines various additional functions.  You can write you own
;; additional functions in such files.
EC(5): (load "~/examples")

;; Loading ~/examples.lisp
;; Print the previously generated list of genes as a table.
EC(6): (object-table g)
EG11512                             b0010
EG11555                             b0007
EG10998                             thrA
EG11000                             thrC
EG10999                             thrB
EG11556                             talB
EG11511                             mog
G6081                               b0005
EG10011                             b0006
EG11277                             thrL
NIL

;; End our session
EC(7): (exit)
; killing "Initial Lisp Listener"
; Exiting Lisp

To abort an error
:continue 4

store queries in file
You can create a .lisp file containing functions that you define that store queries and associated functions so that you can reuse them in the future. 

The file will define one or more Lisp functions (procedures). For example, the following file defines a function that counts the number of monomers present in the current DB. All lisp files you create for use with the Pathway Tools should begin with the line (in-package :ecocyc) to select the proper Lisp package (namespace).
(in-package :ecocyc)
(defun num-monomers ()
  (length (get-class-all-instances '|Polypeptides|))
  )
To define or redefine this function within your Lisp session, load the file using the Lisp load function. If the preceding file was called monomers.lisp in your home directory, you could load it into your session by typing:
(load "~/monomers")

Cooper book (Basic Lisp Techniques)
With Franz ACL
you can use pathway-tools -lisp to try

to exit
(excl:exit) 
strong exit
(excl:exit 0 :no-unwind t) 
a short cut, the toplevel command
:exit

prepare a source file /tmp/hello.lisp, make /tmp/hello.fasl (the fast load compiled machine-code file)
(in-package :user) 
(defun hello () 
(write-string "Hello, World!")) 

to compile a source file
EC(1): (compile-file "/tmp/hello") 
;;; Compiling file /tmp/hello.lisp 
;;; Writing fasl file /tmp/hello.fasl 
;;; Fasl write complete 
#P"/tmp/hello.fasl" 
NIL 
NIL 
to load (you can load without compiling, the source will be compiled automatically)
EC(2): (load "/tmp/hello") 
; Fast loading /tmp/hello.fasl 
T 

switch namespace and then call hello function
(in-package :user)
CL-USER(3): (hello) 
Hello, World! 
"Hello, World!




Tips on using Allegro Common Lisp
http://www.cs.utexas.edu/~mooney/cs351/allegro-tips.html

Abort an error
If an error occurs, the Lisp interpreter will invoke the "break package" with error message and a different prompt. To simply exit from the error, type ":res" for "resume" and it will abort the run and return to the top-level prompt. For example: 
USER(1): (rest 3)
Error: Attempt to take the cdr of 3 which is not listp.
  [condition type: SIMPLE-ERROR]
[1] USER(2): :res
USER(3): 

Abort a run
C-c C-c
Then :res

Format
~%: newline forced
~&: newline when necessary
~T: tab
http://www.gigamonkeys.com/book/a-few-format-recipes.html
Some resources

Common lisp cook book
http://cl-cookbook.sourceforge.net/index.html

Practical common lisp
http://www.gigamonkeys.com/book/

ACL online free training
http://www.franz.com/services/classes/

ACL documentation
http://franz.com/support/documentation/current/doc/

ANSI common lisp specification
http://www.franz.com/support/documentation/current/ansicl/ansicl.htm

resource center from ACL
http://www.franz.com/newtolisp/index.lhtml

MIT opencourseware
http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-001-structure-and-interpretation-of-computer-programs-spring-2005/video-lectures/
On Lisp
/home/wanding/sync/Books/Programming/Lisp/onlisp.pdf

SBCL
http://www.sbcl.org/
http://cs.gmu.edu/~sean/lisp/

common lisp the language
http://www.cs.cmu.edu/Groups/AI/html/cltl/clm/clm.html
* CMake
﻿1. in-source build:
cd src/
ccmake .  ## this will look for the CMakeLists.txt in the current directory
make
1. out-of-source build:
   mkdir build
   cd build
   ccmake ../src
   make
   Warning: there shouldn’t be any CMakeCache.txt in src/
1. cmake -DCMAKE_BUILD_TYPE=Debug
2. cmake -DCMAKE_BUILD_TYPE=Release
3. add_executable - add an executable to the project using the specified source files
add_executable(<name> [WIN32] [MACOSX_BUNDLE] [EXCLUDE_FROM_ALL] source2, ... sourceN)
<name> is also made a globally unique logical target name, which might be used to be fed into target_link_libraries.
add_executable(MolAligner ${CXX_MAIN} ${CXX_SOURCES} ${MOC_FILES})
1. project - set a name for the entire project
        project(<projectname> [languageName1 languageName2 ...])
        one can specify which languages the project supports. languageName can be CXX, C, Fortran, etc.
        project(MolAligner)
1. find_package - load settings for an external project
        find_package(<package> [version] [EXACT] [QUIET] [[REQUIRED | COMPONENTS] [components...]] [NO_POLICY_SCOPE])
        This command usually executes the Find<package>.cmake file in the CMAKE_MODULE_PATH which is specified by the cmake installation. This command set a bunch of package-specific variables.
        see Standard CMake Modules section for a list of packages, note that the name of the module (e.g., FindQt4) is not the same as the name of the package (e.g. Qt4).
        find_package(Qt4 REQUIRED)
        find_package(Qt4 4.4.3 COMPONENTS QtCore QtGui QtXml REQUIRED)
        If no Find<package>.cmake can be found in CMAKE_MODULE_PATH, then cmake looks for <name>Config.cmake or <lower-case-name>config.cmake in the <package>_DIR directory. 
        see cmake documentation for detail
1. cmake_minimum_required - set the minimum required version of cmake for  a project
        cmake_minimum_required(VERSION major[.minor[.patch]] [FATAL_ERROR])
        this implicitly calls cmake_policy function, see documentation
        cmake_minimum_required(VERSION 2.8)
1. set - set a CMAKE variable to a given value
        set(<variable> <value> [[CACHE <type> <docstring> [FORCE]] | PARENT_SCOPE])
        <type> and <docstring> are required when CACHE is present.
        If <value> is not specified then the variable is removed instead of set (equivalent to unsert() command). 
        set(CXX_SOURCES
      ${CMAKE_SOURCE_DIR}/mol_render.cpp
      ${CMAKE_SOURCE_DIR}/mainwindow.cpp
              ${CMAKE_SOURCE_DIR}/molecule.cpp
              ${CMAKE_SOURCE_DIR}/layout_gen.cpp
)
set(CMAKE_CXX_FLAGS “${CMAKE_CXX_FLAGS} -Wno-deprecated -O2”)
1. QT4_WRAP_CPP - create moc code from a list of files containing Qt class with the Q_OBJECT declaration. 
        
        options can be added such as those found by executing “moc -help”
        QT4_WRAP_CPP(MOC_FILES ${CXX_QOB_HEADERS})
1. include_directories - ddd include directories to the build
        include_directories([AFTER|BEFORE] [ SYSTEM] dir1 dir2 ...]
        By default, the directories are appended to the current list of directories (AFTER behavior). But one can toggle to BEFORE to prepend the directories.
        include_directories(${QT_QTSQL_INCLUDE_DIR})
1. target_link_libraries - link a target to a given libraries
        target_link_libraries(<target> [item1 [item2 [...]]] [[debug|optimized|general] <item>] ...)
        Library dependencies are transitive by default. When this target is linked into another target then the libraries linked to this target will appear on the link line or the other target too.
        Although one can link either a library or an executable, the link command would be issued only when the executable is built.
        CMake will generate a dependency for the executable on the library. Always remember that CMake script is a meta-compilation system. It doesn’t have to follow the order of the real compilation.
        target_link_libraries(MolAligner ${QT_LIBRARIES} ${QT_QTSQL_LIBRARY})
1. add_subdirectory - add a subdirectory to the build
        add_subdirectory(source_dir [binary_dir] [EXCLUDE_FROM_ALL])
        If source_dir is a relative path, it will be interpreted with respect to the current directory.
        binary_dir specify the directory in which to place the output files. It’s default to the source_dir.
        add_subdirectory(test)
1. add_custom_target - Add a target with no output so it will always be built.
        add_custom_target(Name [All] [command1 [args1 ...]] 
         [COMMAND command2 [args2 ...] ...]
         [DEPENDS depend depend depend ...]
         [WORKING_DIRECTORY dir]
         [COMMENT comment] [VERBATIM]
         [SOURCES src1 [src2...]])
        If one wants to add dependency on non-file targets, he should consider add_dependencies. This command can be used to transfer file-dependency across directories.
        The target has no output and is always considered out of date.
add_custom_target(moc_is_ready DEPENDS ${MOC_FILES})
1. add_dependencies - add a dependency between top-level targets
        add_dependencies(target-name depend-target1 depend-target2 ...)
top-level targets include ones created by ADD_EXECUTABLE, ADD_LIBRARY, ADD_CUSTOM_TARGET.
        add_dependencies(test_render moc_is_ready)
1. configure_file - copy a file to another location and modify its contents
        configure_file(<input> <output> [COPYONLY] [ESCAPE_QUOTES] [@ONLY])
        Copies a file <input> to file <output> and substitutes variable values referenced in the file content.
        <input> must be a file, not a directory. <output> is an existing directory the input file is placed in that directory with its original name.
        This command replaces any variables in the input file referenced as ${VAR} or @VAR@ with their values as determined by CMake. If a variable is not defined, it will be replaced with nothing. If COPYONLY is specified, then no variable expansion will take place. 
configure_file(“${CMAKE_CURRENT_SOURCE_DIR}/test_molecule.ATP.dat””${CMAKE_CURRENT_BINARY_DIR}” COPYONLY)
1. string - 
2. message - 
        message(SEND_ERROR “You need Python”)
        message(${_py_file})
        message(“In directory var: ${var} LCV: ${LCV_SUB}”)
1. set_source_files_properties
set_source_files_properties(${MOC_FILES} PROPERTIES GENERATED 1)
1. foreach
        foreach(_py_file ${_py_files})
                configure_file(${_py_file} ${CMAKE_CURRENT_BINARY_DIR} COPYONLY}
        endforeach(_py_file)
1. file
file(MAKE_DIRECTORY ${CMAKE_BINARY_DIR}/python/mysubdir)
        file(GLOB _py_files “*.py”)
1. add_library
add_library(uGraph SHARED ${CXX_SOURCE_STAND})
1. execute_process
execute_process(COMMAND ${CMAKE_COMMAND} -E copy_directory “${CMAKE_SOURCE_DIR}/pyHypergraph” “${CMAKE_BINARY_DIR}/pyHypergraph”)
1. set_target_properties
set_target_properties(uGraph PROPERTIES OUTPUT_NAME “Graph”)
1. Some useful variables
See www.cmake.org/Wiki/CMake_Useful_Variables
CMAKE_CXX_FLAGS - a feature not appearing in the official documentation
CMAKE_CURRENT_BINARY_DIR
CMAKE_CURRENT_SOURCE_DIR
CMAKE_BINARY_DIR
CMAKE_SOURCE_DIR
* SQLite
** tutorial by Mike Chirico
    http://souptonuts.sourceforge.net/readme_sqlite_tutorial.html
*** common commands
     sqlite3 test.db  "create table t1 (t1key INTEGER PRIMARY KEY,data TEXT,num double,timeEnter DATE);"
     sqlite3 test.db  "insert into t1 (data,num) values ('This is sample data',3);"
     sqlite3 test.db  "insert into t1 (data,num) values ('More sample data',6);"
     sqlite3 test.db  "insert into t1 (data,num) values ('And a little more',9);"

     sqlite3 test.db  "select * from t1 limit 2";
     sqlite3 test.db  "select * from t1 order by t1key limit 1 offset 2";
     sqlite3 test.db ".table"	# show table names.
     sqlite3 test.db "select * from sqlite_master" # get table information from "sqlite_master"
     sqlite3 test.db ".dump"
     sqlite3 test.db ".dump"|sed -e s/t1/t2/|sqlite3 test2.db
*** triggers
     CREATE TRIGGER insert_t1_timeEnter AFTER INSERT ON t1
     BEGIN
     UPDATE t1 SET timeEnter = DATETIME('NOW')  WHERE rowid = new.rowid;
     END;
     
     sqlite3 test.db  "insert into t1 (data,num) values ('First entry with timeEnter',19);"
     sqlite3 test.db "select * from t1";
**** scripting in sqlite
      sqlite3 test.db < script
      Comments are preceded by "--".
*** Logging all inserts, updates and deletes.
*** UTC and localtime.
*** Other date and time commands.
*** The ATTACH command: Build a virtual table that spans multiple tables on separate databases.
*** The power of the SIGN function -- A mathematical explanation.
** official tutorial
    http://www.sqlite.org/sqlite.html
* matlab
** help commands
    help, lookfor, helpwin, helpdesk, demos
** install matlab
    cd
    mkdir -p matlab_iso
    sudo mount -o loop /media/FlashDriv/ml2011au.iso /home/wanding/matlab_iso
    # loop means using the same device
    # or open iso with archive_mounter in ubuntu (right click)
    umount /home/wanding/matlab_iso
*** other libraries necessary (if complained)
     libXp (on fedora)
*** libc linking issue (on ubuntu) /oscheck.sh: 605: /lib64/libc.so.6: not found
     sudo ln -s /lib64/x86_64-linux-gnu/libc-2.13.so /lib64/libc.so.6
     for 32 bit system, find libc by "locate libc.so.6"
     on a 32-bit ubuntu, the solution is:
     sudo ln -s /lib/i386-linux-gnu/libc.so.6 /lib/libc.so.6
*** libc++ linking issue (on ubuntu) libc++.so.6: not found
     cd ~/MATLAB/R2011a/sys/os/glnx86
     mv libstdc++.so.6 libstdc++.so.6.orig
     mv libstdc++.so.6.0.10 libstdc++.so.6.0.10.orig
     the following is optional
     ln /usr/lib/x86_64-linux-gnu/libstdc++.so.6 libstdc++.so.6
** pdf tutorials
    matlab quick tutorial
    file:/home/wanding/sync/Books/Computing/matlab_quick_tutorial.pdf
    matlab plotting tutorial
    file:/home/wanding/sync/Books/Computing/matlab_plotting_tutorial.pdf
** website tutorials
*** official
     demos and webinars http://www.mathworks.com/products/demos/#
     mathworks support http:/www.mathworks.com/support
     self-training tutorial http://click.em.mathworks.com/?qs=f247efb39e1584fbfeca1b1b0f3057e24c1feab710f253cf6462ed49ac13a08dbee65a098fe39a8f
*** third party
     UFL Matlab tutorial http://www.math.ufl.edu/help/matlab-tutorial/
     matlab tips and tricks http://www.ee.columbia.edu/~marios/matlab/matlab_tricks.html
     SIU Matlab tutorial http://www.math.siu.edu/MATLAB/tutorials.html
     UCSD Matlab Primer http://www.math.ucsd.edu/~bdriver/21d-s99/matlab-primer.html
     MIT Matlab Wiki http://matlab.wikia.com/wiki/FAQ
** get environment variable
    getenv("COMMUNITY_DATA")
** run .m file from terminal
    matlab -nojvm -nodisplay -nosplash something.m
** grammar
*** matrix multiplication
**** a * b
**** a .* b
      element-wise multiplication
**** a ./ b
      element-wise division
**** a .^2
      element-wise power
*** matrix creation
     eye(3); zeros(3); ones(3)
*** matrix filtration
     y(t()<=1) = t(t()<=1);
     y(t()>1) = 1./ t(t()>1);
*** data import/export
**** load filename
      load all variables in filename
**** load filename x
      load variable x from filename
**** load filename a*
      load all variable start with a from filename
**** save filename
      save all variable in current workspace in filename.mat
**** save filename x,y
      save variables x,y in filename.mat
**** xlsread, xlswrite
      copy data from excel sheet
*** file IO
**** read
      fid = fopen(‘filename.txt’,‘r’);
      X = fscanf(fid,‘%5d’);
      fclose(fid);
      fread
**** write
      fid = fopen(‘filename.txt’,‘w’);
      count = fwrite(fid,x);
      count returns the number of variables successfully stored.
      fclose(fid);
      fprintf
      disp
**** escape single quote
      two quotes instead of one
*** flow control
**** if
**** switch
**** for
**** while
**** break
* GnuPlot
   http://gnuplot-tricks.blogspot.com/2009/11/broken-histograms.html
   file://home/wanding/sync/Books/Computing/gnuplot.pdf
   file://home/wanding/sync/Books/Computing/gpcard.pdf
   gnuplot demo
   http://gnuplot.sourceforge.net/demo/
   http://t16web.lanl.gov/Kawano/gnuplot/index-e.html
** IBM tutorial
*** the basics
     help <command>
     q to quit
     
     plot sin(x)
     
     set xrange [-pi:pi]
     replot
     reset
     
     set title "My first graph"
     set xlabel "Angle. \n in degrees"
     set ylabel "sin(angle)"
     plot sin(x)
     
     set xrange [-pi:pi]
     # major tics have level 0, minor tics have level 1
     set xtics ("0" 0, "90" pi/2, "-90" -pi/2, "" pi/4 1, "" -pi/4 1, "" 3*pi/4 1, "" -3*pi/4 1)
     set grid
     set xlabel "Angle,\n in degrees"
     set ylabel "sin(angle)"
     plot sin(x)

     unset grid
     load "filename"
     
     pause -1

     set terminal png
     set output "output.png"
     replot

     set term aifm		# adobe illustrator
     set term corel		# corel draw
     set term dxf		# AutoCad
     eepic, latex, pstricks, texdraw, tpic and so on
     set ylabel "$sin(\\theta)$"	# latex output

     \input{output.tex}		# the output file in your latex file

     set term post enh		# enhanced PostScript, essentially PostScript with bounding boxes
     set out 'gplt.eps'
     set xlabel '{/Symbol q_1}'
     set ylabel 'sin^2({/Symbol q_1})'
     plot sin(x)**2

     # square is synonymous to an aspect ratio of 1;
     # scale y-axis by 2, retain x-axis size
     set size ratio square 1,2
     
*** plotting more than one curve
     plot sin(x), cos(x)
     
     unset xtics # keep all other things simple
     plot sin(x) with linespoints pointtype 5, cos(x)  w boxes lt 4
     
     set key top left
     set key box
     plot [-pi:pi] sin(x) title "sinusoid" with linespoints pointtype 5, cos(x) t 'cosine' w boxes lt 4

     set xrange [-pi:pi]
     
     # Uncomment the following to line up the axes
     # set lmargin 6
     # Gnuplot recommends setting the size and origin before going to
     # multiplot mode
     # this sets up bounding boexes and may be required on some terminals
     set size 1,1
     set origin 0,0

     # Done interactively, this takes gnuplot into multiplot mode
     # and brings up a new prompt ("multiplot >" instead of "gnuplot >")
     set multiplot

     # plot the first graph so that it takes a quarter of the screen
     set the size 0.5, 0.5
     set origin 0, 0.5
     plot sin(x)

     # plot the second graph so that it takes a quarter of the screen
     set size 0.5, 0.5
     set origin 0.5, 0.5
     plot cos(x)

     # plot the fourth graph so that it takes a quarter of the screen
     set size 0.5, 0.5
     set origin 0.5,0
     plot 1/cos(x)
     
     # on some terminals, nothings gets plotted until this command is issued
     unset multiplot

     # remove all customization
     reset
*** plotting data
     set xdata time		# the x axis data is time
     set timefmt "%d-%b-%y"	# the dates in the file look like 10-Jun-04
     set format x "%b %d"	# on the x axis, we want tics like Jun 10
     plot ["31-May-04":"11-Jun-04"] 'ibm.dat' using 1:2 with linespoints
     # "using 1:2" tells gnuplot to use the first column for x-axis and second column for y-axis.
     plot ["31-May-04":"11-Jun-04"] 'ibm.dat' using 1:($2+$3+$4+$5)/4:4:3 title 'daily prices, IBM' with yerrorbars # the format is x:y:ylow:yhigh

     set bar 5
     plot ["31-May-04":"11-Jun-04"] 'ibm.dat' using 1:2:3:4:5 with financebars 

     set datafile separator <string>
     set datafile commentschar <char>

     plot "< awk --f preprocess.awk data.file" # popen
     set datafile missing 'NaN'	# specify the symbol for not a number
     
     plot 'file'
     plot 'file' using 1:2
     plot 'file' using ($1):($2)

     help ternary
     plot 'file' using 1:($3>10 ? $2 : 1/0) # plot the second column as the y-value unless the third column is more than 10.

     A(jw) = ({0,1}*jw/({0,1}*jw+p1)) * (1/(1+{0,1}*jw/p2))
     p1 = 10
     p2 = 10000
     set dummy jw
     set grid x y2
     set key default
     set logscale xy
     set log x2
     unset log y2
     set title "Amplitude and Phase Frequency Response"
     set xlabel "jw (radians)"
     set xrange [1.1 : 90000.0]
     set x2range [1.1 : 90000.0]
     set ylabel "magnitude of A(jw)"
     set y2label "Phase of A(jw) (degrees)"
     set ytics nomirror
     set y2tics
     set tics out
     set autoscale  y
     set autoscale y2
     plot abs(A(jw)), 180/pi*arg(A(jw)) axes x2y2
     
     save set 'filename'
     save var <file>
     save func <file>
** Duke University tutorial
    http://www.duke.edu/~hpgavin/gnuplot.html
*** plotting functions
     plot sin(x) / x
     splot sin(x*y/20)
     plot sin(x) title 'Sin Function', tan(x) title 'Tangent'
*** plotting data
     plot "force.dat" using 1:2 title 'Column',\
     "force.dat" using 1:3 title 'Beam'
     
     Abbreviation:
     u using
     t title
     w with

     plot "fileA.dat" using 1:2 title 'data A', \
     "fileB.dat" using 1:3 title 'data B'
     
     help splot datafile
     "splot" is about plotting 3D data
*** customizing your plot
     plot "force.dat" using 1:2 title 'Column' with lines, \
     "force.dat" u 1:3 t 'Beam' w linespoints

     plots may be in "lines, points, linespoints, impulses, dots, steps, fsteps, histeps, errorbars, xerrorbars, yerrorbars, xyerrorbars, boxes, boxerrorbars, boxxyerrorbars, financebars, candlesticks or vector."
     
     set title "Force-Deflection Data"
     set xlabel "Deflection (meters)"
     set ylabel "Force (kN)"
     set xrange [0.001:0.005]
     set yrange [20:500]
     set autoscale		# have gnuplot determine ranges
     set key 0.01, 100		# move the key, ## NOTE: key refers to the legend.
     unset key			# delete the key
     set label "yield point" at 0.003, 260 # Put a label on the plot
     unset label		# remove all labels
     set logscale
     unset logscale; set logscale y
     set xtics (0,002, 0.004, 0.006, 0.008)
     unset xtics; set xtics auto
     
     set datafile commentschars "#%" # change comment character
*** scripts
     gnuplot> load "force.p"
* mac admin
** how to remap mac iterm2 key binding
http://stackoverflow.com/questions/6205157/iterm2-how-to-get-jump-to-beginning-end-of-line-in-bash-shell
preferences -> keys
alt/opt <-  SEND ESC SEQ b
alt/opt ->  SEND ESC SEQ f
command a   HEX CODE 0x01
command e   HEX CODE 0x05

command d   HEX CODE 0x04 # this is deletion! otherwise iterm2 will emit the annoying vertical split
see http://www.unix-manuals.com/refs/misc/ascii-table.html
then remember to "copy preferences to profile"

you can also use 'showkey -a'
to find hex code
** fink
*** install fink
sh Install\ fink.tool
then follow the instruction

fink will install everything under /sw/bin/

*** use fink

search packages
fink list bioperl

install packages
fink install bioperl-pm1546

there would be a separate version of perl5.16.2 instead of the default perl

* vim

C-d half-page down
C-u half-page up

C-b page up
C-f page down

C-o last cursor
C-i next cursor

% jump match bracket

4j move down 4 lines

:syntax on

open file and split window
:n
:new [new filename]

open file and not 
:e 

turn on line number
:set number
:set nu

turn off line number
:set nonumber
:set nu!

mark current line
V
